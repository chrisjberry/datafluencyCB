[["index.html", "PSYC753 Data Fluency: Analysis Overview", " PSYC753 Data Fluency: Analysis Chris Berry Overview This workbook contains worksheets for the seven sessions given by Chris Berry: 17/01/22. Simple regression 24/01/22. Multiple regression 1: multiple continuous predictors 31/01/22. ANOVA 1 07/02/22. Multiple regression 2: one continuous, one categorical 14/02/22. Multiple regression 3: hierarchical regression (21/02/22. No session) 28/02/22. ANOVA 2 07/03/22. Pre-post data, effect sizes, clinically significant change It also contains a link to the: the Analysis Assessment (50% of module grade) FAQs Use the left and right arrow keys to navigate this workbook Ben Whalley's materials from part 1 of PSYC753: https://benwhalley.github.io/datafluency/ The PSYC753 DLE page 2021-22: https://dle.plymouth.ac.uk/course/view.php?id=56923 "],["simple1.html", "Session 1 Simple Regression 1.1 Overview 1.2 Worked Example 1.3 Predicting 1.4 Residuals 1.5 Evaluating the model 1.6 Exercise 1.7 Further Exercises 1.8 Summary 1.9 References", " Session 1 Simple Regression Chris Berry 2022 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} 1.1 Overview Slides from the lecture part of the session: Download In simple regression, we create a linear (straight line) model of the relationship between one outcome variable and one predictor variable The outcome variable is what we want to predict or explain (e.g., anxiety scores) The predictor variable is what we use to predict the outcome variable (e.g., average hours of screen time per week) Both the outcome and predictor variable are continuous variables Regression is actually a more general technique that underpins a wide variety of analyses. Simple regression is the most basic form of regression. The simple regression equation has the form Predicted outcome = a + b(Predictor) a is the intercept, it is the height of the line. More formally, it's the value of the outcome when the predictor is equal to zero. b is the slope (or coefficient for the predictor variable). It determines the steepness of the line, or, more formally, the amount of change in the outcome variable for a one unit increase in the predictor variable. The goal of simple regression is to obtain the values of the intercept (a) and slope (b) so that the line 'fits' or 'goes through' our data as closely as possible. The specific method of 'fitting' the line to the data is called the method of least squares (described in the lecture). R can do this all automatically for us (with lm()). 1.2 Worked Example Is screen time linked to mental health? Teychenne and Hinkley (2016) used regression to investigate the association between anxiety and daily hours of screen time (e.g., TV, computer, or device use) in 528 mothers with young children. Read in the data from their study to R and store in mentalh: # ensure tidyverse is loaded library(tidyverse) # read the data to R using read_csv() mentalh &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/1_mental_health_data.csv&#39;) There are numerous variables in mentalh (use mentalh %&gt;% glimpse() to take a look). We will focus only on two here: anxiety_score: a score representing the number of anxiety symptoms experienced in the past week, and screen_time: hours per day of screen time use on average. (Note: the data are publicly available, but I've changed some of the variable names for clarity.) First, create a scatterplot of the two variables. Put the predictor variable on the x-axis, and the outcome variable on the y-axis. # The code below takes mentalh and pipes it to ggplot() # aes() is used to tell ggplot() to put # screen_time on the x-axis, and anxiety_score on the y-axis # The settings in geom_smooth() tell ggplot() to # add the regression line (method = &#39;lm&#39;) and # not show confidence intervals (se = FALSE) mentalh %&gt;% ggplot(aes(x = screen_time, y = anxiety_score)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = FALSE) Figure 1.1: Scatterplot of anxiety score according to screen time (hours per day) Exercise 1.1 Describe the relationship between screen time and anxiety evident in the scatterplot (pick one): Individuals with lower levels of screen time tend to have higher anxiety scores No association between screen time and anxiety scores is apparent Individuals with higher levels of screen time tend to have higher anxiety scores Use lm() to run the simple regression and store the results in simple1: # conduct a simple regression to predict anxiety_score from screen_time # store the results in simple1 simple1 &lt;- lm(anxiety_score ~ screen_time, data = mentalh) Explanation: To specify the regression equation, we use outcome_variable ~ predictor_variable.The ~ symbol is a tilde. We use it to specify certain formulas in R. When you see ~, you can read it as \"as a function of\". So, outcome variable ~ predictor variable means \"outcome variable as a function of the predictor variable\". In our case, \"anxiety_score as a function of screen_time\". The intercept (a) and slope (b) are automatically calculated by R and stored in simple1: # look at the results simple1 ## ## Call: ## lm(formula = anxiety_score ~ screen_time, data = mentalh) ## ## Coefficients: ## (Intercept) screen_time ## 5.5923 0.1318 Exercise 1.2 The value of the intercept a is 5.590.13 The value of the slope b for the screen_time predictor is 5.590.13 The regression equation (Predicted Outcome = a + b(Predictor)) can therefore be written as what? predicted screen time = 5.59 + 0.13(anxiety score) predicted anxiety score = 5.59 + 0.13(screen_time) predicted anxiety score = 0.13 + 5.59(screen_time) 1.3 Predicting The regression equation can be used for prediction. Suppose someone asked us what the anxiety_score would be for a new person whose screen_time score is 10 hours per week. By reading off from the regression line on the scatterplot from earlier, the anxiety_score looks to be around 7: Figure 1.2: Predicted anxiety score for a person with 10 hours screen time Using the regression equation, we can substitute 10 for screen_time, then calculate predicted anxiety_score more precisely. The augment() function in the broom package can be used to work out the prediction for new data automatically: # load the broom package library(broom) # store new scores as a tibble new_scores &lt;- tibble(screen_time = 10) # give new_scores to &#39;newdata&#39; option in augment() augment(simple1, newdata = new_scores) screen_time .fitted 10 6.91012 The predicted anxiety_score is in the .fitted column and is 6.91 Predictions for multiple individuals can also be made at once. Here we obtain the predictions for two people with screen_time scores of 10 and 15. # store the scores we want predictions for in new_scores new_scores &lt;- tibble(screen_time = c(10, 15)) # use augment() to obtain the predicted anxiety_scores augment(simple1, newdata = new_scores) screen_time .fitted 10 6.910120 15 7.569031 Each row shows one individual. Their predicted anxiety_scores are 6.91 and 7.57. 1.4 Residuals The residual for a given datapoint is its vertical distance from the regression line. It is the error in prediction of the outcome variable for that datapoint. Residual = Observed - Predicted To view the residuals, again use the augment() function in the broom package (this time without including newdata). The residual for each observation is given in the column .resid # look at the residuals for simple1 (in .resid) # pipe to head() to only show the first 6 rows augment(simple1) %&gt;% head() anxiety_score screen_time .fitted .resid .hat .sigma .cooksd .std.resid 7 2.571429 5.931167 1.068833 0.0022003 3.514985 0.0001024 0.304677 10 1.428571 5.780559 4.219441 0.0029659 3.510454 0.0021534 1.203238 13 4.214286 6.147666 6.852334 0.0019133 3.502526 0.0036559 1.953016 13 7.285714 6.552426 6.447574 0.0039508 3.503969 0.0067111 1.839532 3 18.571430 8.039682 -5.039682 0.0402420 3.508118 0.0449817 -1.464784 2 1.500000 5.789972 -3.789972 0.0029045 3.511390 0.0017011 -1.080735 Exercise 1.3 What was the residual for a person with anxiety_score equal to 13, and screen_time score equal to 7.29? 6.556.856.45 For this person, was the anxiety score predicted by the model overpredicted (too high)fit exactlyunderpredicted (too low) Explain The person has an anxiety_score of 13 and screen_time score of 7.29. The predicted anxiety_score for this datapoint is 6.55 (in the .fitted column), so the model underpredicts the observed value of anxiety_score. An assumption underlying regression is that the residuals are like random noise. When we plot the residual against the predicted values, there should be no trend evident in the datapoints in the plot. We can use this plot for checking this assumption of regression. # Create a plot of the predicted values vs. residuals # Use the &quot;.fitted&quot; and &quot;.resid&quot; columns in augment() # Use geom_hline() to draw a horizontal line at y = 0 augment(simple1) %&gt;% ggplot(aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0) Figure 1.3: Predicted anxiety score vs. the residual Explanation: If there's no trend in the residuals, we'd expect the points to look like a random cloud above and below the horizontal line (at y = 0). There should be no patterns, and the points should be pretty symmetrically distributed around a single point in the middle of the plot. There's some slight indication that the residuals tend to have lower values as the predicted values (.fitted) increase. In other words, there's some tendency for the model to overestimate anxiety_score as screen_time becomes more extreme. The residuals also seem more spread out above the horizontal at lower predicted values, but this doesn't look too serious and the plot seems okay. Issues here can indicate that improvement in the model is possible. 1.5 Evaluating the model 1.5.1 R2 R2 is a statistic that describes how well our model explains the outcome variable. It ranges between 0 and 1 and can be interpreted as the proportion of variance in the outcome variable that is explained by the predictor variable. To obtain R2 for the model: # use glance() to obtain R-squared glance(simple1) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.0148346 0.0129617 3.511952 7.920493 0.0050709 1 -1411.456 2828.913 2841.72 6487.581 526 528 The column r.squared contains R-squared for the model, and is equal to 0.0148. To report as a percentage, multiply by 100. This means that screen_time explains 1.48% of the variance in anxiety_score. In psychological research, this is a relatively small amount of variance to explain with a model. It may still be meaningful in some contexts though (e.g., where it may be better to have a model with some predictive power rather than none at all). In simple regression, R2 is actually the squared value of the Pearson correlation between the outcome and predictor variable: # load corrr package library(corrr) # calculate the Pearson correlation between screen_time and anxiety_score mentalh %&gt;% select(screen_time, anxiety_score) %&gt;% correlate(method = &quot;pearson&quot;) term screen_time anxiety_score screen_time NA 0.1217973 anxiety_score 0.1217973 NA The correlation between screen_time and anxiety_score is r = 0.1217973. 0.1217973 * 0.1217973 = 0.0148, which is equal to the R2 value obtained with glance() 1.5.2 Bayes Factor To further evaluate the model, we can obtain a Bayes Factor. The Bayes Factor tells us how much more likely the model is than one comprising the mean of the outcome variable only. We call this baseline model the intercept-only model. It is a model in which the regression line is a flat line (i.e., has a slope equal to zero), and the predictor does not predict the outcome at all. To compute the Bayes Factor, we use lmBF() in the BayesFactor package: # load BayesFactor package library(BayesFactor) # Compute the Bayes Factor lmBF(anxiety_score ~ screen_time, data = data.frame(mentalh)) ## Bayes factor analysis ## -------------- ## [1] screen_time : 4.465124 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS The Bayes Factor is 4.47. We'd report this as BF = 4.47. This BF means that a model consisting of screen_time alone as a predictor of anxiety_score is over four times more likely than an intercept-only model (in which screen_time has a zero-slope and so does not predict anxiety_score). In other words, there's sufficient evidence to say that screen_time predicts anxiety_score. Reporting the simple regression: A simple regression was conducted to model the number of anxiety symptoms reported in the past week (anxiety score) from average hours of screen time usage per day (screen time). Screen time was found to have a positive association with anxiety scores, whereby individuals who reported greater levels of screen time also tended to have greater anxiety scores. The regression equation was \"Predicted anxiety score = 5.59 + 0.13(screen time)\", indicating that every hour of screen time use was associated with an increase in 0.13 in the anxiety score. Screen time explained 1.30% of the variance in anxiety score (adjusted R2 value). The Bayes Factor, comparing the model against an intercept-only model, was 4.47, indicating substantial evidence for the model, with it being over four times more likely than an intercept-only model. 1.6 Exercise Exercise 1.4 Is screen time predicted by age? In addition to screen time, Teychenne and Hinkley (2016) also asked participants their age in years, recorded in age in the mentalh dataset. Let's explore whether age predicts screen_time using simple regression. Adapt the code in this worksheet to do the following: 1. Produce a scatterplot of age vs. screen_time Hint Pipe mentalh to ggplot() and use geom_point() and geom_smooth(). Put the new predictor variable (age) on the x-axis and the outcome variable (screen_time) on the y-axis. Solution mentalh %&gt;% ggplot(aes(x = age, y = screen_time)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = FALSE) + xlab(&quot;Age&quot;) + ylab(&quot;Screen time (hours)&quot;) Describe the relationship between age and screen time in the scatterplot (pick one): Older individuals tend to have higher screen time scores Older individuals tend to have lower screen time scores No association between age and screen time appears to be present 2. Conduct a simple regression, with screen_time as the outcome variable, and age as the predictor variable Hint Use lm() to specify the simple regression Solution simple2 &lt;- lm(screen_time ~ age, data = mentalh) simple2 What is the value of the intercept a (to two decimal places)? What is the value of the slope b (to two decimal places)? What is the regression equation? Predicted screen time = 7.48 - 0.10(age) Predicted screen time = 0.10 - 7.48(age) Predicted screen time = 7.48 + 0.10(age) 3. Obtain R-squared Hint Make sure you have stored the regression results (e.g., in simple2), then use glance() with those results Solution glance(simple2) What proportion of variance in the screen time is explained by age? (Report the adjusted R-squared value, to two decimal places) Report the value of adjusted R-squared as a percentage, to two decimal places: The adjusted R2 value is equal to % 4. Obtain the Bayes Factor for the model Hint Use lmBF() to specify the model Solution simple2_BF &lt;- lmBF(screen_time ~ age, data = data.frame(mentalh)) simple2_BF How many times more likely is the model with age as a predictor of screen_time, compared to an intercept-only model? (to two decimal places) 5. Produce a plot of the fitted (predicted) values against the residuals Hint Use augment() with ggplot() and geom_point() Solution augment(simple2) %&gt;% ggplot(aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0) + geom_smooth() What type of trend is evident between the predicted values and the residuals? Further interpretation No association is apparent, but the points above the line appear to be more spread out than the points below the horizontal line. This indicates that the model tends to underestimate some of the screen time scores. This could be because the screen time scores are positively skewed, e.g., see mentalh %&gt;% ggplot(aes(screen_time)) + geom_density(), and therefore taking the log transform of the scores prior to analysis may improve this plot (though may not necessarily change the outcome of the analysis). 6. On balance, does age seem to be a good predictor of a person's daily screen time use? No Yes Cannot determine Explanation Yes, the older the individuals were, the lower the screen time score tended to be. A model with age as a predictor of screen time explained only 1.68% of the variance in screen time scores (adjusted R2), but the Bayes Factor (BF = 12.19) indicated strong evidence for this model compared to an intercept-only model. The regression equation was \"Predicted screen time = 7.48 - 0.10(age)\", indicating that an increase in age of one year was associated with a reduction of approximately 6 minutes (i.e., one tenth of 1 hour) of screen time per week. 1.7 Further Exercises For those feeling confident with everything so far. Exercise 1.5 Further Exercise The variable physical_activity in the mentalh dataset is a measure of moderate-to-vigorous physical activity, based on participant's self reported weekly activity. To what extent is participants' anxiety_score explained by their physical_activity? Investigate by producing the following: Scatterplot Correlation Simple regression model Adjusted R-squared value Bayes Factor On balance, does the anxiety_score seem to be predicted by physical_activity? Solution: code # scatterplot mentalh %&gt;% ggplot(aes(x = physical_activity, y = anxiety_score)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = F) + xlab(&quot;Physical activity&quot;) + ylab(&quot;Anxiety score&quot;) + theme_classic() # correlation mentalh %&gt;% select(anxiety_score, physical_activity) %&gt;% correlate() # simple regression model model_activity &lt;- lm(anxiety_score ~ physical_activity, data = mentalh) # look at intercept and slope model_activity # look at plot of fitted values and residuals augment(model_activity) %&gt;% ggplot(aes(x=.fitted, y=.resid)) + geom_point() + geom_hline(yintercept = 0) # look at R-squared glance(model_activity) # calculate Bayes Factor lmBF(anxiety_score ~ physical_activity, data = mentalh) Solution: interpretation No, there's no evidence that the anxiety scores are predicted by self reported measures of moderate-to-vigorous levels of physical activity. The two measures showed virtually no correlation, r = -0.01. The regression equation was Predicted Anxiety Score = 6.23 - 0.0001(physical activity), and the model explained no variance in anxiety score with the adjusted R2 = -0.0017. The Bayes Factor was equal to 0.10. Given that this value of the Bayes Factor is less than 0.33, this indicates substantial evidence for the intercept-only model, compared to the simple regression model where physical activity is the sole predictor of anxiety scores. In other words, if we only had these two variables, the best predictor of anxiety scores would be the mean value of the anxiety scores. Interestingly, although there appears to be no relationship between anxiety and physical activity in this sample of individuals (mothers), other populations do apparently show reductions in anxiety with greater levels of vigorous physical activity (e.g., in adolescents, see Hrafnkelsdottir et al., 2018). 1.8 Summary Simple regression can be used to model the relationship between an outcome and predictor variable, where both variables are continuous. Once obtained, the regression equation allows us to: precisely describe the relationship between the outcome and predictor variables (whether positive or negative). derive predictions for the outcome variable, given new values of the predictor variable. evaluate the model with R2 and use Bayes factor to compare how much more likely it is to an intercept-only model Key functions Visualise the data: ggplot() Simple regression: lm() R2: glance() Residuals: augment() Bayes Factor: lmBF() 1.9 References Hrafnkelsdottir S.M., Brychta R.J., Rognvaldsdottir V., Gestsdottir S., Chen K.Y., Johannsson E., et al. (2018) Less screen time and more frequent vigorous physical activity is associated with lower risk of reporting negative mental health symptoms among Icelandic adolescents. PLoS ONE 13(4): e0196286. https://doi.org/10.1371/journal.pone.0196286 Teychenne M, &amp; Hinkley T (2016) Associations between screen-based sedentary behaviour and anxiety symptoms in mothers with young children. PLoS ONE, 11(5): e0155696. https://doi.org/10.1371/journal.pone.0155696 "],["multiple1.html", "Session 2 Multiple regression: multiple continuous predictors 2.1 Overview 2.2 Worked example 2.3 Understanding the contribution of individual predictors 2.4 Multicollinearity 2.5 Exercise 2.6 Further exercises 2.7 Summary of key points 2.8 References", " Session 2 Multiple regression: multiple continuous predictors Chris Berry 2022 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} 2.1 Overview Slides from the lecture part of the session: Download This worksheet assumes you have gone through the previous one on simple regression. When we want to determine the extent to which an outcome variable (e.g., psychological wellbeing) is predicted by multiple continuous predictors (e.g., both worry and mindfulness scores), we can use multiple regression. Adding multiple predictors to a model may serve to improve the prediction of the outcome variable. It can also be a way to test specific theories or hypotheses. Simple vs. Multiple Regression Simple regression is a linear model of the relationship between one outcome variable and one predictor variable. For example, can we predict wellbeing on the basis of worry scores? Multiple regression is a linear model of the relationship between one outcome variable and more than one predictor variable. For example, can we predict wellbeing based on worry, mindfulness, and emotional intelligence scores? 2.2 Worked example Iani et al. (2019) looked at factors associated with psychological wellbeing and distress in 66 individuals with generalised anxiety disorder. For educational purposes, we'll focus on a subset of their data, namely whether wellbeing is predicted by worry and describing scores in a multiple regression. Describing is the mindfulness skill of being able to describe one's inner experiences and feelings with words. Read the data to R. The data are stored at: https://raw.githubusercontent.com/chrisjberry/Teaching/master/2_wellbeing_data.csv # First ensure tidyverse is loaded, i.e., &#39;library(tidyverse)&#39; # read in the data using read_csv(), store in wellbeing_data wellbeing_data &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/2_wellbeing_data.csv&#39;) # preview the data with glimpse() wellbeing_data %&gt;% glimpse() We'll use these three variables in the dataset: wellbeing: Higher scores indicate higher wellbeing. worry: Higher scores indicate higher levels of worry. describing: Higher scores indicate higher self-reported ability to describe one's inner experiences. (Note. The data are publicly available, but I've changed the variable names for clarity. As in Iani et al. (2019), missing values were replaced with the mean of the relevant variable.) Visualise the data with a scatterplot. Place the outcome variable wellbeing on the y-axis, the predictor worry on the x-axis, and let the size of each point represent the second predictor, the describing score: # Scatterplot of all three variables # Use alpha to alter transparency of points # to make overlap easier to see wellbeing_data %&gt;% ggplot(aes(x = worry, y = wellbeing, size = describing)) + geom_point(alpha = 0.6, colour = &quot;cornflowerblue&quot;) Figure 2.1: Scatterplot of wellbeing scores vs. worry and describing Exercise 2.1 From inspection of the scatterplot: Greater worry scores tend to be associated with lowerconstanthigher wellbeing scores. Explain A negative trend between worry and wellbeing is evident in the scatterplot Greater describing scores tend to be associated with lowerconstanthigher wellbeing scores. Explain The size of the describing points tend to be larger when wellbeing scores are higher. The above trends are also apparent in the Pearson correlations between variables: # select the relevant columns from wellbeing_data and # use correlate() to obtain the correlations wellbeing_data %&gt;% select(wellbeing, worry, describing) %&gt;% correlate(method = &quot;pearson&quot;) term wellbeing worry describing wellbeing NA -0.5419352 0.5356548 worry -0.5419352 NA -0.2477970 describing 0.5356548 -0.2477970 NA The correlation between wellbeing and worry (to 2 decimal places) is r = The correlation between wellbeing and describing (to 2 decimal places) is r = The correlation between the two predictors (worry and describing) (to 2 decimal places) is r = Multiple regression using lm() To include more than one predictor in a regression model, use the + symbol when specifying the model with lm(): lm(outcome ~ predictor_1 + predictor_2 + predictor_3.... , data = mydata) This runs a model of the form: \\(Predicted \\ outcome = a + b_1(Predictor \\ 1) + b_2(Predictor \\ 2) + b_3(Predictor \\ 3) ...\\) Note that we don't need to specify the intercept a in lm() since it is included automatically by R (as is the case with simple regression). # conduct a multiple regression, store it in multiple1 multiple1 &lt;- lm(wellbeing ~ worry + describing, data = wellbeing_data) # look at the coefficients multiple1 ## ## Call: ## lm(formula = wellbeing ~ worry + describing, data = wellbeing_data) ## ## Coefficients: ## (Intercept) worry describing ## 70.7306 -0.7708 1.2484 (Intercept) is the value of the intercept a in the regression equation. Type to two decimal places: worry is the value of the coefficient \\(b_1\\) for the worry predictor. describing is the value of the coefficient \\(b_2\\) for the describing predictor. The regression equation is therefore written as: \\(Predicted\\ wellbeing = 70.73 - 0.77(worry) + 1.25(describing)\\) 2.2.1 Residual plot We can obtain a plot of the predicted values vs. the residuals in the same way as for simple regression by using augment() in the broom package. augment(multiple1) %&gt;% ggplot(aes(x = .fitted, y = .resid)) + geom_hline(yintercept = 0) + geom_point() Figure 2.2: Scatterplot of the predicted values vs. residuals The points seem randomly and evenly distributed around the horizontal, in line with assumptions of homoscedasticity (equal variance of residuals at each predicted value), and independence of residuals. 2.2.2 Evaluating the model: Bayes Factor The Bayes Factor tells us how many more times likely the multiple regression model is, relative to the intercept-only model. Use lmBF() to obtain the Bayes Factor for the multiple regression model: # store the BF for the model in multiple1_BF multiple1_BF &lt;- lmBF(wellbeing ~ worry + describing, data = data.frame(wellbeing_data)) # show the BF multiple1_BF ## Bayes factor analysis ## -------------- ## [1] worry + describing : 4190994 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS The Bayes Factor for the model is . This tells us that the model with worry and describing is over four million times more likely than an Intercept only model (one with no predictors). Thus, there's strong evidence that the multiple regression model explains wellbeing. 2.2.3 Evaluating the model: R2 R2 tells us how much variance in the outcome variable is explained by the multiple regression model. Use glance() in the broom package to obtain the R2 for the model: glance(multiple1) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.4653263 0.4483526 9.393314 27.41444 0 2 -239.9547 487.9093 496.6679 5558.763 63 66 r.squared is R2, the proportion of variance in wellbeing explained by the model. Thus, the model explains 0.4653, or 46.53% of the variance in wellbeing. adj.r.squared is the Adjusted R2 value, which is R2 adjusted for the sample size and the number of predictors in the model. It is an estimate of R2 for the population (not merely the scores we have in the sample), and is always less than R2. You'll see researchers reporting either R2 or the adjusted R2 in the literature. If you're not sure which one to use, report the adjusted R2, and say so (e.g., \"adjusted R2 = 44.83%\"). The adjusted R2 value is 0.4483, so in a report we could say that a model with worry and describing explains 44.83% of the variance in wellbeing. 2.3 Understanding the contribution of individual predictors Because predictor variables are often correlated to a degree, some of the variance they explain in the outcome will be shared. A predictor's contribution to a model must therefore only be interpreted after the other predictors in the model have been taken into account. This is explained in more detail below. 2.3.1 R2 in simple vs. multiple regression In a simple regression of wellbeing ~ worry, the variance in wellbeing explained by worry is R2 = 29.37%: s1 &lt;- lm(wellbeing ~ worry, data = wellbeing_data) glance(s1) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.2936938 0.2826578 10.71152 26.61226 2.6e-06 1 -249.1416 504.2832 510.8522 7343.15 64 66 In a simple regression of wellbeing ~ describing, the variance in wellbeing explained by describing is R2 = 28.69% s2 &lt;- lm(wellbeing ~ describing, data = wellbeing_data) glance(s2) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.286926 0.2757842 10.76272 25.75226 3.6e-06 1 -249.4563 504.9126 511.4816 7413.512 64 66 So, by looking at the (non-adjusted) R2 values of our simple and multiple regressions so far, an interesting pattern is evident: In the simple regression of wellbeing ~ worry, R2 = 29.37% In the simple regression of wellbeing ~ describing, R2 = 28.69% Yet, in a multiple regression of wellbeing ~ worry + describing, R2 = 46.56%, which is less than the sum of R2 from the simple regressions (29.37 + 28.69 = 58.06%). The reason why the R2 values from the simple regressions don't add up to the same value as the R2 for the multiple regression is because worry and describing are correlated (r = -0.25, as you obtained earlier). This means that some of the variance that they explain in wellbeing is shared. 2.3.2 Venn Diagrams Venn diagrams are useful for understanding the variance that predictors explain in the outcome variable. They are especially useful for understanding R2 in multiple regression. Here, I use them to explain why the R2 values from simple regressions don't necessarily add up to R2 when the same predictors are in a multiple regression. Suppose the rectangle below represents all of the variance in wellbeing to be explained. The area of the circle below represents the variance in wellbeing explained by worry in the first simple regression. If this diagram were drawn to scale (it's not!), the area of the circle would be equal to the value of \\(R^2\\) (i.e., 29.37%). The part of the rectangle not inside the circle represents the variance in wellbeing that is not explained by the model (i.e., the unexplained or residual variance). We'll now add describing to the model with worry. We could represent this on a Venn diagram as follows: The correlation is represented as an overlap in the circles. Their total area (R2 for the multiple regression model = 46.56%) is less than the area they'd explain if there were no overlap (58.06%). This highlights an important point: Predictors are often correlated to some degree, so in multiple regression it only really makes sense to talk about the contribution a predictor makes in the context of the other predictors in the model. That is, a given predictor explains variance in the outcome variable only after the other predictors in the model have been taken into account. Given that the unique contributions of worry and describing are lower in a multiple regression model, we must ask whether there's evidence that each predictor makes a unique contribution, over an above the other predictor. For example, is there sufficient evidence for a unique contribution of worry, once describing has been taken into account? If not, we probably wouldn't include it in the model. 2.3.3 Using Bayes factors to assess the unique contribution of predictors We can compare Bayes Factors to determine whether a given predictor in a multiple regression model makes a unique contribution to the prediction of the outcome variable. First, obtain the Bayes Factor for the model in which predictor_1 has been left out of the full model. Then obtain the Bayes Factor for model in which predictor_2 has been left out of the model. Repeat this for as many predictors you have in the model. In our example, this involves obtaining the BFs of the model of wellbeing ~ describing, and the BF of the model of wellbeing ~ worry: # BF for wellbeing ~ describing describing_BF &lt;- lmBF(wellbeing ~ describing, data = data.frame(wellbeing_data)) # BF for wellbeing ~ worry worry_BF &lt;- lmBF(wellbeing ~ worry, data = data.frame(wellbeing_data)) We also need the BF for the full model: # BF for wellbeing ~ worry + describing # (this is the same as multiple1_BF above, but # with a different name, which will help us see what&#39;s going on later) worry_describing_BF &lt;- lmBF(wellbeing ~ worry + describing, data = data.frame(wellbeing_data)) Comparing models We can use the following general formula to determine whether there's evidence for a more complex version of a model, relative to a simpler model: BF_more_complex_model / BF_simpler_model That is, we divide the BF for the more complex model by the BF for the simpler one. This then tells us how many more times more likely the more complex model is, relative to the simpler one. For example, if BF_more_complex_model = 10 and BF_simpler_model = 2, then the more complex model is five times more likely than the simpler one (because 10 / 2 = 5). There'd be substantial evidence to prefer the more complex model. In our case, we can compare Bayes Factor of the full model (worry_describing_BF) with that of our simpler models (describing_BF) and (worry_BF) in order to determine whether each predictor makes a unique contribution to the full model or not. To determine if there's evidence for the unique contribution of worry to the model: # compare BFs for the full model and one in which worry is left out worry_describing_BF / describing_BF ## Bayes factor analysis ## -------------- ## [1] worry + describing : 981.4912 ±0% ## ## Against denominator: ## wellbeing ~ describing ## --- ## Bayes factor type: BFlinearModel, JZS The BF for the comparison is 981.49, indicating that there's substantial evidence that the addition of worry to a model containing describing improves the model. In other words, there's evidence for a unique contribution of worry to the prediction of wellbeing. To determine if there's evidence for the unique contribution of describing to the model: # compare the BFs for the full model and one in which describing is left out worry_describing_BF / worry_BF ## Bayes factor analysis ## -------------- ## [1] worry + describing : 738.4095 ±0% ## ## Against denominator: ## wellbeing ~ worry ## --- ## Bayes factor type: BFlinearModel, JZS The BF comparing the full model with one containing worry alone is 738.41. This indicates that the model with both worry and describing is over seven hundred times more likely than the model with worry alone. The BF is greater than 3, so this indicates substantial evidence for the unique contribution of describing to the prediction of wellbeing. Thus, in a multiple regression model, there's substantial evidence that both worry and describing predict wellbeing overall (BF = 4,190,994). There's also substantial evidence that worry (BF = 981.49) and describing (BF = 738.41) make unique contributions to this model. The model with both predictors included therefore seems justified. 2.4 Multicollinearity If the correlation between predictors is very high (greater than r = 0.8 or less than -0.8), this is known as multicollinearity. Multicollinearity can be a problem in multiple regression. Predictors may explain a large amount of variance in the outcome variable, but their 'unique' contribution in a multiple regression may be small. In an extreme scenario, neither predictor makes a unique contribution, even though the overall regression explains a large amount of variance in the outcome variable! On a Venn diagram, the circles representing the predictors would almost completely overlap: Multicollinearity should be avoided. Therefore, check for extreme correlations between your predictor variables before including them in a multiple regression. The correlation between the predictor variables in the model of wellbeing ~ worry + describing was , and multicollinearity was therefore of concernof no concernnot possible to determine. How many predictor variables should be included in a model? If adding predictors to the regression improves the prediction of the outcome variable, you may think that we can simply keep adding in variables to the model, until all the residual variance has been explained. This seems fine until we learn that if we were to add as many predictors to the model as there are rows in our data (e.g., 66 participants inwellbeing_data), then we'd perfectly predict the outcome variable, and have a (non-adjusted) \\(R^2\\) of 100%! This would be true even if the predictors consisted of random values. Our model would clearly be meaningless though. We ideally want to explain the outcome variable with relatively few predictors, or for our selection of predictors to be guided by specific hypotheses or theory. 2.5 Exercise Exercise 2.2 Main Exercise The wellbeing_data also contain these columns: gad : Severity of symptoms of GAD (Generalised Anxiety Disorder). Higher scores indicate greater severity of symptoms. brooding : Higher scores indicate higher levels of brooding (i.e., being preoccupied with depressing, morbid, or painful memories or thoughts). observing : Higher scores indicate greater tendency to notice things in one's environment (e.g., smells, sounds, visual elements). It's another measure of mindfulness. Use multiple regression to investigate the extent to which brooding and observing predict gad. Adapt the code in this worksheet to do the following things (and try not to look at the solution until you've attempted the question): 1. Visualise the relationship between gad, brooding and observing in a scatterplot. Hint Pipe wellbeing_data to ggplot() and use geom_point(). Use the size option to specify the second predictor variable in aes() Solution wellbeing_data %&gt;% ggplot(aes(x = brooding, y = gad, size = observing)) + geom_point() + xlab(&quot;Brooding&quot;) + ylab(&quot;GAD severity of symptoms score&quot;) + theme_classic() Figure 2.3: GAD vs. brooding and observing scores In the scatterplot, observing could have also been swapped around with brooding. Try both ways. Greater brooding scores tend to be associated with lowerno appreciable change inhigher gad scores. Greater observing scores tend to be associated with lowerno appreciable change inhigher gad scores. 2. Obtain the correlations between gad, brooding, and observing Hint Pipe the wellbeing_data to select() and use correlate() Solution wellbeing_data %&gt;% select(gad, brooding, observing) %&gt;% correlate(method = &quot;pearson&quot;) State the correlations to two decimal places: The correlation between the GAD score and brooding is r = The correlation between the GAD score and observing is r = The correlation between brooding and observing is r = Is multicollinearity a concern between the two predictor variables? yesnocannot determine Explanation The correlation between the predictor variables is r = -.18. Although they are weakly correlated, this does not exceed r = -.80, and therefore multicolinearity is not a concern. 3. Conduct a multiple regression, with gad as the outcome variable, and brooding and observing as the predictor variables Hint Use lm() to specify the simple regression. Store it in multiple2. Solution multiple2 &lt;- lm(gad ~ brooding + observing, data = wellbeing_data) multiple2 What is the value of the intercept a (to two decimal places)? What is the value of the coefficient for brooding (to two decimal places)? What is the value of the coefficient for observing (to two decimal places)? What is the regression equation? Predicted GAD score = 0.07 - 0.89(brooding) + 0.02(observing) Predicted GAD score = 0.07 + 0.02(brooding) - 0.89(observing) Predicted GAD score = 0.07 + 0.89(brooding) - 0.02(observing) 4. Obtain R2 for the model Hint Make sure you have stored the regression results (e.g., in mutliple2), then use glance() with that variable. Solution glance(multiple2) What proportion of variance in the GAD score is explained by the model containing brooding and observing? (Report the adjusted R-squared value as a proportion, to two decimal places) Report the value of adjusted R-squared as a percentage, to two decimal places: The adjusted R2 value is equal to % 5. Obtain the Bayes Factor for the model Hint Use lmBF() to specify the multiple regression model Solution multiple2_BF &lt;- lmBF(gad ~ brooding + observing, data = data.frame(wellbeing_data)) How many times more likely is the model with brooding and observing as a predictor of gad, compared to an intercept-only model? (to two decimal places) 6. Plot the predicted values against the residuals. Hint Use augment() with ggplot() and geom_point() Solution augment(multiple2) %&gt;% ggplot(aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0) What type of trend is evident between the predicted values and the residuals positive trendthere's no associationnegative association Further interpretation Although there's a slight negative association for lower predicted values, the assumptions of homoscedasticity and independence of residuals appear to be met. You could also try adding the code + geom_smooth(method = \"lm\", se = FALSE) to your plot to see what the (linear) trend line would actually be. 7. Using lmBF(), determine whether there's evidence for a unique contribution of brooding and observing to prediction of gad The BF associated with the unique contribution of observing is The BF associated with the unique contribution of brooding is Hint 1 Use lmBF() to obtain the BF for both simple regressions and the multiple regression. Then use the general formula BF_more_complex_model / BF_simpler_model to obtain the BF for the unique contribution of each predictor. Hint 2 To get the BF for the two simple regressions: BF_brooding &lt;- lmBF(gad ~ brooding, data = data.frame(wellbeing_data)) BF_observing &lt;- lmBF(gad ~ observing, data = data.frame(wellbeing_data)) The multiple regression BF (same as multiple2_BF): BF_brooding_observing &lt;- lmBF(gad ~ brooding + observing, data = data.frame(wellbeing_data)) Then the BFs for relevant models need to be compared, using: BF_more_complex_model / BF_simpler_model Solution # BF for gad ~ brooding BF_brooding &lt;- lmBF(gad ~ brooding, data = data.frame(wellbeing_data)) # BF for gad ~ observing BF_observing &lt;- lmBF(gad ~ observing, data = data.frame(wellbeing_data)) # BF for full model BF_brooding_observing &lt;- lmBF(gad ~ brooding + observing, data = data.frame(wellbeing_data)) # BF for the unique contribution of observing BF_brooding_observing / BF_brooding # BF for the unique contribution of brooding BF_brooding_observing / BF_observing 8. On balance, is there evidence for a model containing both brooding and observing as predictors of GAD scores? Hint Look at BF for the unique contributions. If the BF is greater than 3, this indicates substantial evidence for the inclusion of that predictor in the model. If the BF is less than 0.33, then there's substantial evidence against its inclusion (the simpler model is preferable). Intermediate BFs are inconclusive, and the predictor should therefore be left out. Solution Together, brooding and observing explain 34.50% of the variance in GAD scores (adjusted R2), and there was strong evidence for this model, compared to an intercept only model, BF = 26889.69. There was, however, no evidence for a unique contribution of observing to the model (BF = 0.19), but there was substantial evidence for a unique contribution of brooding to the model, BF = 741841.28, whereby greater GAD scores were associated with higher brooding scores. In the interest of parsimony (not making models more complex than they need to be), the results suggest that only brooding predicts GAD and observing should not be included in the model that also contains brooding. 2.6 Further exercises Not essential. For those confident with everything. 2.6.1 regressionBF() When determining the BF for the unique contribution of a predictor, as a shortcut, we could have used regressionBF(), rather than using lmBF() multiple times. regressionBF() automatically calculates the BFs for all permutations of a set of predictors in a model. For our first model of wellbeing on the basis of worry and describing: # obtain BFs for all permutations of the model predictors all_BFs &lt;- regressionBF(wellbeing ~ worry + describing, data = data.frame(wellbeing_data)) # look at the BFs all_BFs ## Bayes factor analysis ## -------------- ## [1] worry : 5675.703 ±0% ## [2] describing : 4270.027 ±0% ## [3] worry + describing : 4190994 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS \"[1]\" is the BF for the model with worry as the sole predictor of wellbeing \"[2]\" is the BF for the model with describing as the sole predictor of wellbeing \"[3]\" is the BF for the model with worry and describing as predictors Comparing the BF for [3] and [2] will tell us whether there's evidence that worry makes a unique contribution. # compare multiple regression with simple regression 2 all_BFs[3] / all_BFs[2] ## Bayes factor analysis ## -------------- ## [1] worry + describing : 981.4912 ±0% ## ## Against denominator: ## wellbeing ~ describing ## --- ## Bayes factor type: BFlinearModel, JZS The BF for this comparison is 981.49, which matches the BF for the unique contribution of describing calculated earlier. Comparing the BF for [3] and [1] will therefore tell us whether there's evidence that describing makes a unique contribution to the model: # compare the BF for [3] and [1] all_BFs[3] / all_BFs[1] ## Bayes factor analysis ## -------------- ## [1] worry + describing : 738.4095 ±0% ## ## Against denominator: ## wellbeing ~ worry ## --- ## Bayes factor type: BFlinearModel, JZS The BF for this comparison is 738.41, which matches the BF for the unique contribution of describing calculated earlier. Thus, the BFs derived by this method are the same as when we used lmBF() in the main section of the worksheet. Exercise 2.3 Further exercise 1 Use regressionBF() to obtain the BFs for the unique contribution of brooding and observation in the model in the Main Exercise (i.e., gad ~ brooding + observation), and check that they are the same as the ones produced when you used lmBF() 2.6.2 Predicting new data As with simple regression, we can use augment() to predict what the outcome variable would be, given new data. For example, for a new individual with a worry score of 20 and describing score of 15: # specify the new data (for each predictor in the model) new_scores &lt;- tibble(worry = 20, describing = 15) # use augment() in the broom package to obtain the prediction library(broom) augment(multiple1, newdata = new_scores) worry describing .fitted 20 15 74.03992 .fitted contains the predicted wellbeing score, and is equal to 74.04. To derive predictions for several new participants, use c(score1, score2...) when specifying the new_scores. To add another individual with a worry score of 25 and describing score of 10 # specify the new data (for both predictors) new_scores &lt;- tibble(worry = c(20, 25), describing = c(15, 10)) # use augment() in the broom package for the predictions augment(multiple1, newdata = new_scores) worry describing .fitted 20 15 74.03992 25 10 63.94410 Each row contains the details for a new person. The .fitted column contains the predicted wellbeing scores: 74.04 and 63.94. Exercise 2.4 Predicting data For the model in the main exercise (i.e., gad ~ brooding + observing), what are the predicted values of gad for three new individuals with brooding scores of 10, 15, and 20, and observing scores of 3, 6 and 9, respectively. Predicted GAD of participant 1 (to one decimal place) = Predicted GAD of participant 2 (to one decimal place) = Predicted GAD of participant 3 (to one decimal place) = Solution new_scores &lt;- tibble(brooding = c(10, 15, 20), observing = c(3, 6, 9)) multiple2 &lt;- lm(gad ~ brooding + observing, data = wellbeing_data) augment(multiple2, newdata = new_scores) Exercise 2.5 Further exercise Not essential. Only for if you are really confident with everything. The variables attention, clarity and repair in the wellbeing_data are measures of emotional intelligence. To what extent do these three predictors uniquely explain wellbeing in a multiple regression? The BF for the unique contribution of attention = The BF for the unique contribution of clarity = The BF for the unique contribution of repair = Hint Obtain the BF for the full model and the BFs for the models in which each of the predictors have been left out. Compare the BF for the full model with that of the model with the predictor left out in order to obtain the BF for the unique contribution of that predictor. Solution Check correlations: wellbeing_data %&gt;% select(wellbeing, attention, clarity, repair) %&gt;% correlate(method = &quot;pearson&quot;)   Run multiple regression: multiple3 &lt;- lm(wellbeing ~ attention + clarity + repair, data = wellbeing_data) Adjusted R2 for the overall model: glance( multiple3 ) Using lmBF(): BF_attention_clarity_repair &lt;- lmBF(wellbeing ~ attention + clarity + repair, data = data.frame(wellbeing_data)) BF_attention_clarity &lt;- lmBF(wellbeing ~ attention + clarity, data = data.frame(wellbeing_data)) BF_attention_repair &lt;- lmBF(wellbeing ~ attention + repair, data = data.frame(wellbeing_data)) BF_clarity_repair &lt;- lmBF(wellbeing ~ clarity + repair, data = data.frame(wellbeing_data)) # unique contribution of attention BF_attention_clarity_repair / BF_clarity_repair # unique contribution of clarity BF_attention_clarity_repair / BF_attention_repair # unique contribution of repair BF_attention_clarity_repair / BF_attention_clarity Using regressionBF(): all_BFs &lt;- regressionBF(wellbeing ~ attention + clarity + repair, data = data.frame(wellbeing_data)) # look at output all_BFs # unique contribution of attention all_BFs[7] / all_BFs[6] # unique contribution of clarity all_BFs[7] / all_BFs[5] # unique contribution of repair all_BFs[7] / all_BFs[4] 2.7 Summary of key points Predictors can be added to a model in lm using the + symbol e.g., lm(wellbeing ~ worry + describing + predictor_3 + ....) Predictor variables are often correlated to some extent. This can affect the interpretation of individual predictor variables. Venn diagrams can help to understand the unique contributions of predictors In multiple regression, it's important to understand that each predictor makes a contribution to explaining the outcome variable only after taking into account the other predictors in the model. The Bayes Factor for a multiple regression model tells us how much more likely it is, compared to an intercept-only model. To know whether there's evidence for the unique contribution of a predictor, we have to compare Bayes Factors for the more complex model (which includes it), versus the simpler model (in which the predictor is left out). i.e., BF_more_complex_model / BF_simpler_model. Multicollinearity exists when predictors are highly correlated (r above 0.8 or below -0.8) and should be avoided by dropping one of the predictors. 2.8 References Iani, L., Quinto, R. M., Lauriola, M., Crosta, M. L., &amp; Pozzi, G. (2019). Psychological well-being and distress in patients with generalized anxiety disorder: The roles of positive and negative functioning. PloS one, 14(11), e0225646. https://doi.org/10.1371/journal.pone.0225646 "],["anova1.html", "Session 3 ANOVA: between-subjects designs 3.1 Overview 3.2 One-way between subjects ANOVA 3.3 Two-way between-subjects ANOVA 3.4 Exercise: one-way 3.5 Exercise: two-way 3.6 Summary 3.7 References", " Session 3 ANOVA: between-subjects designs Chris Berry 2022 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} 3.1 Overview Slides from the lecture part of the session: Download So far we have used regression where both the outcome and predictor are continuous variables. When all the predictor variables in a regression are categorical, the analysis is called ANOVA, which stands for Analysis Of VAriance. Here we consider two types of ANOVA for between-subjects designs: one-way ANOVA and two-way ANOVA. We will consider other types in future sessions (e.g., for within-subjects/repeated measures designs). 3.2 One-way between subjects ANOVA A one-way between subjects ANOVA is used to compare the scores from a dependent variable across different groups of individuals. For example, do mood scores differ between three groups of individuals, where each group undergoes a different type of therapy as treatment? one-way means that there is one independent variable, for example, type of therapy. Independent variables are also called factors in ANOVA. A factor is made up of different levels. Type of therapy could have three levels: CBT (Cognitive Behavioural Therapy), EMDR (Eye Movement Desensitisation and Reprocessing), and Control. between subjects means that a different group of participants gives us mood scores for each level of the independent variable. For example, if type of therapy is manipulated between-subjects, one group receives CBT, another group receives EMDR, and another group are the control group. Each participant provides exactly one score. 3.2.1 Worked Example What is the effect of viewing pictures of different aesthetic value on a person's mood? To investigate, Meidenbauer et al. (2020) showed three groups of participants pictures of urban environments that were either very low in aesthetic value (n = 102), low in aesthetic value (n = 100), or high (n = 104). Participants' change in State Trait Anxiety Inventory (STAI: a measure of negative symptoms such as upset, tense, worried) as a result of viewing the pictures was measured. Exercise 3.1 Design check. What is the independent variable (or factor) in this design? change in STAI scoreaesthetic appeal of the pictures How many levels does the factor have? 1234 What is the dependent variable? VeryLow, Low, Highchange in STAI scoreaesthetic appeal of images What is the nature of the independent variable? categoricalcontinuous Is the independent variable manipulated within- or between-subjects? between-subjectswithin-subjects 3.2.2 Read in the data Read in the data at the link below and store in affect_data. Preview the data using head(). https://raw.githubusercontent.com/chrisjberry/Teaching/master/3_affect.csv (Note. The data in are taken from the Urban condition of Meidenbauer et al. (2020, Experiment 1). The data are publicly available through the links in their article. The variable names have been changed here for clarity.) # Read in the data affect_data &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/3_affect.csv&#39;) # look at the first 6 rows affect_data %&gt;% head() Key variables of interest in affect_data: group: the aesthetic value group, with levels VeryLow, Low and High score: the change in STAI score. Higher scores indicate fewer negative symptoms after viewing the images (i.e., improvement in mood). Variable labels Notice that the group column is by default read into R as a character variable (that's what the label &lt;chr&gt; means in the output). This is because the levels of group have been recorded in the dataset as words e.g., \"VeryLow\". 3.2.3 Convert the independent variable to a factor To enable us to use the group column in an ANOVA, we need to tell R that group is a factor. Use mutate() and factor() to convert group to a factor. # use mutate() to convert the group variable to a factor # store the changes back in affect_data # (i.e., overwrite what&#39;s already in affect_data) affect_data &lt;- affect_data %&gt;% mutate(group = factor(group)) Remember mutate() can be used to change variables or create new ones. The code mutate(group = factor(group)) means: group =: create a variable called group. Because group already exists in affect_data it will be overwritten. factor(group): convert the group variable to a factor. Tip If we'd have used: factor(group, levels = c(\"VeryLow\",\"Low\",\"High\")) instead of factor(group) this would mean that the levels of group will additionally be ordered according to the order in levels. This can be useful when plotting the data. Check Check that the variable label for group has now changed from &lt;chr&gt; to &lt;fct&gt; (i.e., a factor) by looking at the dataset again. affect_data 3.2.4 n of each group Use group_by() and count() to obtain the number of participants in each group: # use group_by() to group the output by &#39;group&#39; column, # then obtain number of rows in each group with count() affect_data %&gt;% group_by(group) %&gt;% count() How many participants were there in the VeryLow aesthetic value group? How many participants were there in the Low aesthetic value group? How many participants were there in the High aesthetic value group? 3.2.5 Visualise the data The way the data are distributed in each group can be inspected with histograms or density plots: # Histogram of scores in each group # Use facet_wrap(~group) to create a separate # panel for each group affect_data %&gt;% ggplot(aes(score)) + geom_histogram() + facet_wrap(~group) Figure 1.2: Histogram of scores in each aesthetic value group Tip To produce density plots, swap geom_histogram() with geom_density(). Comments on the histograms The spread of scores in each group appears relatively similar, suggesting the assumption of homogeneity of variance in ANOVA, may be met. The distributions are reasonably symmetrical, and, aside from the very high number of 0 scores in each group (indicative of zero change in STAI score in a large number of individuals), the data appear approximately normally distributed. In practice, ANOVA is considered reasonably robust against violations of the test's assumptions (see e.g., Glass et al. 1972, Schmider et al., 2010). 3.2.6 Plot the means Visualise the data further by obtaining a plot of the mean score in each group. The package ggpubr can produce high quality plots with ease. Using ggerrorplot() in ggpubr: # load the ggpubr package library(ggpubr) # plot the mean of each group # specify desc_stat = &quot;mean_se&quot; to # add error bars representing the standard error affect_data %&gt;% ggerrorplot(x = &quot;group&quot; , y = &quot;score&quot;, desc_stat = &quot;mean_se&quot;) + xlab(&quot;Aesthetic value group&quot;) Figure 3.1: Mean change in STAI score across aesthetic value groups (error bars indicate SE) From inspection of the means: Which aesthetic value group has the greatest improvement in STAI score? HighLowVeryLow Which aesthetic value group has the lowest improvement in STAI score? HighLowVeryLow Did STAI scores appear to worsen (i.e., be below zero) in any group as a result of viewing the images? High groupLow groupVeryLow group Developing the plot As with plots generated in ggplot(), the figure can be enhanced by adding further code, e.g., try adding the line: + ylab(\"Change in STAI (negative symptoms)\") Beneath the surface, ggpubr uses ggplot() to make graphs. Other types of plot are available in ggpubr, see e.g.: Errorplots: ?ggerrorplot() Boxplots: ?ggboxplot() Violin plots: ?ggviolin() I encourage you to play around to find clear and effective ways to visualise your data! To see more types of plot: help(package = ggpubr) 3.2.7 Descriptives: Mean of each group Use summarise() and group_by() to obtain the mean (M) in each group: affect_data %&gt;% group_by(group) %&gt;% summarise(M = mean(score)) The mean score in the High group is (to 2 decimal places) The mean score in the Low group is (to 2 decimal places) The mean score in the VeryLow group is (to 2 decimal places) Tip - Standard Error The formula for the standard error of the mean is \\(SD / \\sqrt{n}\\). We can therefore obtain the standard error of the mean for each group as follows: affect_data %&gt;% group_by(group) %&gt;% summarise(SE = sd(score) / sqrt( n() )) To show the mean and SE in the same output: affect_data %&gt;% group_by(group) %&gt;% summarise( M = mean(score), SE = sd(score) / sqrt( n() ) ) 3.2.8 Bayes factor A Bayes factor can be obtained for the one-way ANOVA model using anovaBF(). The model we specify is of score on the basis of group (i.e., score ~ group). The BF will tell us how much more likely the model (with different three groups) is than an intercept-only model, in which all the scores are treated as coming from one large group. In other words, the BF will tell us whether we have evidence for an effect of aesthetic appeal on the STAI scores or not. To obtain the BF for the one-way ANOVA model, use anovaBF(): # ensure BayesFactor package is loaded # library(BayesFactor) # obtain the BF for the ANOVA model with lmBF() anovaBF( score ~ group, data = data.frame(affect_data) ) ## Bayes factor analysis ## -------------- ## [1] group : 66.65842 ±0.04% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS The Bayes Factor for the model is equal to BF = . This indicates that the model is over sixsixtysix hundred times more likely than an intercept-only model. There is therefore substantial evidence for an effect of the aesthetic value of urban images on changes in STAI scores. Tip - lmBF() lmBF() in the BayesFactor package can be used in place of lmBF() to produce exactly the same result: lmBF( score ~ group, data = data.frame(affect_data) ) Remember, this works because ANOVA is a special case of regression. 3.2.9 R2 R2 can be reported for ANOVA models as a measure of effect size. As with simple and multiple regression, R2 represents the proportion of variance explained by the model, where our model is that the scores come from distinct groups of individuals (three groups in our example) with different means. To obtain R2, first use lm() to specify the model, then use glance() from the broom package: # specify and store the anova anova_1 &lt;- lm(score ~ group, data = affect_data) # make sure broom package is loaded, then # use glance() with anova_1 glance(anova_1) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.0523547 0.0460996 0.4743 8.369943 0.0002896 2 -204.4377 416.8754 431.7697 68.16302 303 306 Adjusted R2 (as a percentage, to two decimal places) = %, which represents the percentage of the variance in the change in STAI scores is explained by the affect value of an image. (Remember, the values given by glance() are proportions, so need to be multiplied by 100 to get the percentage.) 3.2.10 Follow-up tests The Bayes factor (66.66) tells us that there's evidence that the means of the three groups differ from one another, but not which groups differ from which. With three groups, there are three possible pairwise comparisons that can be made: VeryLow vs. Low VeryLow vs. High Low vs. High To compare scores from two groups, we can use filter() to filter the affect_data so that it contains only the two groups we want to compare. Then use anovaBF() again to compare the scores across groups. The BF will tell us how many times more likely it is that there's a difference between means, compared to no difference. # Compare scores of VeryLow vs. Low groups # # Step 1. Filter affect_data for VeryLow and Low groups only # Store in &#39;groups_VeryLow_Low&#39; groups_VeryLow_Low &lt;- affect_data %&gt;% filter(group == &quot;VeryLow&quot; | group == &quot;Low&quot;) # # Step 2. Obtain BF for VeryLow vs. Low groups anovaBF(score ~ group, data = data.frame(groups_VeryLow_Low)) # Compare scores of VeryLow vs. High groups # # Step 1. Filter affect_data for VeryLow and High groups only # Store in &#39;groups_VeryLow_High&#39; groups_VeryLow_High &lt;- affect_data %&gt;% filter(group == &quot;VeryLow&quot; | group == &quot;High&quot;) # # Step 2. Obtain BF for VeryLow vs. Low groups anovaBF(score ~ group, data = data.frame(groups_VeryLow_High)) # Compare scores of Low vs. High groups # # Step 1. Filter affect_data to store Low and High groups only # Store in &#39;groups_Low_High&#39; groups_Low_High &lt;- affect_data %&gt;% filter(group == &quot;Low&quot; | group == &quot;High&quot;) # # Step 2. Obtain BF for VeryLow vs. Low groups anovaBF(score ~ group, data = data.frame(groups_Low_High)) ## Bayes factor analysis ## -------------- ## [1] group : 10.25237 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS ## ## Bayes factor analysis ## -------------- ## [1] group : 55.43484 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS ## ## Bayes factor analysis ## -------------- ## [1] group : 0.1774611 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS To two decimal places: The BF for the comparison of VeryLow vs. Low groups = The BF for the comparison of VeryLow vs. High groups = The BF for the comparison of Low vs. High groups = Interpretation: The one-way between subjects ANOVA indicated that there was evidence for an effect of aesthetic value on change in STAI scores (BF = 66.66). The scores in the VeryLow group were lower than those of the Low (BF = 10.25) and High groups (BF = 55.43), but scores in the Low and High groups did not differ (BF = 0.18). This indicates that STAI scores were lower (i.e., there were more negative symptoms) after viewing images that were very low in aesthetic value, compared to images that were low or high in aesthetic value. Viewing pictures that were low or high in aesthetic value resulted in similar changes in STAI scores. Further information on filter() The | symbol means \"or\". The == symbol (an equals sign typed twice) means \"is equal to\" So filter(group == \"VeryLow\" | group == \"Low\") means \"filter the rows of affect_data when the labels in group are equal to VeryLow OR the labels in group are equal to Low An equivalent approach: ttestBF() anovaBF() was used to conduct follow-up tests. Because each test had two groups, this is equivalent to a Bayesian t-test, and so the exact same BFs could have been obtained using ttestBF(): # Compare scores of VeryLow vs. Low groups ttestBF( x = affect_data$score[ affect_data$group==&quot;VeryLow&quot; ], y = affect_data$score[ affect_data$group==&quot;Low&quot; ] ) # compare scores of VeryLow vs. High groups ttestBF( x = affect_data$score[ affect_data$group==&quot;VeryLow&quot; ], y = affect_data$score[ affect_data$group==&quot;High&quot; ] ) # compare scores of Low vs. High groups ttestBF( x = affect_data$score[ affect_data$group==&quot;Low&quot; ], y = affect_data$score[ affect_data$group==&quot;High&quot; ] ) 3.3 Two-way between-subjects ANOVA In a two-way ANOVA, there are two categorical independent variables or factors. When there are multiple factors, the ANOVA is referred to as factorial ANOVA. For example, if the design has two factors, and each factor has two levels, then we refer to the design as a 2 x 2 factorial design. The first number (2) denotes the number of levels of the first factor. The second number (2) denotes the number of levels of the second factor. If, instead, the second factor had three levels, we'd say we have a 2 x 3 factorial design. 3.3.1 Worked example What is the role of resilience in the distress experienced from childhood adversities? Beutel et al. (2017) analysed the distress scores from 2,437 individuals who were either low or high in trait resilience and had experienced either low or high levels of childhood adversity. Exercise 3.2 Design check. What is the first independent variable (or factor) that is mentioned in this design? distress scorechildhood adversityresilience How many levels does the first factor have? 1234 What is the second independent variable (or factor)? distress scorechildhood adversitytrait resilience How many levels does the second factor have? 1234 What is the dependent variable? distress scorechildhood adversitytrait resilience What is the nature of the independent variables? categoricalcontinuous What type of design is this? 2 x 2 x 2 between subjects factorial design2 x 3 between subjects factorial design2 x 2 between subjects factorial designcorrelational design 3.3.2 Read in the data Read in the data at the link below and store in resilience_data: https://raw.githubusercontent.com/chrisjberry/Teaching/master/3_resilience_data.csv # read in the data resilience_data &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/3_resilience_data.csv&#39;) # preview head(resilience_data) distress resilience adversity sex partnership education income unemployed 0 high low 1 1 2 2 0 1 low low 1 1 1 2 0 0 high low 1 2 1 1 0 0 high low 1 1 1 2 0 1 low low 1 2 2 2 0 1 high low 1 1 1 2 0 distress contains the distress scores. Higher scores indicate greater levels of distress. resilience labels the levels of childhood adversity ( low or high) adversity labels the levels of trait resilience ( low or high) (Note. The data are publicly available, but I have changed some of the variable names for clarity.) 3.3.3 Convert the independent variables to factors To enable the factors to be used as such in ANOVA, we need to convert them to factors using factor(): # use mutate() and factor() to convert # resilience and adversity to factors resilience_data &lt;- resilience_data %&gt;% mutate(resilience = factor(resilience), adversity = factor(adversity)) 3.3.4 n in each group Obtain the number of participants in each group: # use count() to obtain the number of rows in the dataset, # group_by() both resilience AND adversity resilience_data %&gt;% group_by(resilience, adversity) %&gt;% count() resilience adversity n high high 106 high low 997 low high 284 low low 1050 In a between-subjects factorial design, participants are assigned to groups by crossing the levels of one variable with those of the second variable. Thus, each row labels the level of resilience and level of adversity for that group. The number of participants in the high resilience, high adversity group is The number of participants in the high resilience, low adversity group is The number of participants in the low resilience, high adversity group is The number of participants in the low resilience, low adversity group is 3.3.5 Visualise the data The way the data are distributed in each group can be inspected with histograms or density plots. # Use `facet_wrap(~ resilience * adversity)` # to plot scores for all combinations of # resilience x adversity levels # use &#39;labeller = label_both&#39; to label levels by factor name resilience_data %&gt;% ggplot(aes(distress)) + geom_density() + facet_wrap(~ resilience * adversity, labeller = label_both) Figure 3.2: Density plots showing distress scores in each group) Interpretation: The data in each group appear positively skewed - the tail of the distribution goes towards the right (i.e., towards more positive values of distress). Beutel et al. (2017) took no further action and analysed the scores as they were. 3.3.6 Plot the means Use ggbarplot() in the ggpubr package: library(ggpubr) # plot the mean of each group # use &#39;desc_stat = &quot;mean_se&quot; to add SE error bars # use &#39;position = position_dodge(0.3)&#39; so that points are spaced resilience_data %&gt;% ggerrorplot(x = &quot;resilience&quot;, y = &quot;distress&quot;, color = &quot;adversity&quot;, desc_stat = &quot;mean_se&quot;, position = position_dodge(0.3)) + xlab(&quot;Resilience&quot;) + ylab(&quot;Distress&quot;) Figure 3.3: Distress as a function of adversity and resilience (error bars indicate SE of the mean) In a two-way design, researchers look at three things: The main effect of factor 1: Overall, do scores differ according to the levels of factor 1? The main effect of factor 2: Overall, do scores differ according to the levels of factor 2? The interaction between the factors: Is the effect of one factor different at each level of the other factor? We can get some idea of the main effects and interaction by inspecting the plot of the means: The main effect of resilience: Overall, distress scores in the high resilience groups appear to be lower thanabout the same ashigher than those in the low resilience groups. The main effect of adversity: Overall, distress scores in the low adversity groups appear to be lower thanabout the same ashigher than those in the high adversity groups. The interaction between resilience and distress: When trait resilience is low rather than high, the effect of adversity on distress appears to be lowersimilargreater. (Hint: the effect of adversity is indicated by the difference between the red and blue points.) 3.3.7 Bayes factors Use anovaBF() in the BayesFactor package to obtain Bayes Factors corresponding to the main effect of factor 1, the main effect of factor 2, and the interaction between factor 1 and factor 2. To specify the two-way ANOVA in anovaBF(), use dependent_variable ~ factor1 * factor2. For the resilience data: # Obtain the Bayes Factors for the ANOVA model anova2x2_BF &lt;- anovaBF( distress ~ resilience * adversity, data = data.frame(resilience_data) ) # look at the output anova2x2_BF ## Bayes factor analysis ## -------------- ## [1] resilience : 1.822053e+27 ±0% ## [2] adversity : 7.870878e+25 ±0% ## [3] resilience + adversity : 3.339074e+46 ±2.16% ## [4] resilience + adversity + resilience:adversity : 3.130195e+49 ±2.06% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS What does 1.82e+27 mean? It means 1.82 x 1027. Or 1820000000000000000000000000. A very large number! For more information see: FAQ (Opens a new tab.) Each BF in the output compares how likely the model is, compared to an intercept only model: [1] resilience is the BF for the main effect of resilience. It's how much more likely a model with resilience alone is than an intercept-only model. [2] adversity is the BF for the main effect of adversity. It's how much more likely a model with adversity alone is than an intercept-only model. [3] resilience + adversity is the BF for the main effects of resilience and adversity. It's how much more likely a model with main effects of resilience and adversity is than an intercept-only model. [4] resilience + adversity + resilience:adversity is the BF for the main effects of resilience and adversity and the interaction between them (resilience:adversity). It's how much more likely a model with main effects of resilience and adversity and also the interaction (resilience:adversity) is than an intercept-only model. [1] and [2] correspond to the main effects of resilience and adversity, respectively. To obtain the BF for the interaction, we need to divide [4] by [3]: # BF for the interaction anova2x2_BF[4] / anova2x2_BF[3] ## Bayes factor analysis ## -------------- ## [1] resilience + adversity + resilience:adversity : 937.444 ±2.98% ## ## Against denominator: ## distress ~ resilience + adversity ## --- ## Bayes factor type: BFlinearModel, JZS This BF then tells us whether there's evidence for the addition of an interaction term to a model containing the main effects of each factor. This is the BF for the interaction. Record the Bayes factors below: The BF for the main effect of resilience is BF = x 1027. The BF for the main effect of adversity is BF = x 1025. The BF for the resilience and adversity interaction is approximately (to the nearest whole number). Remember, this is the BF you calculated by dividing [4] by [3]. BF = . Meaning of ±number% You'll notice that some of BFs had ±1.03% or similar next to them in the output. This is the error associated with the BF. It's like saying my height is 185 cm, plus or minus a millimeter or so. It can be non-zero because generation of the BFs involves random sampling processes. Larger error values mean that the exact same value of the BF won't necessarily be output each time the line of code containing anovaBF() is run, so there's a chance that the BFs in your output differ slightly from those above (particularly for the interaction). This is why I asked you to give your answer to the nearest whole number. 3.3.8 R2 Once again, glance() can be used to obtain R2 for the ANOVA model. The model first needs to be specified with lm(). Using factor1 * factor2 when specifying the model is a shortcut, which will automatically specify the full model containing the interaction. Thus, we can use lm(distress ~ resilience * adversity), which is equivalent to lm(distress ~ resilience + adversity + resilience*adversity). # specify the ANOVA model using lm() anova2x2 &lt;- lm(distress ~ resilience * adversity, data = resilience_data) # R^2^ glance(anova2x2) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.0963021 0.0951878 2.142569 86.42379 0 3 -5312.959 10635.92 10664.91 11168.93 2433 2437 The adjusted R2 of the ANOVA model (to two decimal places, as a percentage) is %, which is the percentage of variance in distress explained by the model. 3.3.9 Follow-up comparisons Given that we have evidence for an interaction, anovaBF() can be used to conduct follow-up comparisons to explore the nature of the interaction. (Note, we would not do this if the BF did not show evidence for the interaction, i.e., if the BF was less than 3.) The interaction implies the effect of adversity is different in individuals with high resilience, and those with low resilience. To determine the evidence for the effect of adversity in individuals with high resilience: # 1. The effect of adversity in individuals with high resilience # First use filter() to store only # the data from the &#39;high&#39; resilience groups resilience_high &lt;- resilience_data %&gt;% filter(resilience == &quot;high&quot;) # Then use `anovaBF()` to look at effect of adversity in resilience_high anovaBF( distress ~ adversity, data = data.frame(resilience_high) ) ## Bayes factor analysis ## -------------- ## [1] adversity : 1.026598 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS To determine the evidence for the effect of adversity in individuals with low resilience: # 2. The effect of adversity in individuals with low resilience # First use filter() to store only # the data from the &#39;low&#39; resilience groups resilience_low &lt;- resilience_data %&gt;% filter(resilience == &quot;low&quot;) # Then use `anovaBF()` to look at effect of adversity in resilience_low anovaBF( distress ~ adversity, data = data.frame(resilience_low) ) ## Bayes factor analysis ## -------------- ## [1] adversity : 2.389569e+17 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS This confirms the interaction that is apparent in the plot: There's no evidence for an effect of adversity within high resilience individuals (BF = ), but there's substantial evidence for an effect of adversity within low resilience individuals (BF = x 1017), such that levels of distress are greatest when the level of adversity is highest. Interestingly, this suggests that resilience may have a buffering effect on the distress experienced as a result of childhood adversity (see Beutel et al., 2017, for further discussion). 3.4 Exercise: one-way Exercise 3.3 One-way ANOVA The data at the link below are from a study by Bobak et al. (2016). They looked at the face matching performance of individuals with superior face recognition abilities, so called \"super recognisers\". Their performance was compared to two control groups. In one control group, payment was linked to performance (labelled \"motivated_control\"). The individuals in the other control group were not paid (labelled \"control\"). The column face_group contains the labels for each group. https://raw.githubusercontent.com/chrisjberry/Teaching/master/3_super_data.csv Conduct a one-way ANOVA to compare the performance across the three groups. Adapt the code in this worksheet to do the following: Try not to look at the solutions before you've attempted them 1. Read in the data and store in super_data Hint Ensure the tidyverse package is loaded. See ?read_csv() Solution super_data &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/3_super_data.csv&#39;) # look at the raw data super_data 2. Convert the independent variable to a factor Hint See ?mutate() and ?factor() Solution super_data &lt;- super_data %&gt;% mutate( face_group = factor(face_group) ) 3. Obtain n in each group Hint Pipe the data to group_by() and count() Solution super_data %&gt;% group_by(face_group) %&gt;% count() n super recognisers = n motivated_control = n control = 4. Produce a histogram of scores in each group Hint Pipe the data to ggplot() and geom_histogram() and facet_wrap() Solution super_data %&gt;% ggplot(aes(performance)) + geom_histogram() + facet_wrap(~face_group) 5. Produce a plot of the means (with SEs) in each group Hint ?ggerrorplot() in the ggpubr package Solution # produce means plot super_data %&gt;% ggerrorplot(x = &quot;face_group&quot;, y =&quot;performance&quot;, desc_stat = &quot;mean_se&quot;) + xlab(&quot;Group&quot;) + ylab(&quot;Face matching performance&quot;) 6. Obtain the mean and SE of each group Hint Use group_by() and summarise() Solution super_data %&gt;% group_by(face_group) %&gt;% summarise( M = mean(performance), SE = sd(performance) / sqrt( n() ) ) The mean score of the super_recogniser group is , SE = The mean score of the motivated_control group is , SE = The mean score of the control group is , SE = 7. Obtain the Bayes factor for the ANOVA model The Bayes factor for the model is equal to (to two decimal places) . This indicates that there is no evidencesubstantial evidence for an effect of the face group on matching performance. Hint ?anovaBF() Solution anovaBF(performance ~ face_group, data = data.frame(super_data)) 8. What is R2 for the model? The adjusted R2 for the effect of face group on matching performance is (as a proportion, to two decimal places) Hint obtain the model with lm(), then use glance() Solution super_anova &lt;- lm(performance ~ face_group, data = super_data) glance(super_anova) 9. Conduct follow-up tests to compare the mean score of each group Hint Use filter() to create new variables containing the scores of two groups at a time Then use anovaBF() to compare the scores of each group. Solution using anovaBF() # super_recognisers vs. motivated_control super_vs_motivated_controls &lt;- super_data %&gt;% filter(face_group == &quot;super_recogniser&quot; | face_group == &quot;motivated_control&quot;) anovaBF(performance ~ face_group, data = data.frame(super_vs_motivated_controls)) # super_recognisers vs. control super_vs_controls &lt;- super_data %&gt;% filter(face_group == &quot;super_recogniser&quot; | face_group == &quot;control&quot;) anovaBF(performance ~ face_group, data = data.frame(super_vs_controls)) # motivated_control vs. control motivated_control_vs_control &lt;- super_data %&gt;% filter(face_group == &quot;motivated_control&quot; | face_group == &quot;control&quot;) anovaBF(performance ~ face_group, data = data.frame(motivated_control_vs_control)) Solution using ttestBF() # super_recognisers vs. motivated_control ttestBF(x = super_data$performance[super_data$face_group == &quot;super_recogniser&quot;], y = super_data$performance[super_data$face_group == &quot;motivated_control&quot;]) # super_recognisers vs. control ttestBF(x = super_data$performance[super_data$face_group == &quot;super_recogniser&quot;], y = super_data$performance[super_data$face_group == &quot;control&quot;]) # motivated_control vs. control ttestBF(x = super_data$performance[super_data$face_group == &quot;motivated_control&quot;], y = super_data$performance[super_data$face_group == &quot;control&quot;]) The Bayes factor comparing the super_recogniser and motivated_control groups is , indicating substantial evidence forsubstantial evidence against there being a difference between groups. Face matching performance in the super_recogniser group was the samelowerhigher than that of the motivated_control group. The Bayes factor comparing the super_recogniser and control groups is , indicating substantial evidence forsubstantial evidence against there being a difference between groups. Face matching performance in the super_recogniser group was the samelowerhigher than that of the control group. The Bayes factor comparing the motivated_control and control groups is , indicating that there was substantial evidence for ainsufficient evidence for asubstantial evidence for an absence of a difference between the scores of each group. 3.5 Exercise: two-way Exercise 3.4 Two-way ANOVA Horstmann et al. (2018) looked at whether the type of exchange that people had with a robot would affect how long it would then take for them to switch it off. The type of exchange was either 'functional' or 'social'. Additionally, the researchers looked at the effect of the type of objection that the robot made during the conversation. The robot either voiced an 'objection' to being switched off, or voiced 'no objection'. The robot_data are located at the link below: https://raw.githubusercontent.com/chrisjberry/Teaching/master/3_robot_data.csv Adapt the code in this worksheet to do the following: Conduct a two-way ANOVA to compare the effects of exchange and objection on time. Determine whether there's sufficient evidence for the following: The main effect of objection: BF (to two decimal places) = The main effect of exchange: BF (to two decimal places) = The interaction between exchange and objection: BF (nearest whole number) = Conduct any follow-up tests concerning the interaction (if necessary). The effect of exchange when the robot objected: BF (to two decimal places) = The effect of exchange when the robot did not object: BF (to two decimal places) = Interpret the main effects and interaction. What do they mean? Hint - code Read in the data to a variable called robot_data Convert the independent variables to factors using factor() Examine the distribution of time in each group using geom_histogram() or geom_density() in ggplot() Obtain summary statistics using group_by(), count() and summarise(mean()) Use a plot from the ggpubr package to create a plot of the means (e.g., ggerrorplot()) Use anovaBF() to obtain the Bayes factors to allow you to assess evidence for the main effects and interaction. If there's evidence for an interaction, then conduct follow-up tests using anovaBF() or ttestBF() Solution - code # ensure following packages have been loaded # library(tidyverse) # library(BayesFactor) # library(ggpubr) # read the data robot_data &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/3_robot_data.csv&#39;) # convert IVs to factors robot_data &lt;- robot_data %&gt;% mutate(exchange = factor(exchange), objection = factor(objection)) # inspect distributions robot_data %&gt;% ggplot( aes(time) ) + geom_histogram() + facet_wrap(~ exchange*objection) # obtain M and SE each group robot_data %&gt;% group_by(exchange, objection) %&gt;% summarise( M = mean(time), SE = sd(time)/sqrt(n()) ) # plot of means robot_data %&gt;% ggerrorplot(x = &quot;objection&quot;, y = &quot;time&quot;, color = &quot;exchange&quot;, desc_stat = &quot;mean_se&quot;, position = position_dodge(0.3)) + xlab(&quot;Type of objection&quot;) + ylab(&quot;Switch off time (seconds)&quot;) # 2x2 ANOVA BF_robot &lt;- anovaBF( time ~ exchange*objection, data = data.frame(robot_data) ) # look at BFs BF_robot # main effect objection BF_robot[1] # main effect exchange BF_robot[2] # interaction BF_robot[4] / BF_robot[3] # Given the evidence for an interaction, conduct further comparisons # 1. The effect of exchange when the robot objected (i.e., `objection`) # use filter() to store only # the data from the &#39;objection&#39; groups robot_objection &lt;- robot_data %&gt;% filter(objection == &quot;objection&quot;) # use `anovaBF()` to look at effect of exchange in the objection groups anovaBF( time ~ exchange, data = data.frame(robot_objection) ) # 2. The effect of exchange when the robot did not object # use filter() to store only # the data from the &#39;no_objection&#39; groups robot_no_objection &lt;- robot_data %&gt;% filter(objection == &quot;no_objection&quot;) # use `anovaBF()` to look at effect of exchange in the no objection groups anovaBF( time ~ exchange, data = data.frame(robot_no_objection) ) # For the interpretation of main effects: # # Obtain M and SE for the main effect of objection robot_data %&gt;% group_by(objection) %&gt;% summarise( M = mean(time), SE = sd(time)/sqrt(n()) ) # Obtain M and SE for the main effect of exchange robot_data %&gt;% group_by(exchange) %&gt;% summarise( M = mean(time), SE = sd(time)/sqrt(n()) ) Hint - interpretation To aid your interpretation of the main effects and interaction, look at the mean of the scores in each group. In which groups is the time taken to switch off the robot longer than others? How does time differ according to type of exchange? How does time differ according to type of objection? Does the effect of exchange seem to be the same at each level of objection? Solution - interpretation There was substantial evidence for a main effect of objection on the time it took for a participant to switch off a robot (BF = 6.29). Participants took longer when the robot had previously mentioned that it objected to being switched off (M = 10.00 seconds, SE = 2.21), compared to when no objection was mentioned (M = 4.69, SE = 0.37). There was insufficient evidence for a main effect of exchange, given that the Bayes factor was inconclusive (BF = 0.74). Thus, there was no evidence to suggest that the time to switch off a robot differed according to whether the type of exchange was functional (M = 8.69, SE = 2.00) or social (M = 5.54, SE = 0.57). The main effects should be viewed in light of the substantial evidence for an interaction between exchange and objection (BF = 3.28). This indicated that it took people longer to switch the robot off when the exchange had been functional, rather than social, but only if the robot had objected to being switched off (M functional = 14.40, SE = 4.11 vs. M social = 6.19, SE = 1.15). When the robot had not previously objected to being switched off, the times were similar following functional and social exchanges (M functional = 4.28, SE = 0.59, vs. M social = 5.05, SE = 0.48). Follow-up tests indicated insufficient evidence for the effect of exchange at each level of objection (i.e., BFs &lt; 3 and BFs &gt; 0.33): The Bayes factor for the effect of exchange when the robot objected was 1.55, and the Bayes factor for the effect of exchange when the robot did not object was 0.47. Thus, although there was evidence for an interaction, the pattern of differences within objection conditions was not confirmed. 3.6 Summary ANOVA is a special case of regression when the predictor variables are entirely categorical. A one-way between-subjects ANOVA has one independent variable, and separate groups of participants for each level of the independent variable. The test looks at whether the mean scores differ between groups. A two-way between-subjects ANOVA has two independent variables (factors). Each factor has levels. Researchers examine evidence for the main effect of each factor and their interaction. The interaction examines the effect of one factor at each level of the other factor. If there's evidence for an interaction, follow-up comparisons can be performed. Use anovaBF() to obtain Bayes factors for the main effects and interaction. 3.7 References Beutel M.E., Tibubos A.N., Klein E.M., Schmutzer G., Reiner I., Kocalevent R-D., et al. (2017) Childhood adversities and distress - The role of resilience in a representative sample. PLoS ONE, 12(3): e0173826. https://doi.org/10.1371/journal.pone.0173826 Glass G.V., Peckham P.D., Sanders J.R. (1972). Consequences of failure to meet assumptions underlying the fixed effects analyses of variance and covariance. Review of Educational Research. 42, 237288. https://doi.org/10.3102%2F00346543042003237 Horstmann A.C., Bock N., Linhuber E., Szczuka J.M., Straßmann C., Kramer N.C. (2018) Do a robots social skills and its objection discourage interactants from switching the robot off? PLoS ONE, 13(7): e0201581. https://doi.org/10.1371/journal.pone.0201581 Meidenbauer, K. L., Stenfors, C. U., Bratman, G. N., Gross, J. J., Schertz, K. E., Choe, K. W., &amp; Berman, M. G. (2020). The affective benefits of nature exposure: What's nature got to do with it?. Journal of Environmental Psychology, 72, 101498. https://doi.org/10.1016/j.jenvp.2020.101498 Schmider E., Ziegler M., Danay E., Beyer L., Bühner M. (2010). Is it really robust? Methodology. 6, 147151. https://doi.org/10.1027/1614-2241/a000016 "],["multiple2.html", "Session 4 Multiple regression: one continuous, one categorical 4.1 Overview 4.2 Worked example 4.3 Read in the data 4.4 Visualise the data 4.5 Evidence for the interaction 4.6 Simple slopes analysis 4.7 Exercises 4.8 Further exercise 4.9 Further knowledge 4.10 Summary 4.11 References", " Session 4 Multiple regression: one continuous, one categorical Chris Berry 2022 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} 4.1 Overview Slides from the lecture part of the session: Download Slides for Rmd support So far we have looked at simple and multiple regression with continuous variables. (And in the last session, we saw that between-subjects ANOVA was equivalent to a multiple regression in which all the predictors are categorical.) Here, we look at multiple regression with a mixture of continuous and categorical predictors. Specifically, one predictor is continuous, and the other is a dichotomous categorical variable (i.e., made up of two levels or groups). This type of analysis can be used to see whether the relationship between two variables is the same or different across groups of individuals. For example, adherence to a particular treatment may be associated with attitudes towards the efficacy of the treatment, but the nature of the association may differ according to the type of psychological disorder a person has. Thus, in this type of analysis, in addition to looking at the ability of individual predictors to explain an outcome variable, we're able to look at their combined effect in explaining the outcome. This is referred to as an interaction effect. The interaction can tell us whether the relationship between one of the predictors and the outcome differs as a function of the levels of the other predictor variable. 4.2 Worked example Scientists often research things that are of personal interest to themselves. Do people end up trusting the researcher more when they have some personal investment in what they are researching? Altenmuller et al. (2021) looked at whether participants' trust in the researcher is related to 1) whether they are told that the researcher is personally affected by the research or not affected by their research, and 2) the participant's attitude towards the research topic. The data from Altenmuller et al.'s (2021) study (Experiment 2) are stored at the link below. https://raw.githubusercontent.com/chrisjberry/Teaching/master/4_trust_data.csv The key variables: trustworthiness: how trustworthy the participant finds the researcher. Higher scores indicate higher levels of trustworthiness. attitude: the participant's attitude towards the research topic. Higher scores indicate a more positive attitude. group: whether participants were told that the researcher is personally affected or not_affected by their own research. More about the data The data were made publicly available by the researchers and have been pre-processed (using the researcher's open R code). Only a subset of the data is used here for teaching purposes; variable names have been changed for clarity. Exercise 4.1 Design check. What is the outcome variable in this design? attitudetrustworthinessgroup What is the nature of the outcome variable? categoricalcontinuous What is the name of the continuous predictor variable? groupattitudetrustworthiness What is the name of the categorical predictor variable? groupattitudetrustworthiness How many levels of group are there? 123 When a variable has two levels it is called a continuousdichotomousquantitative variable. 4.3 Read in the data Read in the data to trust_data # ensure tidyverse is loaded # library(tidyverse) # read in the data trust_data &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/4_trust_data.csv&#39;) # look at the data trust_data %&gt;% head() ppt group attitude trustworthiness credibility evaluation 1 not_affected 5.714286 4.428571 4.428571 2.583333 2 affected 3.142857 4.285714 4.428571 3.250000 3 not_affected 5.285714 4.428571 4.428571 2.750000 4 not_affected 4.428571 4.642857 3.857143 2.416667 5 affected 6.000000 6.000000 5.285714 2.333333 6 not_affected 3.642857 5.500000 4.571429 2.500000 4.4 Visualise the data Use a scatterplot to look at the relationship between trustworthiness, attitude and group. Use different colours for each group by specifying colour = group in aes(): trust_data %&gt;% ggplot(aes(x = attitude, y = trustworthiness, colour = group )) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_light() Figure 1.1: Trustworthiness vs. participant attitude to research Exercise 4.2 Visual inspection of the scatterplot Does the relationship between attitude and trustworthiness appear to be the same within each group? yesno Within the group that were told that the researcher was personally affected by their research: The association between attitude and trustworthiness appears to be negativepositive A more positive attitude towards the research topic tends to be associated with lowerno changehigher values of trustworthiness. Within the group that were told that the researcher was personally not_affected by their research: The association between attitude and trustworthiness appears to be negativepositive A more positive attitude towards the research topic tends to be associated with lowerno changehigher values of trustworthiness. Go further with ggplot The scatterplot can be improved through further customisation. For example, to change the x- and y- labels, add the code: + xlab(\"Participants' attitude to the research\") + ylab(\"Trustworthiness of researcher\") Feel free to customise the plot further if you feel it can be improved! Further inspection It is good practice to inspect the distributions of the data prior to analysis, for example, with geom_histogram() or geom_density(). We can check if the data appear normally distributed, or positively or negatively skewed. We can also check for outliers using geom_boxplot(). Get to know your data! # look at the distribution of trustworthiness trust_data %&gt;% ggplot(aes(trustworthiness)) + geom_histogram() # look at the distribution of attitude trust_data %&gt;% ggplot(aes(attitude)) + geom_histogram() # obtain n in affected &amp; not_affected groups trust_data %&gt;% group_by(group) %&gt;% count() 4.5 Evidence for the interaction An interaction between the attitude and group predictors is suggested by the scatterplot. That is, the association between trustworthiness and attitude appears to be different in the affected and non_affected groups. We may therefore want to include the interaction term in our multiple regression model and determine whether we have evidence for the interaction or not using Bayes factors. We'll do this in three steps: Specify the model without the interaction Specify the model with the interaction Compare the model with and without the interaction 4.5.1 Model without the interaction The model without an interaction looks like a typical multiple regression model with two predictors (from Session 2): model &lt;- lm(outcome ~ predictor_1 + predictor_2, data = mydata) As we did in the previous session, we need to also convert the categorical variable (group) to a factor() to ensure it's treated as categorical by R. Thus, for our data: # Convert group to a factor trust_data &lt;- trust_data %&gt;% mutate(group = factor(group)) # Specify the model without the interaction without_interaction &lt;- lm(trustworthiness ~ attitude + group, data = trust_data) # R² for the model # library(broom) glance(without_interaction) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.0130486 0.0065555 0.7588684 2.009602 0.1358188 2 -349.3972 706.7944 721.7018 175.0679 304 307 How much of the variance in trustworthiness is explained by the model with attitude and group? The adjusted R2 (as a proportion, to two decimal places) = . Next use lmBF() to obtain the BF for the model with no interaction: # library(BayesFactor) # obtain BF for the model BF_without_interaction &lt;- lmBF(trustworthiness ~ attitude + group, data = data.frame(trust_data)) # look at the BF BF_without_interaction ## Bayes factor analysis ## -------------- ## [1] attitude + group : 0.1032236 ±2% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS The BF for the model without the interaction is equal to (to 2 decimal places) . 4.5.2 Model with the interaction To specify an interaction between two predictors, use the term predictor1 * predictor2. The * symbol means multiply, so the interaction term is simply the predictors multiplied together. To specify the model with the interaction, add the interaction term predictor1 * predictor2 to the model without the interaction term: # Specify the model with the interaction with_interaction &lt;- lm(trustworthiness ~ attitude + group + attitude*group, data = trust_data) # R² for the model glance(with_interaction) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.0623098 0.0530257 0.740907 6.711482 0.0002128 3 -341.5378 693.0756 711.7098 166.3298 303 307 How much of the variance in trustworthiness is explained by the model with attitude,group, and the attitude*group interaction? The adjusted R2 (as a proportion, to two decimal places) = . Use lmBF() to obtain the BF for the model with the interaction: # obtain BF for the model BF_with_interaction &lt;- lmBF(trustworthiness ~ attitude + group + attitude*group, data = data.frame(trust_data)) # look at the BF BF_with_interaction ## Bayes factor analysis ## -------------- ## [1] attitude + group + attitude * group : 32.30543 ±1.75% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS The BF for the model with the interaction is equal to . (Note: yours may come out slightly different to that shown in the worksheet due to random sampling processes used in the BF generation; as long as the BF is around 30, that's okay.) 4.5.3 Compare the model with and without the interaction To determine whether there's evidence for the interaction, we need to compare the Bayes factor for the model with and without the interaction. Exercise 4.3 Bayes factors can be compared with the formula: \\(\\frac{Bayes \\ factor \\ more \\ complex \\ model}{Bayes \\ factor \\ simpler \\ model}\\) Dividing the BFs in this way will return another Bayes factor, which is a number that tells us how many times more likely the more complex model compared to the simpler model, given the data. So, if we perform this calculation: \\(\\frac{Bayes \\ factor \\ for \\ model \\ with \\ interaction}{Bayes \\ factor \\ for \\ model \\ without \\ interaction}\\) this will tell us how many times more likely the model with the interaction is than the model without the interaction. In other words, dividing the BF for the model with the interaction by the BF for the model without the interaction will tell us whether there's evidence for an interaction between the predictors. # compare BFs of models # to determine evidence for the interaction BF_with_interaction / BF_without_interaction ## Bayes factor analysis ## -------------- ## [1] attitude + group + attitude * group : 312.9657 ±2.65% ## ## Against denominator: ## trustworthiness ~ attitude + group ## --- ## Bayes factor type: BFlinearModel, JZS The Bayes factor for the comparison of the model with and without the interaction is approximately 300. (Yours may not be exactly equal to 300 because of the error associated with the generation of the Bayes factor; it should be roughly the same though!) Exercise 4.4 Assessing the interaction Adjusted R2: The adjusted R2 for the model without the interaction (that you noted earlier) was: The adjusted R2 for the model with the interaction (that you also noted earlier) was: What is the increase in adjusted R2 as a result of the addition of the interaction term to the model? Hint. Work out the difference between the two adjusted R2 values you noted above Bayes factors According to comparison of BFs for the models, which statement is true? There's substantial evidence for an absence of an interaction between attitude and group The model with the interaction is as likely as the model without the interaction, given the data There's substantial evidence for an interaction between attitude and group Explanation As a result of adding in the interaction term to the model, the adjusted R2 value increases by approximately 0.04 (i.e., from 0.01 to 0.05). The comparison of BFs for the model with and without the interaction term indicates that there's substantial evidence for the interaction between attitude and group - it's around 300 times more likely that there is an interaction than there isn't one. Thus, as indicated in the scatterplot, there's evidence that the association between trustworthiness and attitude is different in each group. Namely, the association is positive in the affected group, whereas it appears to be negative in the not_affected group. 4.6 Simple slopes analysis Given evidence for the interaction, we can conduct follow-up analyses to further characterise it. In a simple slopes analysis the relationship between the outcome and first predictor is examined at each level of the second predictor. Another way of thinking about the interaction is that it implies that the slopes of the lines for the affected (red line in the scatterplot) and not_affected (blue line) groups are not the same. We'll conduct a simple slopes analysis by conducting two simple regressions. The first will be the regression of trustworthiness on the basis of attitude in the affected group. The second will be the regression of trustworthiness on the basis of attitude in the not_affected group. The simplest way to do this is to store the data for each group separately, then perform a simple regression with each dataset. First, filter trust_data for each group: # Filter the dataset for when group is equal to &quot;affected&quot; # store in affected_data affected_data &lt;- trust_data %&gt;% filter(group == &quot;affected&quot;) # Filter the dataset for when group is equal to &quot;not_affected&quot; # store in not_affected_data not_affected_data &lt;- trust_data %&gt;% filter(group == &quot;not_affected&quot;) Now run a simple regression of trustworthiness ~ attitude in each group. First, do the affected group: # affected group: simple regression coefficients lm(trustworthiness ~ attitude, data = affected_data) # affected group: BF lmBF(trustworthiness ~ attitude, data = data.frame(affected_data)) ## ## Call: ## lm(formula = trustworthiness ~ attitude, data = affected_data) ## ## Coefficients: ## (Intercept) attitude ## 3.9631 0.1839 ## ## Bayes factor analysis ## -------------- ## [1] attitude : 1483.223 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS Exercise 4.5 For the affected group (the red line in the scatter plot) The intercept of the regression line is = The slope of the regression line is = The BF for the model is This is inconclusivesubstantial evidence for a negativean absence of ana positive association between trustworthiness and attitude in the affected group. In this group, participants with more positive attitudes to the research topic perceived the researcher to be lessbe more credible. For the not_affected group: # not_affected group: simple regression coefficients lm(trustworthiness ~ attitude, data = not_affected_data) # not_affected group BF lmBF(trustworthiness ~ attitude, data = data.frame(not_affected_data)) ## ## Call: ## lm(formula = trustworthiness ~ attitude, data = not_affected_data) ## ## Coefficients: ## (Intercept) attitude ## 5.09157 -0.09505 ## ## Bayes factor analysis ## -------------- ## [1] attitude : 0.5874073 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS Exercise 4.6 For the not_affected group (the blue line in the scatterplot) The intercept of the regression line is = The slope of the regression line is = The BF for the model is Although the scatterplot suggests a negativean absence of ana positive association between trustworthiness and attitude in the not_affected group, there was insufficient evidence for this association because the Bayes factor was inconclusivesubstantial In sum, this analysis has shown that when participants hold a more favourable attitude towards a research topic, they perceived researchers who were personally affected by their own research as being more trustworthy. The association appeared to be reversed for researchers who were not personally affected by their research, but a Bayes factor analysis indicated that there was no association between trustworthiness and attitude in this group. 4.7 Exercises Exercise 4.7 Credibility In addition to asking about trustworthiness, Altenmuller et al. (2021) also asked participants how credible they found the researcher. The scores are stored in credibility in the trust_data; higher scores indicate greater perceived credibility. As with trustworthiness, the authors looked at whether attitude and group predicted credibility. Adapt the code in this worksheet to do the following: Create a scatterplot with credibility on the y-axis, attitude on the x-axis, and group as separate lines. Hint Pipe the data to ggplot() and use colour = group in aes Solution trust_data %&gt;% ggplot(aes(x = attitude, y = credibility, colour = group )) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_light() The slope for the affected group appears to be close to zeropositivenegative The slope for the not_affected group appears to be close to zeropositivenegative 2. Obtain adjusted R2 and the BF for the model without an interaction Adjusted R2 (as a proportion, to 2 decimal places) for the model without an interaction is: The BF for the model without an interaction is Hint Specify the model with predictor1 + predictor2 with lm(), pass to glance(), then use lmBF() to get the BF. Solution # Specify the model without an interaction credibility_no_interaction &lt;- lm(credibility ~ attitude + group, data = trust_data) # library(broom) glance(credibility_no_interaction) # BF model BF_credibility_no_interaction &lt;- lmBF(credibility ~ attitude + group, data = data.frame(trust_data)) 3. Obtain adjusted R2 and the BF for the model with an interaction Adjusted R2 (as a proportion, to 2 decimal places) for the model without an interaction is: The BF for the model without an interaction is x 1012 Hint Specify the model with predictor1 + predictor2 + predictor1*predictor2 with lm(), pass to glance(), then use lmBF() to get the BF. Solution # Specify the model with an interaction credibility_with_interaction &lt;- lm(credibility ~ attitude + group + attitude*group, data = trust_data) # library(broom) glance(credibility_with_interaction) # BF model BF_credibility_with_interaction &lt;- lmBF(credibility ~ attitude + group + attitude*group, data = data.frame(trust_data)) 4. Compare the models with and without the interaction The increase in adjusted R2 as a result of including the interaction term in the model is (as a proportion to 2 decimal places) = The Bayes factor for the comparison of the models with and without the interaction = Is there substantial evidence for an interaction between attitude and group in the prediction of percieved credibility of the researcher? noyes Hint Work out the difference in adjusted R2 in the model with and without the interaction. Use BF_model_with_interaction / BF_model_without_interaction If the BF &gt; 3, then by convention we say there's substantial evidence for the interaction. Solution # Adj R-square difference 0.21 - 0.15 # Compare BFs BF_credibility_with_interaction / BF_credibility_no_interaction 5. Simple slopes analysis For the affected group: The intercept of the regression line is = The slope of the regression line is = The BF for the model is x 1015 This is inconclusivesubstantial evidence for a negativean absence of ana positive association between credibility and attitude in the affected group. In this group, participants with more positive attitudes to the research topic perceived the researcher to be lessbe more credible. For the not_affected group: The intercept of the regression line is = The slope of the regression line is = The BF for the model is This is inconclusivesubstantial evidence for a negativean absence of ana positive association between credibility and attitude in the not_affected group. Hint Use filter() to separate out the groups of each dataset. Conduct one simple regression for the affected group. Conduct one simple regression for the not_affected group. Code # Filter the dataset for when group is equal to &quot;affected&quot; affected_data &lt;- trust_data %&gt;% filter(group == &quot;affected&quot;) # Filter the dataset for when group is equal to &quot;not_affected&quot; not_affected_data &lt;- trust_data %&gt;% filter(group == &quot;not_affected&quot;) # affected group coefficients lm(credibility ~ attitude, data = affected_data) # affected group BF lmBF(credibility ~ attitude, data = data.frame(affected_data)) # not_affected group coefficients lm(credibility ~ attitude, data = not_affected_data) # not_affected group BF lmBF(credibility ~ attitude, data = data.frame(not_affected_data)) Exercise 4.8 Critical evaluation of the field Altenmuller et al. (2021) also asked participants to report how critical they were in their evaluation of the entire research field. The scores are stored in evaluation in the trust_data; higher scores indicate that the participant evaluated the field more critically. As with the other outcome variables we've considered, the authors looked at whether attitude and group predicted evaluation. Adapt the code in this worksheet to do the following: Create a scatterplot with evaluation on the y-axis, attitude on the x-axis, and group as separate lines. Hint Pipe the data to ggplot() and use colour = group in aes() Solution trust_data %&gt;% ggplot(aes(x = attitude, y = evaluation, colour = group )) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_light() The slope for the affected group appears to be close to zeropositivenegative The slope for the not_affected group appears to be close to zeropositivenegative 2. Obtain adjusted R2 and the BF for the model without an interaction Adjusted R2 (as a proportion, to 2 decimal places) for the model without an interaction is: The BF for the model without an interaction is x 1013 Hint Specify the model with predictor1 + predictor2 with lm(), pass to glance(), then use lmBF() to get the BF. Solution # Specify the model without an interaction evaluation_no_interaction &lt;- lm(evaluation ~ attitude + group, data = trust_data) # library(broom) glance(evaluation_no_interaction) # BF model BF_evaluation_no_interaction &lt;- lmBF(evaluation ~ attitude + group, data = data.frame(trust_data)) 3. Obtain adjusted R2 and the BF for the model with an interaction Adjusted R2 (as a proportion, to 2 decimal places) for the model without an interaction is: The BF for the model without an interaction is x 1018 Hint Specify the model with predictor1 + predictor2 + predictor1*predictor2 with lm(), pass to glance(), then use lmBF() to get the BF. Solution # Specify the model with an interaction evaluation_with_interaction &lt;- lm(evaluation ~ attitude + group + attitude*group, data = trust_data) # library(broom) glance(evaluation_with_interaction) # BF model BF_evaluation_with_interaction &lt;- lmBF(evaluation ~ attitude + group + attitude*group, data = data.frame(trust_data)) 4. Compare the model with and without the interaction The increase in adjusted R2 as a result of including the interaction in the model is (as a proportion to 2 decimal places) = The Bayes factor for the model with the interaction vs. without = Is there evidence for an interaction between attitude and group? noyes Hint Work out the difference in adjusted R2 in the model with and without the interaction. Use BF_more_complex_model / BF_simpler_model If the BF &gt; 3, then there's substantial evidence for the interaction. Solution # Adj R-square difference 0.27 - 0.20 # Compare BFs BF_evaluation_with_interaction / BF_evaluation_no_interaction 5. Simple slopes analysis For the affected group: The intercept of the regression line is = The slope of the regression line is = The BF for the model is x 1020 This is inconclusivesubstantial evidence for a negativean absence of ana positive association between evaluation and attitude in the affected group. In this group, participants with more positive attitudes to the research topic evaluated the research field lessmore critically For the not_affected group: The intercept of the regression line is = The slope of the regression line is = The BF for the model is This is substantialinsufficient evidence for an association between evaluation and attitude in the not_affected group. Hint Use filter() to separate out the groups of each dataset. Conduct one simple regression for the affected group. Conduct one simple regression for the not_affected group. Code # Filter the dataset for when group is equal to &quot;affected&quot; affected_data &lt;- trust_data %&gt;% filter(group == &quot;affected&quot;) # Filter the dataset for when group is equal to &quot;not_affected&quot; not_affected_data &lt;- trust_data %&gt;% filter(group == &quot;not_affected&quot;) # affected group lm(evaluation ~ attitude, data = affected_data) # affected group BF lmBF(evaluation ~ attitude, data = data.frame(affected_data)) # not_affected group lm(evaluation ~ attitude, data = not_affected_data) # not affected group BF lmBF(evaluation ~ attitude, data = data.frame(not_affected_data)) 4.8 Further exercise Exercise 4.9 No interaction Using the data from Teychenne and Hinkley (2016) that we used in Session 1, determine whether there is evidence for an interaction between anxiety_score and level of education in the prediction of screen_time. education is made up of groups 'No uni degree' and 'University degree'. The data are located at: https://raw.githubusercontent.com/chrisjberry/Teaching/master/1_mental_health_data.csv The increase in Adjusted R2 associated with the addition of the interaction to the model is (as a proportion) The Bayes factor comparing the model with and without the interaction is (to two decimal places) There's substantial evidence that the relationship between anxiety_score and screen_time is the samedifferent in those who have a degree and those who don't. Given the lack of evidence for an interaction, in an additive model containing only anxiety_score and education as predictors of screen_time: Higher anxiety scores tended to be associated with fewerno change ingreater hours of screen time, BF = Individuals without a university degree tended to spend a similar amount of timegreater amounts of timelower amounts of time using screens (e.g., devices, TV, computer) each week than those with a university degree, BF = Solution code # read the data to R using read_csv() mentalh &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/1_mental_health_data.csv&#39;) # scatterplot mentalh %&gt;% ggplot(aes(x = anxiety_score, y = screen_time, colour = education )) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_light() # Specify the model without an interaction screen_time_no_interaction &lt;- lm(screen_time ~ anxiety_score + education, data = mentalh) # in library(broom) glance(screen_time_no_interaction) # adj R² = 0.06 # BF model BF_screen_time_no_interaction &lt;- lmBF(screen_time ~ anxiety_score + education, data = data.frame(mentalh)) # Specify the model with an interaction screen_time_with_interaction &lt;- lm(screen_time ~ anxiety_score + education + anxiety_score*education, data = mentalh) # library(broom) glance(screen_time_with_interaction) # adj R² = 0.06 # BF model BF_screen_time_with_interaction &lt;- lmBF(screen_time ~ anxiety_score + education + anxiety_score*education, data = data.frame(mentalh)) # Adj R-square difference 0.06 - 0.06 # Compare BFs BF_screen_time_with_interaction / BF_screen_time_no_interaction # There&#39;s no evidence for the interaction, # therefore assume additive model # # BF of unique contribution of anxiety_score in model with education BF_screen_time_no_interaction / lmBF(screen_time ~ education, data = data.frame(mentalh)) # BF of unique contribution of education in model with anxiety_score BF_screen_time_no_interaction / lmBF(screen_time ~ anxiety_score, data = data.frame(mentalh)) 4.9 Further knowledge 4.9.1 Moderation Moderation The analysis that we've performed with attitude and group in this session is sometimes referred to as a test of moderation (Baron &amp; Kenny, 1986). Moderation is when the relationship between two variables changes as a function of a third variable. Altenmuller et al. (2021) concluded that participants' attitude moderated \"the effect of a researcher disclosing being personally affected (vs. not affected ) by their own research on participants' trustworthiness ascriptions regarding the research\". That is, attitude moderates the effect of group on trustworthiness. It could also be said that the effect of a researcher saying that they are personally affected by their own research moderates the relationship between attitude and trustworthiness (or credibility). That is, the group moderates the relationship between attitude and trustworthiness. A relationship is present when the researcher says they are affected, but absent when they say they are not affected. Which variable is said to be the moderator variable appears to be down to the choice of the researcher and the context of the research. If we don't find evidence for the interaction term, then we don't have evidence that one variable moderates the relationship between the other two. Instead, researchers may say that there is an additive effect of both predictors on the outcome variable. The absence of an interaction indicates that the regression lines for each group do not differ (i.e., the lines for each group in a scatterplot are statistically parallel to one another). 4.9.2 Coefficients Only for those wanting a deeper understanding. Coefficients If we obtain the coefficients for the full regression model (with the interaction), we can write out the regression equation: lm(trustworthiness ~ attitude + group + attitude*group, data = trust_data) \\(Predicted\\ outcome = a + b_1X_1 + b_2X_2 + b_3X_1X_2\\) So, \\(Predicted\\ trustworthiness = 3.96 + 0.18(attitude) + 1.13(group) - 0.28(attitude\\times group)\\) From this equation, we can derive the simple regression equations for each group. As we saw in the previous session, behind the scenes, R uses dummy coding to code the two levels of the categorical variable (i.e., coding levels of a categorical variable with 0s and 1s). It uses 0s to code the affected group and 1s to code the not_affected group. It does it this way because it assigns 0s and 1s alphabetically. Thus, taking the regression equation and substituting group = 0 for the affected group: \\[ \\begin{align} Predicted\\ trustworthiness &amp;= 3.96 + 0.18(attitude) + (1.13\\times0) - 0.28(attitude\\times0)\\\\ &amp;= 3.96 + 0.18(attitude) + 0 - 0 \\\\ &amp;= 3.96 + 0.18(attitude) \\end{align} \\] The intercept (3.96) and slope (0.18) in this simple regression equation match those obtained for the affected group in the earlier simple slopes analysis. Next, taking the full regression equation and substituting group = 1 for the not_affected group: \\[ \\begin{align} Predicted\\ trustworthiness &amp;= 3.96 + 0.18(attitude) + (1.13\\times1) - 0.28(attitude\\times1)\\\\ &amp;= 3.96 + 0.18(attitude) + 1.13 - 0.28(attitude) \\\\ &amp;= 5.09 + 0.18(attitude) - 0.28(attitude) \\\\ &amp;= 5.09 -0.10(attitude) \\end{align} \\] The intercept (5.09) and slope (-0.10) in this simple regression correspond to those we obtained for the not_affected group in the earlier simple slopes analysis. In sum, when one of the predictors is dichotomous, it is possible to derive the simple regression equation for each level of that predictor from the regression equation for the model with the interaction term included. 4.9.3 Centering Again, only for those wanting deeper knowledge. Centering When testing for moderation effects, it is common to center the predictor variables prior to the analysis. (Indeed, Altenmuller et al. (2021) centered attitude prior to running their analyses.) Centering is where you subtract the mean of a variable from every score of that variable. For example, to center the attitude scores, we'd obtain the mean of attitude, and then subtract that value from each of our attitude scores. Thus, a participant with a score of 0 on the centered attitude score would therefore have an attitude value that is equal to the mean. Centering is usually performed to help increase the interpretability of the coefficients of the predictor variables in a model with an interaction. It can also help to reduce the chances of multicollinearity between the predictor variables and the interaction term. Multicollinearity can occur because the interaction term is derived from the individual predictors themselves (by multiplying them together). scale() can be used to center variables automatically. Set the option center = TRUE to center the variable. Setting the option scale = TRUE would also standardise the variable (i.e., divide each score by the standard deviation, to create z-scores), so scale = FALSE means that we won't also standardise the scores: # center the attitude scores # use mutate() to create a new variable in trust_data # called attitude_centered trust_data &lt;- trust_data %&gt;% mutate( attitude_centered = scale(attitude, center = TRUE, scale = FALSE)[,1] ) # compare the means to check that centered scores have mean of 0 trust_data %&gt;% summarise(mean(attitude), mean(attitude_centered)) Before centering, the mean of the attitude scores was 4.26. After centering, the mean (of attitude_centered) is 0, or close enough to zero, being 4.08 x 10-16, more precisely because there's some rounding error along the way. Now re-run the analysis using attitude_centered in place of attitude and look at adjusted R2 and the BF: # full model with attitude_centered full_centered &lt;- lm(trustworthiness ~ attitude_centered + group + attitude_centered*group, data = trust_data) # R-squared glance(full_centered) # BF lmBF(trustworthiness ~ attitude_centered + group + attitude_centered*group, data = data.frame(trust_data)) The adjusted R2 for the full model is , which is the same as we found previously when attitude was not centered. The BF for the model is approximately 33, which is the same as we found previously with the non-centered version of attitude. Thus, centering predictors does not affect the adjusted R2 or evidence for the model. This is because subtracting the mean from a variable is equivalent to a linear transformation and this does not affect the association. To go even further, centering affects the values of the coefficients of the individual predictors in the model, but has no effect on the coefficient for the interaction. When attitude wasn't centered, the regression equation was: \\(Predicted\\ trustworthiness = 3.96 + 0.18(attitude) + 1.13(group) - 0.28(attitude\\times group)\\) Using attitude_centered, the coefficients for the regression equation are obtained from: full_centered And so the regression equation is: \\(Predicted\\ trustworthiness = 4.75 + 0.18(attitude) -0.06(group) - 0.28(attitude\\times group)\\) Notice that the intercept has changed and so has the coefficient for group. As before, to obtain the simple regression equations for each group we can once again substitute in the dummy codes for group. Thus, for the affected group: \\[ \\begin{align} Predicted\\ trustworthiness &amp;= 4.75 + 0.18(attitude) - (0.06\\times0) - 0.28(attitude\\times0)\\\\ &amp;= 4.75 + 0.18(attitude) + 0 - 0 \\\\ &amp;= 4.75 + 0.18(attitude) \\end{align} \\] For the not_affected group: \\[ \\begin{align} Predicted\\ trustworthiness &amp;= 4.75 + 0.18(attitude) - (0.06\\times1) - 0.28(attitude\\times0)\\\\ &amp;= 4.75 + 0.18(attitude) -0.06 - 0.28(attitude) \\\\ &amp;= 4.69 + 0.18(attitude) - 0.28(attitude) \\\\ &amp;= 4.69 -0.10(attitude) \\end{align} \\] The values in the simple regression equations come out slightly differently when attitude is centered, but the direction on the sign of the coefficient for attitude is the same. In sum, centering of predictor variables is often performed in moderation analyses, but does not affect the variance explained by the model, the evidence (i.e., the BF) for the model, nor the direction (positive/negative) of the associations in the simple slopes analyses (because it doesn't change the slope). Centering can be desirable to aid interpretation of the coefficients of individual predictors and can reduce multicollinearity. 4.10 Summary A multiple regression model can consist of a mixture of continuous and categorical predictors, Predictors may have a combined effect in explaining the outcome variable. Evidence for this interaction effect can be examined by adding the interaction term to the model, i.e., with lm(outcome ~ predictor1 + predictor2 + predictor1*predictor2) With a dichotomous categorical variable, the interaction implies that the slopes of the simple regressions of outcome ~ continuous_predictor are different. This can be examined with a simple slopes analysis. The interaction implies that the relationship between the outcome and continuous predictor are different in each group. 4.11 References Altenmuller M.S., Lange L.L., Gollwitzer M. (2021). When research is me-search: How researchers motivation to pursue a topic affects laypeoples trust in science. PLoS ONE 16(7): e0253911. https://doi.org/10.1371/journal.pone.0253911 Baron, R. M., &amp; Kenny, D. A. (1986). The moderatormediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations. Journal of Personality and Social Psychology, 51(6), 1173. https://psycnet.apa.org/doi/10.1037/0022-3514.51.6.1173 "],["multiple3.html", "Session 5 Multiple regression: hierarchical regression 5.1 Overview 5.2 Worked example 1: wellbeing 5.3 Exercise 1 5.4 Worked Exercise 2: Controlling for categorical variables 5.5 Exercise 2 5.6 Further knowledge and exercises 5.7 Summary 5.8 References", " Session 5 Multiple regression: hierarchical regression Chris Berry 2022 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} 5.1 Overview Slides from the lecture part of the session: Download Hierarchical regression is a form of multiple regression analysis and can be used when we want to add predictor variables to a model in discrete steps or stages. The technique allows the unique contribution of the variables on each step to be separately determined. We can use it when we want to know whether a predictor variable (e.g., sense_of_belonging) predicts an outcome (e.g., social_interaction) after controlling for background variables that are categorical (e.g., gender, level of education) or continuous (e.g., age, score on a cognitive test) in nature. The variables entered on each step can also be determined by theoretical considerations, to test specific hypotheses. At each step in the analysis, the increase in the variance explained in the outcome variable (i.e., R2) and evidence for the unique contribution of the predictors (with Bayes factors) can be assessed. Hierarchical regression is sometimes called sequential regression. 5.2 Worked example 1: wellbeing In Session 2, we analysed some of the data from Iani et al. (2019) and found evidence that brooding and worry predicted wellbeing in a multiple regression. Iani et al. (2019) were actually primarily interested in whether mindfulness and emotional intelligence predicted psychological wellbeing scores, but after controlling for brooding and worry. The reason for asking the question in this way is because it's a well-established finding that brooding and worry explain wellbeing, but less is known about mindfulness and emotional intelligence. To control for brooding and worry, these variables are entered into a regression first. Next, mindfulness and emotional intelligence are added, and the change in R2 associated with their addition to the model can be evaluated. Bayes factors can also be used to assess the unique contribution of the predictors added at each step. 5.2.1 Read in the data Read the data to R, and store in pwb_data (to stand for Psychological WellBeing data). The data we'll use are located at: https://raw.githubusercontent.com/chrisjberry/Teaching/master/2_wellbeing_data.csv # First ensure tidyverse is loaded, i.e., &#39;library(tidyverse)&#39; # read in the data using read_csv(), store in pwb_data pwb_data &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/2_wellbeing_data.csv&#39;) (Note. The data are publicly available, but I've changed the variable names for clarity. As in Iani et al., missing values were replaced with the mean of the relevant variable.) Preview the data with head(): pwb_data %&gt;% head() describing observing acting nonreactivity nonjudging attention clarity repair brooding worry wellbeing gad 13 14 19 6 15 23 20 21 18 19 64 13 15 6 15 13 12 24 20 15 14 30 72 7 14 14 16 17 11 31 27 26 15 30 59 11 10 10 18 18 18 27 18 20 16 29 62 13 10 14 15 14 16 21 24 14 8 27 78 5 21 15 18 14 14 30 26 23 10 23 78 4 About the data: Mindfulness variables: describing: Higher scores indicate greater ability to describe one's inner experiences. observing: Higher scores indicate greater levels of observing. acting: Higher scores indicate greater levels of acting with awareness. nonreactivity: Higher scores indicate greater levels of nonreactivity. nonjudging: Higher scores indicate greater levels of nonjudging. Emotional intelligence variables: attention: Higher scores indicate greater skill in attending to their feelings and moods. clarity: Higher scores indicate greater skill in experiencing their feelings clearly. repair: Higher scores indicate greater skill in regulating unpleasant moods or prolonging pleasant ones. Negative functioning variables: brooding: Higher scores indicate greater levels of brooding. worry: Higher scores indicate greater levels of worry. Outcome variables: wellbeing: Higher scores indicate higher levels of psychological wellbeing in terms of self-acceptance, positive relations with others, autonomy, environmental mastery, purpose in life, and personal growth. gad: higher scores indicate greater severity of Generalised Anxiety Disorder. 5.2.2 Visualisation There are a number of continuous variables in the dataset that we can visualise with density plots or histograms. We've done this individually, variable by variable in past worksheets. Here I'd like to show you a more advanced way of plotting. The code below uses will create a density plot of every variable in a dataset that is numeric (continuous) in nature, using different facets: # Plot density plots of all the numeric (continuous) variables # The code below: # -Keeps only the numeric columns in a data frame # -Uses pivot_longer() to code each set of scores by its variable name # -Specifies the &#39;score&#39; to plot in aes() # -Uses facet_wrap() to plot each variable in a separate panel # -Uses scales = &quot;free&quot; to allow the range on the x- and y-axis to be different across panels pwb_data %&gt;% keep(is.numeric) %&gt;% pivot_longer(everything(), names_to = &quot;variable&quot;, values_to = &quot;score&quot;) %&gt;% ggplot(aes(score)) + facet_wrap(~ variable, scales = &quot;free&quot;) + geom_density() Figure 2.1: Density plots for each continous predictor in pwb_data 5.2.3 Correlations With many variables being used in a multiple regression, it is good practice to inspect the correlations between all continuous variables to get an idea of the inter-relations and to check for multicollinearity between predictors. Obtain a correlation matrix of all of the numeric variables. Ensure the corrr package is loaded, then use correlate(): # library(corrr) # ensure this is loaded # correlations of all numeric variables # use mutate() with round() to round to 2 D.P. pwb_data %&gt;% keep(is.numeric) %&gt;% correlate(method = &quot;pearson&quot;) %&gt;% mutate(across(where(is.numeric), round, digits = 2)) Exercise 5.1 Before conducting the hierarchical regression, Iani et al. (2019) reported the Pearson correlations between wellbeing and gad and the mindfulness and emotional intelligence variables. We'll report a subset of those here. Report the correlations below to two decimal places: describing and clarity, r = describing and wellbeing, r = repair and wellbeing, r = nonreactivity and brooding, r = nonreactivity and gad, r = Does multicollinearity seem an issue (check the correlations between the predictors for r &lt; -0.8 or r &gt; 0.8)? yesno 5.2.4 Hierarchical regression To conduct the hierarchical regression, variables will be entered to the multiple regression in progressive steps, and the change in R2 associated with the predictors added to the model at each step obtained. Likewise, Bayes factors can be used to determine whether there is evidence that the predictors added at each step make a unique contribution to the prediction of the outcome variable. Iani et al. (2019) wanted to explain variance in wellbeing (the outcome variable). They reported the non-adjusted R2, so that's what we'll do too. Due to theoretical considerations, the variables in each step were entered as follows: Step 1: brooding Step 2: brooding, worry Step 3: brooding, worry and mindfulness variables (describing, observing, acting, nonreactivity, nonjudging) Step 4: brooding, worry, mindfulness variables (describing, observing, acting, nonreactivity, nonjudging), and emotional intelligence variables (attention, clarity, repair). 5.2.4.1 Step 1: brooding First, use brooding to predict wellbeing. Use lm() and glance() to obtain the R2 for the model, then use lmBF() to obtain the BF for the model. # specify the model in the step step1 &lt;- lm(wellbeing ~ brooding, data = pwb_data) # R^2^ # ensure broom package loaded, i.e., &#39;library(broom)&#39; glance(step1) # store the BF # library(BayesFactor) BF_step1 &lt;- lmBF(wellbeing ~ brooding, data = data.frame(pwb_data)) # look at BF BF_step1 r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.189052 0.1763809 11.4776 14.91998 0.0002642 1 -253.7007 513.4014 519.9704 8431.064 64 66 ## Bayes factor analysis ## -------------- ## [1] brooding : 95.76111 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS The R2 (non-adjusted, to 2 decimal places) for the model in Step 1 = . The BF for the model in Step 1 = 5.2.4.2 Step 2: brooding + worry Next, add worry to the model in Step 1, using + worry and look at R2 again and obtain the BF. We'll look at: whether the model in Step 2 explains more variance in wellbeing by looking at whether R2 increases. whether there's evidence for a contribution of worry after controlling for brooding, by dividing the Bayes factor for the model in Step 2 by the BF for the model in Step 1 # specify the model in the step step2 &lt;- lm(wellbeing ~ brooding + worry, data = pwb_data) # R-sq glance(step2) # store the BF BF_step2 &lt;- lmBF(wellbeing ~ brooding + worry, data = data.frame(pwb_data)) # compare the BFs for the models in Step 2 and Step 1 BF_step2 / BF_step1 r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.3288693 0.3075636 10.52393 15.43572 3.5e-06 2 -247.4558 502.9116 511.6702 6977.446 63 66 ## Bayes factor analysis ## -------------- ## [1] brooding + worry : 56.11175 ±0% ## ## Against denominator: ## wellbeing ~ brooding ## --- ## Bayes factor type: BFlinearModel, JZS The R2 for the model in Step 2 is The increase in R2 associated with the addition of worry to the model is Hint. To calculate this, take the R2 for the model in step 2 and subtract the R2 for the model in Step 1. The BF for the contribution of worry to the model is . Hint. This is the BF produced by dividing the BF for the model in Step 2, by the BF for the model in Step 1. calculatoR Did you know that R can function like a calculator too? Simply type the formula next to &gt; in the console window and hit enter, e.g., &gt; 2 + 2 Or use code in your script: 0.33 - 0.19 Change in R-squared symbol In reports and articles, you will often see the change in R2 written as \\(\\Delta R^2\\). The \\(\\Delta\\) symbol means \"change\". For example \\(\\Delta R^2 = 0.33\\). 5.2.4.3 Step 3: brooding + worry + mindfulness variables Next, add the variables associated with mindfulness to the model. The mindfulness measures are observing, describing, acting, nonjudging, and nonreactivity. Add these five variables to the model all in the same step. As before, note R2 for the model and the BF. To determine whether there's evidence for the addition of the mindfulness variables, compare the BF for the model in Step 3 and the BF of the model in Step 2. # specify the model in the step step3 &lt;- lm(wellbeing ~ brooding + worry + observing + describing + acting + nonjudging + nonreactivity, data = pwb_data) # R-sq glance(step3) # store the BF BF_step3 &lt;- lmBF(wellbeing ~ brooding + worry + observing + describing + acting + nonjudging + nonreactivity, data = data.frame(pwb_data)) # compare the BFs for the models in step 3 and step 2 BF_step3 / BF_step2 r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.5330601 0.4767053 9.148738 9.458999 1e-07 7 -235.4846 488.9692 508.6761 4854.566 58 66 ## Bayes factor analysis ## -------------- ## [1] brooding + worry + observing + describing + acting + nonjudging + nonreactivity : 35.49308 ±0% ## ## Against denominator: ## wellbeing ~ brooding + worry ## --- ## Bayes factor type: BFlinearModel, JZS The R2 for the model in Step 3 is The increase in R2 associated with the addition of the mindfulness variables is Hint. Take the R2 for the model in Step 3 and subtract the R2 for the model in Step 2. The BF for the contribution of the mindfulness variables to the model is Hint. This is the BF produced by dividing BF_step3 by BF_step2. After controlling for brooding and worry, is there sufficient evidence for the contribution of mindfulness to the prediction of wellbeing? yes, BF&gt;3no Explain The BF comparing the models in Steps 3 and 2 was BF = 34.59, indicating that the model in Step 3 is more than thirty five times more likely than the model in Step 2, given the data. Thus, there's substantial evidence that the mindfulness variables contribute to the prediction of wellbeing after controlling for brooding and worry. The mindfulness variables explain an additional R2 = 0.20, or 20%. of the variance in wellbeing, over and above brooding and worry. 5.2.4.4 Step 4: brooding + worry + mindfulness + emotional intelligence variables Next, add the variables associated with emotional intelligence to the model. The emotional intelligence variables are attention, clarity, and repair. These three variables are added to the model all in the same step. Once again, note R2 for the model and the BF. To determine whether there's evidence for the addition of the emotional intelligence variables, compare the BF for the model in Step 4 and the BF of the model in Step 3. # specify the model in step 4 step4 &lt;- lm(wellbeing ~ brooding + worry + observing + describing + acting + nonjudging + nonreactivity + attention + clarity + repair, data = pwb_data) # R-sq glance(step4) # store the BF BF_step4 &lt;- lmBF(wellbeing ~ brooding + worry + observing + describing + acting + nonjudging + nonreactivity + attention + clarity + repair, data = data.frame(pwb_data)) # compare the BFs for the models in step 4 and step 3 BF_step4 / BF_step3 r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.6045859 0.5326924 8.645487 8.409467 0 10 -229.9978 483.9956 510.2715 4110.944 55 66 ## Bayes factor analysis ## -------------- ## [1] brooding + worry + observing + describing + acting + nonjudging + nonreactivity + attention + clarity + repair : 2.150042 ±0% ## ## Against denominator: ## wellbeing ~ brooding + worry + observing + describing + acting + nonjudging + nonreactivity ## --- ## Bayes factor type: BFlinearModel, JZS The R2 for the model in Step 4 is The increase in R2 associated with the addition of the emotional intelligence variables is Hint. Take the R2 for the model in Step 4 and subtract the R2 for the model in Step 3. The BF representing the evidence for the unique contribution of the emotional intelligence variables to the model is After controlling for brooding, worry, and mindfulness, is there sufficient evidence for the contribution of emotional intelligence to the prediction of wellbeing? yes, BF &gt; 3no, BF &lt; 3 Explain The BF comparing the models in steps 4 and 3 was BF = 2.15. Although this indicates that the model in Step 4 is more than twice as likely than the model in Step 3, given the data, the BF is less than 3, and therefore falls short of the conventional level for declaring that there's substantial evidence for the addition of emotional intelligence. Thus, according to this Bayes factor analysis, there's insufficient evidence that the additional R2 = 0.07, or 7%, of the variance in wellbeing explained by emotional intelligence is meaningful. 5.3 Exercise 1 Exercise 5.2 Hierarchical regression Iani et al. (2019) were also interested in whether whether mindfulness and emotional intelligence predicted gad (anxiety symptoms) after controlling for brooding and worry. Repeat the analysis conducted above, but now with gad as the outcome variable. Step 1: brooding Step 2: brooding, worry Step 3: brooding, worry and mindfulness variables (describing, observing, acting, nonreactivity, nonjudging) Step 4: brooding, worry, mindfulness variables (describing, observing, acting, nonreactivity, nonjudging), and emotional intelligence variables (attention, clarity, repair). Step 1: brooding The R2 (non-adjusted, to two decimal places) for the model in Step 1 = The BF for the model in Step 1 = Solution - code # specify the model in step 1, store in gad1 gad1 &lt;- lm(gad ~ brooding, data = pwb_data) # R-sq glance(gad1) # store BF BF_gad1 &lt;- lmBF(gad ~ brooding, data = data.frame(pwb_data)) # show BF BF_gad1 Step 2: brooding + worry The R2 for the model in Step 2 is The increase in R2 associated with the addition of worry is The BF for the contribution of worry to the model is Hint Next, add worry to the model, using + worry. Look at R2 again and obtain the BF. To determine the R2 change for the model, take the R2 for the model in Step 2 and subtract the R2 value for the model in Step 1. To determine whether there's evidence for a contribution of worry after controlling for brooding, divide the Bayes factor for the model by the BF for the model in Step 1. Solution - code # specify the model in Step 2 gad2 &lt;- lm(gad ~ brooding + worry, data = pwb_data) # R sq glance(gad2) # store the BF BF_gad2 &lt;- lmBF(gad ~ brooding + worry, data = data.frame(pwb_data)) # compare the BFs for the models in Step 2 and Step 1 BF_gad2 / BF_gad1 Step 3: brooding + worry + mindfulness variables The R2 for the model in Step 3 is The increase in R2 associated with the addition of the mindfulness variables is The BF for the contribution of the mindfulness variables to the model is After controlling for brooding and worry, is there sufficient evidence for the contribution of mindfulness to the prediction of gad? yesno There's substantialinconclusive evidence for the model in Step 2, compared to Step 3, because the Bayes factor for the model in Step 3 divided by that of the model in Step 2 is less than 0.33equal to 1greater than 3. Hint Add the variables associated with mindfulness to the model. The mindfulness measures are observing, describing, acting, nonjudging, and nonreactivity. These five variables are added to the model all in the same step. To calculate the increase in R2, take the R2 for the model in Step 3 and subtract the R2 for the model in Step 2. To obtain the BF for the contribution of the mindfulness variables, take the BF for the model in Step 3 and divide it by the BF for the model in Step 2. If the BF is greater than 3, there's substantial evidence for the model in Step 3. If the BF &lt; 0.33, then there's substantial evidence for the model in Step 2. Intermediate BFs are inconclusive. Solution - code # specify the model in Step 3 gad3 &lt;- lm(gad ~ brooding + worry + observing + describing + acting + nonjudging + nonreactivity, data = pwb_data) # R-sq glance(gad3) # store the BF BF_gad3 &lt;- lmBF(gad ~ brooding + worry + observing + describing + acting + nonjudging + nonreactivity, data = data.frame(pwb_data)) # compare the BFs for the models in Step 3 and Step 2 BF_gad3 / BF_gad2 Step 4: brooding + worry + mindfulness + emotional intelligence variables The R2 for the model in Step 4 is The increase in R2 associated with the addition of the emotional intelligence variables to the model is . The BF associated with the contribution of the emotional intelligence variables to the model is After controlling for brooding, worry, and mindfulness, is there sufficient evidence for the contribution of emotional intelligence to the prediction of gad? yesno Hint Add the variables associated with emotional intelligence to the model. The emotional intelligence measures are attention, clarity, and repair. These three variables are added to the model all in the same step. To calculate the increase in R2, take the R2 for the model in Step 4 and subtract the R2 for the model in Step 3. To obtain the BF for the contribution of the emotional intelligence variables, take the BF for the model in Step 4 and divide it by the BF for the model in Step 3. If the BF is greater than 3, there's substantial evidence for the model in Step 4. If the BF &lt; 0.33, then there's substantial evidence for the model in Step 3. Intermediate BFs are inconclusive. Solution - code # specify the model in Step 4 gad4 &lt;- lm(gad ~ brooding + worry + observing + describing + acting + nonjudging + nonreactivity + attention + clarity + repair, data = pwb_data) # R-sq glance(gad4) # store the BF BF_gad4 &lt;- lmBF(gad ~ brooding + worry + observing + describing + acting + nonjudging + nonreactivity + attention + clarity + repair, data = data.frame(pwb_data)) # compare the BFs for the models in Step 3 and Step 2 BF_gad4 / BF_gad3 In summary, regarding the hypothesis of Iani et al. (2019): After controlling for brooding and worry, there's evidence for the contribution of mindfulness and emotional intelligence to the prediction of gad. noyes 5.4 Worked Exercise 2: Controlling for categorical variables All of the variables considered in the previous exercise were continuous (or at were least assumed to be). It is also possible to use hierarchical regression to control for the influence of variables that are categorical in nature. How do human-animal relationships affect mental health? Ratschen et al. (2020) looked at the impact of relationships with animals (e.g., pets) on mental health and loneliness in individuals during the Covid-19 pandemic. Data from their study are located at the link below. https://raw.githubusercontent.com/chrisjberry/Teaching/master/5_animal_data.csv More on the data The data are publicly available and variable names have been changed here for clarity. Missing data have been dealt with differently to the way the researchers dealt with them, so the data you are analysing (and therefore the results) are not identical to theirs. About the data: comfort: Comfort from companion animals. Higher scores indicate greater comfort from the companion animal. mental_health_pre: Mental health before lockdown. Higher scores indicate better mental health. mental_health_since: Mental health during lockdown. Higher scores indicate better mental health. wellbeing_since: Higher scores indicate better wellbeing. gender: Male, female, another option, or prefer not to say age: Age group. partner: Whether the participant lives with their partner or not. species: The type of animal companion. loneliness_pre: Loneliness before lockdown. Higher scores indicate greater loneliness. loneliness_since: Loneliness during lockdown. Higher scores indicate greater loneliness. 5.4.1 Visualisation Inspect the distributions of the continuous variables: # Read in the data animal_data &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/5_animal_data.csv&#39;) # Plot density plots of all the numeric (continuous) variables # The code below: # -Keeps only the numeric columns in a data frame # -Uses pivot_longer() to code each set of scores by its variable name # -Specifies the &#39;score&#39; to plot in aes() # -Uses facet_wrap() to plot each variable in a separate panel # -Uses scales = &quot;free&quot; to allow the range on the x- and y-axis to be different across panels animal_data %&gt;% keep(is.numeric) %&gt;% pivot_longer(everything(), names_to = &quot;variable&quot;, values_to = &quot;score&quot;) %&gt;% ggplot(aes(score)) + facet_wrap(~ variable, scales = &quot;free&quot;) + geom_histogram() Figure 5.1: Histogram plots for each continous predictor The code can be modified to obtain histograms of all of the categorical variables: # Plot histograms of all the categorical vars (i.e., count data) # The code: # -Keeps only the character columns in the dataset # -Uses pivot_longer() to code each set of scores by variable # -Specifies the &#39;score&#39; to plot in aes() # -Uses facet_wrap() to plot in separate panels # -Tells geom_histogram() to plot count data animal_data %&gt;% keep(is.character) %&gt;% pivot_longer(everything(), names_to = &quot;variable&quot;, values_to = &quot;score&quot;) %&gt;% ggplot(aes(score)) + facet_wrap(~ variable, scales = &quot;free&quot;) + geom_histogram(stat = &quot;count&quot;) Figure 5.2: Histograms for each categorical predictor By looking at the histograms of the categorical variables, answer the following: Age: The majority of people were in age group 18_2425_3435_4445_5455_6465_7070_over Gender: The majority of people were in another wayfemalemaleprefer not to say Partner: The majority of people were living with partnernot_living_with_partner Species: The most common animal companion was birdcatdogfishhorseotherreptilesmall The scores on the loneliness_pre variable range from 3 to 9, so there are only 7 possible scores. When there are many data points, this can create issues when visualising the variable in a scatterplot because there are only so many combinations of scores for the variable, so they overlap. The code below uses the gridExtra package to display two plots created with ggplot(). Each plot is stored in a separate variable panel1 and panel2, then placed side-by-side using grid.arrange() from the gridExtra package: # load library(gridExtra) # for displaying different plots on multiple panels library(gridExtra) # create a scatterplot of loneliness scores # without random jittering of points # store in panel1 panel1 &lt;- animal_data %&gt;% ggplot(aes(x=loneliness_pre, y = loneliness_since)) + geom_point() + geom_smooth(method=&quot;lm&quot;, se = F) + ggtitle(&quot;Without geom_jitter()&quot;) # create a scatterplot of loneliness scores # with random jittering of points using geom_jitter() # store in panel2 panel2 &lt;- animal_data %&gt;% ggplot(aes(x=loneliness_pre, y = loneliness_since)) + geom_jitter() + geom_smooth(method=&quot;lm&quot;, se = F)+ ggtitle(&quot;With geom_jitter()&quot;) grid.arrange(panel1, panel2, nrow = 1) Figure 2.3: Using geom_jitter(): loneliness_since vs. londliness_pre Using geom_jitter() instead of geom_point() means that the scores will be randomly jittered by a tiny amount. This reduces overlap, making it much easier to see how the scores are distributed. It's useful to use geom_jitter() when the response variable is on an ordinal scale, but the responses are discrete (e.g., 1, 2, 3, 4, 5), as is often the case with survey data and likert scales. Thus, if you ever create a scatterplot of survey data and it ends up looking like the plot on the left, try using geom_jitter() instead of geom_point(). The next step isn't strictly necessary because the categorical variables are stored as character variables (i.e., &lt;chr&gt;), but it is good practice to convert the categorical variables to factors before the analysis. # Convert categorical variables to factors # (remember, age is grouped, so is categorical here) animal_data &lt;- animal_data %&gt;% mutate(gender = factor(gender), age = factor(age), partner = factor(age), species = factor(species)) 5.5 Exercise 2 Exercise 5.3 Hierarchical regression Using the animal_data, we'll look at whether comfort scores predict mental_health_pre after controlling for gender, age, partner, species and loneliness_pre. How many steps do you think there'll be in the hierarchical regression analyses? 1234567 Explain The variables we want to control for can go into the model in the Step 1. Then we can add comfort to the model in Step 2, to see whether it explains mental_health_pre over and above all the variables in Step 1. The variables that are controlled for in a multiple regression are sometimes called covariates Step 1: Covariates only R2 for the model containing the covariates only is (to three decimal places; this level of precision is required for this answer) The BF for the model containing the covariates only is &lt; 0.33between 0.33 and 3&gt; 10000 Hint Run a regression to predict mental_health_pre from the covariates only. The covariates are the things we want to control for, and are gender, age, partner, species and loneliness_pre. Obtain R2 for the model using glance() in the broom package. Obtain the BF for the model using lmBF() in the BayesFactor package. Solution # store model with covariates only step1_covariates &lt;- lm(mental_health_pre ~ gender + age + partner + species + loneliness_pre, data = animal_data) # R-sq glance(step1_covariates) # Store the BF BF_step1_covariates &lt;- lmBF(mental_health_pre ~ gender + age + partner + species + loneliness_pre, data = data.frame(animal_data)) # View the BF for step 1 BF_step1_covariates Step 2: Covariates plus comfort The R2 for the model with the covariates and the comfort predictor is (report to three decimal places) The increase in R2 as a result of the addition of comfort to the model with the covariates is Hint: subtract the R2 for Step 1 from that of Step 2. It's necessary to use the R2 values to three decimal places because the increase is so small! The Bayes factor for the comparison of the model in Step 2 and Step 1 is &lt; 0.33between 0.33 and 3&gt; 3 Is there sufficient evidence that comfort from animal companions explains mental health levels before lockdown, after controlling for gender, age, partner, species of animal, and loneliness_pre? no, BF &lt; 0.33no, BF between 0.33 and 3yes, BF &gt; 3 Individuals who reported deriving greater comfort from animals also tended to have higherlower levels of mental health, as measured before lockdown. Hint Run a regression to predict mental_health_pre from the covariates and comfort. The covariates are the things we want to control for, and are gender, age, partner, species and loneliness_pre. Obtain R2 for the model using glance() in the broom package. Obtain the BF for the model using lmBF() in the BayesFactor package. Calculate the difference in R2 for the model in Step 2 and the model in Step 1 Divide the BF for the model in Step 2 by the BF for the model in Step 1 to obtain the BF representing the evidence for the contribution of comfort to the model, after controlling for the covariates. To determine whether the association between comfort and mental_health_pre is positive or negative, look at the sign on the coefficient for comfort by using step2_full. Solution - code # covariates + comfort step2_full &lt;- lm(mental_health_pre ~ comfort + gender + age + partner + species + loneliness_pre, data = animal_data) # R-sq glance(step2_full) # store BF BF_step2_full &lt;- lmBF(mental_health_pre ~ comfort + gender + age + partner + species + loneliness_pre, data = data.frame(animal_data)) # evidence for comfort, controlling for covariates BF_step2_full / BF_step1_covariates # look at the sign on the coefficient for comfort step2_full 5.6 Further knowledge and exercises 5.6.1 Standardised Coefficients Standardised coefficients Iani et al. (2019) also reported the values of the coefficients for the predictors at each step (see their Table 2). When a multiple regression has been performed using the raw data, the coefficients given by lm() are unstandardised. This means that they are in the same units as the predictor variable they correspond to. For example, if the coefficient for brooding is -1.57, this means that a 1 unit increase in brooding score is associated with a 1.57 decrease in wellbeing score. We'd like to be able to compare the coefficients of predictors to get some idea of their relative strength of the contribution to the model. The trouble is that predictors are often measured on different scales, with different ranges. For example, scores of brooding range from 5 to 20, and those of clarity range from 10 to 40. Use summary(pwb_data) to see this. Because the scales are so different, it doesn't make sense to directly compare the coefficients of the predictors. To compare the coefficients of predictor variables in a model, we need to compare the standardised regression coefficients. These are the coefficients derived from the data after the scores of each predictor have been standardised. To standardise the scores of a variable, subtract the mean value from each score, and then divide each score by the standard deviation of the scores. The scale() function does this automatically for us. To standardise all the numeric variables in the pwb_data: # Store the result in std_pwb_data # Take pwb_data, pipe it to # mutate_if(). # Tell mutate_if() to standardise a column # using &#39;scale&#39; if the variable type &#39;is.numeric&#39; # (note, it&#39;s not possible to standardise non-numeric variables) std_pwb_data &lt;- pwb_data %&gt;% mutate_if(is.numeric, scale) Now re-run Step 4 (i.e., the final model) of the hierarchical regression in Iani et al. (2019), but with std_pwb_data instead of pwb_data: # run the regression using standardised data std_step4 &lt;- lm(wellbeing ~ brooding + worry + observing + describing + acting + nonjudging + nonreactivity + attention + clarity + repair, data = std_pwb_data) # look at standardised coefficients std_step4 The standardised coefficients are called the beta coefficients, and have the symbol \\(\\beta\\). Beta coefficients range from -1 to +1, so the zero is usually omitted when reporting, e.g., \\(\\beta(brooding) = -.12\\) Make a note of the standardised (beta) coefficients for the model in Iani et al. (2019): brooding = worry = observing = describing = acting = nonjudging = nonreactivity = attention = clarity = repair = As with the unstandardised coefficients, the sign on the beta coefficient indicates the direction of the association with the outcome variable (i.e., positive or negative). Because the beta coefficients are now on the same scale, their magnitudes (i.e., their absolute size, ignoring the sign) can be compared to determine the relative \"importance\" of each predictor. For example, describing has the largest beta coeffcient (.38); it therefore makes the greatest contribution to the prediction of wellbeing in the full model. clarity makes the smallest contribution (beta = .02). Note, some of the beta coefficients differ slightly from those reported by Iani et al. (2019). This is most likely due to differences in rounding introduced during standardisation by different software packages. The values are very close though and the ordinal pattern in the beta coefficients is the same. 5.6.2 Prediction Prediction The final model can be used to predict new data points (as in earlier sessions). For the model with continuous predictors: # specify data for new ppt new_pwb &lt;- tibble( brooding = 6, worry = 12, observing = 12, describing = 18, acting = 17, nonjudging = 20, nonreactivity = 19, attention = 30, clarity = 27, repair = 27 ) # use augment() in broom package. # .fitted = predicted wellbeing value augment(step4, newdata = new_pwb) The predicted value of wellbeing for the new participant in new_pwb is . For the model with categorical predictors: # specify data for new ppt new_animal_dat &lt;- tibble( comfort = 45, gender = &#39;female&#39;, age = &#39;18_24&#39;, partner = &#39;living_partner&#39;, species = &#39;cat&#39;, loneliness_pre = 7) # use augment() in broom package. # .fitted = predicted value augment(step2_full, newdata = new_animal_dat) The predicted value of mental_health_pre for the new participant in new_animal_dat is . 5.6.3 Residuals Residuals As in earlier sessions, the residuals can be inspected in a plot of the predicted values vs. the residuals: # scatterplot of the predicted outcome values vs. residuals augment(step4) %&gt;% ggplot(aes(x=.fitted, y=.resid)) + geom_point()+ geom_hline(yintercept=0) We can also inspect the histogram of the residuals for normality: # histogram of the residuals augment(step4) %&gt;% ggplot(aes(x=.resid)) + geom_histogram() 5.7 Summary Hierarchical regression In hierarchical regression, predictors are added to a regression model in successive steps. It can be used to test particular theories or hypotheses. It can also be used to control the influence of particular variables before analysing whether a predictor variable (or set of variables) of interest explains the outcome variable. As before, lm() and glance() can be used to obtain R2 for the model at each step. The change in R2 associated with each step can be obtained to determine the unique contribution of the predictors in each step. lmBF() can be used to obtain the Bayes factor for the model in each step. Bayes factors of models from successive steps can be compared to determine the evidence for the unique contribution of predictors in a step. Plotting tips: Use geom_jitter() to make scatterplots with overlapping points easier to interpret. Useful for survey data. Use keep() with pivot_longer(), ggplot() and facet_wrap() to plot lots of variables of a certain type (e.g., numeric or character) on separate panels. Use grid.arrange() in the gridExtra package to display figures on separate panels. 5.8 References Iani, L., Quinto, R. M., Lauriola, M., Crosta, M. L., &amp; Pozzi, G. (2019). Psychological well-being and distress in patients with generalized anxiety disorder: The roles of positive and negative functioning. PloS ONE, 14(11), e0225646. https://doi.org/10.1371/journal.pone.0225646 Ratschen E., Shoesmith E., Shahab L., Silva K., Kale D., Toner P., et al. (2020) Human-animal relationships and interactions during the Covid-19 lockdown phase in the UK: Investigating links with mental health and loneliness. PLoS ONE, 15(9):e0239397. https://doi.org/10.1371/journal.pone.0239397 "],["anova2.html", "Session 6 ANOVA: Repeated measures 6.1 Overview 6.2 One-way repeated measures ANOVA 6.3 Long format data 6.4 Worked Example 6.5 Two-way within subjects ANOVA 6.6 Exercise: two-way mixed ANOVA 6.7 Further knowledge 6.8 Summary 6.9 References", " Session 6 ANOVA: Repeated measures Chris Berry 2022 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} 6.1 Overview Slides from the lecture part of the session: Download Previously, we saw that when all of the predictor variables in a multiple regression are categorical then the analysis has the name ANOVA (in Session 3). Here we will analyse repeated measures (or within-subjects) designs. In a repeated measures design, the scores for the different levels of an independent variable come from the same participants, rather than separate groups of participants (as was the case with between-subjects designs in Session 3). We consider two types of ANOVA for within-subjects designs: one-way ANOVA and two-way factorial ANOVA. The exercise at the end is for a mixed factorial design, where one of the factors is manipulated between-subjects, and the other is manipulated within-subjects. 6.2 One-way repeated measures ANOVA A one-way repeated measures ANOVA is used to compare the scores from a dependent variable for the same individuals across different time points. For example, do depression scores differ in a group of individuals across three time points: before CBT therapy, during therapy, and 1 year after therapy? One-way within-subjects ANOVA refers to the same analysis. The scores for each level of the independent variable manipualted within-subjects come from the same group of participants. For example, we can say that the independent variable of time is manipulated within-subjects. One-way means that there is one independent variable, for example, time. Independent variables are also called factors in ANOVA. A factor is made up of different levels. Time could have three levels: before, during and after therapy. In repeated measures/within-subjects designs, these levels are often referred to as conditions. Repeated measures means that the same dependent variable (depression score) is measured multiple times on the same participant. Here, it's measured at different time points. Each participant provides multiple measurements. 6.3 Long format data To conduct a repeated measures ANOVA in R, the data must be in long format. This is crucial. Exercise 6.1 Wide vs. long format When the data are in wide format, all of the data for a single participant is stored: on one row, across multiple columns on multiple rows, with each row representing a separate observation When the data are in long format, all of the data for a single participant is stored: on one row, across multiple columns on multiple rows, with each row representing a separate observation 6.4 Worked Example In an investigation of language learning, Chang et al., (2021) provided neuro-feedback training to 6 individuals attempting to discriminate particular sounds in a foreign language. Performance was measured as the proportion of correct responses in the discrimination task, and was measured at three time points relative to when the training was administered: before training was given (pre), after 3 days (day3), and after two months (month2). The data are at the link below: https://raw.githubusercontent.com/chrisjberry/Teaching/master/6a_discrimination_test.csv Exercise 6.2 Design check. What is the independent variable (or factor) in this design? proportion correcttime since training How many levels does the factor have? 1234 What is the dependent variable? proportion correcttime What is the nature of the independent variable? categoricalcontinuous Is the independent variable manipulated within- or between-subjects? between-subjectswithin-subjects 6.4.1 Read in the data # library(tidyverse) # read in the data discriminate_wide &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/6a_discrimination_test.csv&#39;) # look at the data discriminate_wide ppt pre day3 month2 1 0.56 0.63 0.65 2 0.71 0.79 0.76 3 0.53 0.86 0.92 4 0.56 0.81 0.77 5 0.49 0.85 0.84 6 0.55 0.84 0.84 Columns: ppt: the participant number pre: proportion correct before training day3: proportion correct 3 days after training month2: proportion correct 2 months after training Exercise 6.3 Data check How many participants are there in this dataset? 34618 Are the data in long format or wide format? longwide Explain The data for each of the 6 participants is on a different row in discriminate_wide, with the scores for each level of the independent variable time in different columns. The data are therefore in wide format. 6.4.2 Convert the data to long format Data must be in long format in order to be analysed using a repeated measures ANOVA. Use pivot_longer() to convert the data to long format: # The code below: # - stores the result in discriminate_long # - takes discriminate_wide and pipes it to # - pivot_longer() # - specifies which of the existing columns to make into a new single column # - specifies the name of the new column labeling the conditions # - specifies the name of the column containing the dependent variable discriminate_long &lt;- discriminate_wide %&gt;% pivot_longer(cols = c(&quot;pre&quot;, &quot;day3&quot;, &quot;month2&quot;), names_to = &quot;time&quot;, values_to = &quot;performance&quot;) # look at the first 6 rows discriminate_long %&gt;% head() ppt time performance 1 pre 0.56 1 day3 0.63 1 month2 0.65 2 pre 0.71 2 day3 0.79 2 month2 0.76 Exercise 6.4 Data check - long How many participants are there in discriminate_long? 24618 Are the data in discriminate_long in long format or wide format? longwide How many rows or observations are there in discriminate_long? 34618 Explain Each score is now on a separate row. There is a column to code the participant and condition. The data are therefore in long format. Each participant contributes 3 scores (one for pre, day3 and month2). In discriminate_long, what type of variable is ppt currently? factor character double (numeric) Hint. Look at the label at the top of each column, e.g., &lt;dbl&gt; In discriminate_long, what type of variable is time currently? factor character double (numeric) 6.4.3 Convert the participant and the categorical variable to factors As with the analysis of categorical variables in regression and ANOVA, each categorical variable must be converted to a factor before the analysis. Importantly, in repeated measures designs, the column of participant labels ppt must also be converted to a factor. Because the levels of time are ordered, we can additionally specify the way the levels should be ordered by using levels = c(\"level_1_name\", \"level_2_name\", \"level_3_name\") when using factor(). # use mutate() to transform # both ppt and time to factor # and overwrite existing variables discriminate_long &lt;- discriminate_long %&gt;% mutate(ppt = factor(ppt), time = factor(time, levels = c(&quot;pre&quot;, &quot;day3&quot;, &quot;month2&quot;) ) ) # check discriminate_long %&gt;% head() ppt time performance 1 pre 0.56 1 day3 0.63 1 month2 0.65 2 pre 0.71 2 day3 0.79 2 month2 0.76 Exercise 6.5 Data check - factors After using mutate() and factor() above: In discriminate_long, what type of variable is ppt? factor character double (numeric) In discriminate_long, what type of variable is time? factor character double (numeric) 6.4.4 Visualise the data Look at the distribution of the dependent variable in each condition. discriminate_long %&gt;% ggplot(aes(x = performance)) + facet_wrap(~ time) + geom_density() Figure 6.1: Density plots of performance at each time point The density plots look a little 'lumpy'. Remember that in our sample, there are only six data points per condition, so the plots are unlikely to be representative of the true (population) distribution of scores! 6.4.5 Plot the means # load the ggpubr package library(ggpubr) # use ggline() to plot the means discriminate_long %&gt;% ggline(x = &quot;time&quot; , y = &quot;performance&quot;, add = &quot;mean&quot;) + ylab(&quot;Performance&quot;) + ylim(c(0,1)) Figure 1.2: Mean proportion correct at each time point Error bars Previously, we used desc_stat = \"mean_se\" to add errorbars to the plot. Doing so here would add error bars representing the standard error of the mean to each of the points in the plot. These are appropriate for between-subjects designs, but for within-subjects designs, these are not as appropriate. Refer to the Further Knowledge section at the end of the worksheet to see how to add within-subject error bars (and see Morey, 2008, for more detail). Exercise 6.6 Describe the trend shown in the line plot: Between pre and day3, performance appeared to improveshow no changedecline Between pre and month2, performance appeared to improveshow no changedecline Between day3 and month2, performance appeared to improveshow no changedecline 6.4.6 Descriptives: Mean of each condition Obtain the mean proportion correct at each time point using summarise(): discriminate_long %&gt;% group_by(time) %&gt;% summarise(M = mean(performance)) time M pre 0.5666667 day3 0.7966667 month2 0.7966667 6.4.7 Bayes factor The crucial difference in repeated measures designs compared to between-subjects ones concerns the column labeling the participant, ppt. ppt is entered into the model as another predictor variable, i.e., using ...+ ppt. ppt is specified as a random factor using whichRandom =. This means that the model comprising the other predictors in the model will be evaluated relative to the null model containing ppt alone. ppt is not individually evaluated. The Bayes factors can be obtained with anovaBF() but can also be obtained (less conveniently) using lmBF(). Using anovaBF(): # library(BayesFactor) anovaBF(performance ~ time + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(discriminate_long)) ## Bayes factor analysis ## -------------- ## [1] time + ppt : 331.4158 ±0.66% ## ## Against denominator: ## performance ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS In the output, the Bayes factor provided is for the model of performance ~ time + ppt vs. the model of performance ~ ppt. The latter is the null model containing only ppt as a predictor variable. The Bayes factor therefore represents evidence for the (unique) effect of time on performance, over and above a model containing ppt only. This differs to between-subject designs, where the null model (in the denominator) is the intercept-only model (representing the grand mean). Exercise 6.7 The Bayes factor for the effect of time on performance is This indicates that (select one): it is approximately 300 times more likely that there is difference between the means of the three time conditions, compared to there being no difference. it is approximately 300 times more likely that there is no difference between the means of the three time conditions, compared to there being a difference. On the basis of the Bayes factor analysis, does neuro-feedback training affect performance in the language learning task? yesno Equivalent BFs using lmBF() It is possible to obtain the same result as anovaBF() using lmBF(). Doing so can help us to understand what anovaBF() is doing. Again, we must tell lmBF() that ppt is a random factor, using whichRandom =. # Equivalent results with lmBF() # model with time and ppt only BF_time_ppt &lt;- lmBF(performance ~ time + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(discriminate_long)) # model with ppt only BF_ppt_only &lt;- lmBF(performance ~ ppt, whichRandom = &quot;ppt&quot;, data = data.frame(discriminate_long)) # compare model with vs. without time BF_time_ppt / BF_ppt_only ## Bayes factor analysis ## -------------- ## [1] time + ppt : 332.6118 ±0.51% ## ## Against denominator: ## performance ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS This produces a Bayes factor of approximately 330. This is equivalent to the BF obtained with anovaBF(), though note that because of the random data sampling methods used to generate the Bayes factors, the value may differ by a small amount. 6.4.8 Follow-up tests The Bayes factor obtained with anovaBF() above tells us that there's a difference between the means of the conditions, but not which conditions differ from which. To determine which conditions differ from which, filter the data for the conditions of interest, then use anovaBF() in exactly the same way as before, but this time with relevant filtered data. 6.4.8.1 Pre vs. day3 conditions # pre vs. day3 # filter the data for the relevant conditions pre_v_day3 &lt;- discriminate_long %&gt;% filter(time == &quot;pre&quot; | time == &quot;day3&quot;) # # compare the means of the conditions, using anovaBF() anovaBF(performance ~ time + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(pre_v_day3)) ## Bayes factor analysis ## -------------- ## [1] time + ppt : 79.75876 ±0.59% ## ## Against denominator: ## performance ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS The Bayes factor for the comparison of mean performance in the pre and day3 conditions is . Note. Your BF may differ slightly from the one above due to the random sampling methods anovaBF() uses. This indicates that there is substantialinconclusive evidence for no differencea difference between the performance in the pre and day3 conditions. Performance at day3 was higherlower than performance pre training. 6.4.8.2 Pre vs. month2 conditions # pre vs. month2 # filter the data for these conditions pre_v_month2 &lt;- discriminate_long %&gt;% filter(time == &quot;pre&quot; | time == &quot;month2&quot;) # # compare the means of the conditions, using anovaBF() anovaBF(performance ~ time + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(pre_v_month2)) ## Bayes factor analysis ## -------------- ## [1] time + ppt : 55.06891 ±1.89% ## ## Against denominator: ## performance ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS The Bayes factor for the comparison of mean performance in the pre and month2 conditions is This indicates that there is substantialinconclusive evidence for no differencea difference between the mean performance in the pre and month2 conditions. Performance at month2 was higherlower than performance pre training. 6.4.8.3 day3 vs. month2 conditions # day3 vs. month2 # filter the data for these conditions day3_v_month2 &lt;- discriminate_long %&gt;% filter(time == &quot;day3&quot; | time == &quot;month2&quot;) # # compare the means of the conditions, using anovaBF() anovaBF(performance ~ time + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(day3_v_month2)) ## Bayes factor analysis ## -------------- ## [1] time + ppt : 0.4776541 ±1.94% ## ## Against denominator: ## performance ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS The Bayes factor for the comparison of mean performance in the day3 and month2 conditions is This indicates that there is substantialinconclusive evidence for an absence of a differencea difference between the mean performance in the day3 and month2 conditions. Thus, there's no evidence of a difference in performance between the day3 and month2 conditions. Equivalent results with lmBF() As before, it's possible to conduct the same comparisons and obtain the same BFs using lmBF(): # pre vs. day3 lmBF(performance ~ time + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(pre_v_day3)) / lmBF(performance ~ ppt, whichRandom = &quot;ppt&quot;, data = data.frame(pre_v_day3)) # pre vs. month2 lmBF(performance ~ time + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(pre_v_month2)) / lmBF(performance ~ ppt, whichRandom = &quot;ppt&quot;, data = data.frame(pre_v_month2)) # day3 vs. month2 lmBF(performance ~ time + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(day3_v_month2)) / lmBF(performance ~ ppt, whichRandom = &quot;ppt&quot;, data = data.frame(day3_v_month2)) ## Bayes factor analysis ## -------------- ## [1] time + ppt : 79.62382 ±0.67% ## ## Against denominator: ## performance ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS ## ## Bayes factor analysis ## -------------- ## [1] time + ppt : 54.85038 ±0.96% ## ## Against denominator: ## performance ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS ## ## Bayes factor analysis ## -------------- ## [1] time + ppt : 0.4636084 ±1.2% ## ## Against denominator: ## performance ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS 6.5 Two-way within subjects ANOVA In a two-way repeated measures or within-subjects ANOVA, there are two categorical independent variables or factors. When there are multiple factors, the ANOVA is referred to as factorial ANOVA. For example, if the design has two factors, and each factor has two levels, then we refer to the design as a 2 x 2 factorial design. The first number (2) denotes the number of levels of the first factor. The second number (2) denotes the number of levels of the second factor. If, instead, the second factor had three levels, we'd say we have a 2 x 3 factorial design. The cells of the design are produced by crossing the levels of one factor with each level of the other factor. If both factors are manipulated within-subjects, then the scores from each cell of the design come from the same group of participants. 6.5.1 Worked Example Kreysa et al. (2016) analysed RTs to statements made by faces that had different gaze directions - the eyes of the face were either averted to the left, averted to the right, or were looking directly at the participant when the statement was made. RTs were further broken down according to whether the participant had agreed with the statement or not (they responded 'yes' or 'no'). Exercise 6.8 Design check. What is the first independent variable (or factor) that is mentioned in this design? RTgaze directionagreement with statement How many levels does the first factor have? 1234 What is the second independent variable (or factor) that was mentioned? RTgaze directionagreement with statement How many levels does the second factor have? 1234 What is the dependent variable? RTgaze directionagreement with statement What is the nature of the independent variables? categoricalcontinuous What type of design is this? 2 x 2 x 2 repeated measures factorial design3 x 2 repeated measures factorial design2 x 2 repeated measures design 6.5.2 Read in the data Read in the data from the link below: https://raw.githubusercontent.com/chrisjberry/Teaching/master/6_gaze_data.csv gaze &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/6_gaze_data.csv&#39;) gaze %&gt;% head() ppt gaze_direction agreement RT logRT 1 averted_left no 725.6667 6.553344 1 averted_left yes 787.6667 6.615148 1 averted_right no 621.6667 6.389811 1 averted_right yes 752.6667 6.568404 1 direct no 1001.3333 6.829217 1 direct yes 688.8333 6.477116 Columns: ppt: the participant number gaze_direction: the gaze-direction of the face. agreement: whether the responses made to the statements were 'yes' or 'no' RT: mean response times logRT: the log transform of the RT column, i.e., log(RT) Are the data in long format or wide format? wide formatlong format Explain You can tell this is a repeated measures design and that the data are in long format because a given participant's scores are on multiple rows, and there are multiple scores on the dependent variable (RT) for each participant. 6.5.3 Visualise the data Reaction time data tend to be positively skewed. As result, Kreysa et al. (2016) analysed the RT data after it had been log transformed. The log transformed data are in the column logRT. To see the positive skew, inspect the (untransformed) RTs in RT: gaze %&gt;% ggplot(aes(x=RT)) + facet_wrap(~ gaze_direction * agreement) + geom_density() Figure 5.1: Mean RT according to gaze direction and agreement Now inspect the log transformed RTs in logRT: gaze %&gt;% ggplot(aes(x = logRT)) + facet_wrap(~ gaze_direction * agreement) + geom_density() Figure 5.2: Mean logRT according to gaze direction and agreement Does the positive skew in the distributions appear to be reduced by the log transformation? noyes 6.5.4 Plot the means ggline() in the ggpubr package can be used to plot the mean of each condition. # load the ggpubr package #library(ggpubr) # use ggline to plot the means gaze %&gt;% ggline(x = &quot;gaze_direction&quot; , y = &quot;logRT&quot;, add = &quot;mean&quot;, color= &quot;agreement&quot;) + ylab(&quot;Mean log RT&quot;) Figure 2.3: Mean logRT according to gaze direction and agreement It appears as if people took longer to respond when the gaze was direct and they said 'no' to the statement that was made. 6.5.5 Descriptive statistics - mean To obtain the mean of each condition, use group_by() to group the results of summarise() by the two factors in the design: # take the data in gaze # pipe to group_by() # group by the gaze_direction and agreement factors # create a table of the mean of each condition # and label the column &#39;M&#39; gaze %&gt;% group_by(gaze_direction, agreement) %&gt;% summarise(M = mean(RT)) gaze_direction agreement M averted_left no 793.5092 averted_left yes 818.7174 averted_right no 756.4577 averted_right yes 774.9986 direct no 970.6278 direct yes 800.4069 6.5.6 Convert participant and categorical variables to factors Convert the ppt, gaze_direction, and agreement variables to factors so that R will treat them as such in the ANOVA. # use mutate() to transform # ppt, gaze_direction and agreement to factors # and overwrite existing variables gaze &lt;- gaze %&gt;% mutate(ppt = factor(ppt), gaze_direction = factor(gaze_direction), agreement = factor(agreement)) # check changes gaze %&gt;% head() ppt gaze_direction agreement RT logRT 1 averted_left no 725.6667 6.553344 1 averted_left yes 787.6667 6.615148 1 averted_right no 621.6667 6.389811 1 averted_right yes 752.6667 6.568404 1 direct no 1001.3333 6.829217 1 direct yes 688.8333 6.477116 As before, you could double check that the relevant variable labels in gaze have changed to &lt;fct&gt;. 6.5.7 Bayes factor As with between-subjects two-way designs, researchers are interested in three things in a two-way repeated measures design: the main effect of factor1 the main effect of factor2 the interaction between factor1 and factor2, i.e., the factor1*factor2 interaction term. Obtain the Bayes factors for each of the above using anovaBF(). To specify the full model with the interaction, use factor1 * factor2. This will automatically specify a model with the main effects of factor1 and factor2, in addition to the interaction term: As before, add ppt to the model using ...+ ppt, and tell R it is a random factor using whichRandom = ppt. # obtain BFs for the anova BFs_gaze &lt;- anovaBF(logRT ~ gaze_direction*agreement + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(gaze) ) # look at BFs BFs_gaze ## Bayes factor analysis ## -------------- ## [1] gaze_direction + ppt : 26.22868 ±0.87% ## [2] agreement + ppt : 0.26817 ±2.12% ## [3] gaze_direction + agreement + ppt : 7.382171 ±1.57% ## [4] gaze_direction + agreement + gaze_direction:agreement + ppt : 101.4481 ±54.32% ## ## Against denominator: ## logRT ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS Interpretation of the output is similar as with two-way between-subjects ANOVA (Session 3). Notice, however, that ppt is included in each of models, and the comparison of each model is not against the intercept-only model as it was with between-subjects factorial ANOVA. Because this is a repeated measures design, each model is compared against a null model containing only ppt as a predictor. 6.5.7.1 Main effect of gaze_direction The main effect of gaze direction: BFs_gaze[1] ## Bayes factor analysis ## -------------- ## [1] gaze_direction + ppt : 26.22868 ±0.87% ## ## Against denominator: ## logRT ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS RTs tended not to differto differ between gaze_direction conditions, BF = . 6.5.7.2 The main effect of agreement BFs_gaze[2] ## Bayes factor analysis ## -------------- ## [1] agreement + ppt : 0.26817 ±2.12% ## ## Against denominator: ## logRT ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS RTs tended not to differto differ between agreement conditions, BF = . 6.5.7.3 The gaze_direction x agreement interaction BFs_gaze[4] / BFs_gaze[3] ## Bayes factor analysis ## -------------- ## [1] gaze_direction + agreement + gaze_direction:agreement + ppt : 13.74232 ±54.34% ## ## Against denominator: ## logRT ~ gaze_direction + agreement + ppt ## --- ## Bayes factor type: BFlinearModel, JZS There was substantial evidence forwas insufficient evidence forwas evidence of an absence of an interaction between gaze_direction and agreement, BF = . 6.5.8 Follow up comparisons Given the evidence for an interaction, Kreysa et al. (2016) conducted pairwise comparisons to compare \"yes\" and \"no\" responses in each gaze direction condition. This can be achieved by filtering the data for each gaze direction condition, and then using anovaBF() to compare scores in each agreement condition. If there were no evidence for the interaction, we wouldn't conduct further comparisons. 6.5.8.1 averted_left # filter data for averted_left condition averted_left &lt;- gaze %&gt;% filter(gaze_direction == &quot;averted_left&quot;) # use anovaBF() to compare the agreement conditions anovaBF(logRT ~ agreement + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(averted_left)) ## Bayes factor analysis ## -------------- ## [1] agreement + ppt : 0.2466716 ±0.85% ## ## Against denominator: ## logRT ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS There was substantial evidence for a differencean absence of a difference between the mean log RTs made to \"yes\" and \"no\" responses in the averted_left condition, BF = . 6.5.8.2 averted_right # filter data for averted right condition averted_right &lt;- gaze %&gt;% filter(gaze_direction == &quot;averted_right&quot;) # use anovaBF() to compare the agreement conditions anovaBF(logRT ~ agreement + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(averted_right)) ## Bayes factor analysis ## -------------- ## [1] agreement + ppt : 0.3535601 ±1% ## ## Against denominator: ## logRT ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS The Bayes factor comparing the mean log RTs to \"yes\" and \"no\" responses in the averted_right condition was inconclusive, BF = , although there was more evidence for the null hypothesis of there being no difference between conditions. 6.5.8.3 direct # filter data for direct condition direct &lt;- gaze %&gt;% filter(gaze_direction == &quot;direct&quot;) # use anovaBF() to compare the agreement conditions anovaBF(logRT ~ agreement + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(direct)) ## Bayes factor analysis ## -------------- ## [1] agreement + ppt : 33.84858 ±0.81% ## ## Against denominator: ## logRT ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS There was substantial evidence for a differencean absence of a difference between the mean log RTs made to \"yes\" and \"no\" responses in the direct condition, BF = . RTs tended to be longer in the direct condition when participants made a noyes response. 6.6 Exercise: two-way mixed ANOVA Exercise 6.9 Mixed ANOVA When one of the factors is manipulated within-subjects and the other is manipulated between-subjects, the design is said to be a mixed design. It is also sometimes called a split-plot design. Hammel and Chan (2016) looked at the duration (in seconds) that participants were able to balance on a balance-board on three successive occasions (trials). There were three groups of participants: one group (counterfactual) engaged in counterfactual thinking, another group (prefactual) engaged in prefactual thinking, and another group (control) engaged in an unrelated thinking task. The data (already in long format) are located at the link below: https://raw.githubusercontent.com/chrisjberry/Teaching/master/6_improve_long.csv 1. Create a line plot of trial vs. duration. Represent each group as a separate line. Balance duration seems highest in the counterfactualprefactualcontrol group. Balance duration seems lowest in the counterfactualprefactualcontrol group. Balance duration generally appears to become shorterremain constantbecome longer across trials. Hints convert ppt and the columns labeling the levels of the independent variables to factors. use ggline() in the ggpubr package to create the line plot. Solution - code # read in the data balance &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/6_improve_long.csv&#39;) # plot # # library(ggpubr) balance %&gt;% ggline(x = &quot;trial&quot;, y = &quot;duration&quot;, color = &quot;group&quot;, add = &quot;mean&quot;) 2. Bayes factors The main effect of group: BF = The main effect of trial: BF = The group x trial interaction: Note. It may take a short while for anovaBF() to determine the BFs. The processing time increases for more complex designs, especially those with repeated measures. Solution - code # convert ppt and independent variables to factors balance &lt;- balance %&gt;% mutate(ppt = factor(ppt), group = factor(group), trial = factor(trial)) # anovaBF #library(BayesFactor) bfs &lt;- anovaBF(duration ~ ppt + group + trial, whichRandom = &quot;ppt&quot;, data = data.frame(balance)) # main effect of group bfs[1] # main effect of trial bfs[2] # group*trial interaction bfs[4] / bfs[3] # You&#39;ll notice there is a large amount of error on the interaction term # If you have time to wait, then we can recompute the BF with a larger number of iterations, # which should reduce the error. The BFs come out very similarly, but with smaller error. # Try this: # bfs2 &lt;- recompute(bfs, iterations = 100000) # bfs2[1] # bfs2[2] # bfs2[4]/bfs2[3] 4. Follow up comparisons. Conduct follow-up comparisons to investigate the interaction further. Note, we wouldn't usually do this because there was evidence against there being an interaction between trial and group. In statistical terms, this means that the effect of trial is the same in each group, so there's no need to dig deeper into the data. Instead, the following is a pedagogical exercise only. Assuming there were an interaction, one approach to investigating it is, for each group, to conduct a one-way ANOVA to determine the effect of trial on duration. This is called a simple effects analysis. This is where we assess the effect of trial at each level of the second factor, in this case group. Conduct three one-way repeated measures ANOVAs, comparing duration across the three types of trial in each group. Prefactual group: There's substantial evidence for the effect of trial on duration, BF = . Counterfactual group: There's substantial evidence for the effect of trial on duration, BF = . Control group: There's insufficient evidence for the effect of trial on duration, BF = . Hint Use filter() to subset the data for each group. Use anovaBF() to conduct a one-way repeated measures ANOVA for each group. Solution - code # filter rows corresponding to prefactual group prefactual &lt;- balance %&gt;% filter(group == &quot;prefactual&quot;) # # one-way ANOVA, comparing trial_1, trial_2, trial_3 anovaBF(duration ~ trial + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(prefactual)) # filter rows corresponding to counterfactual group counterfactual &lt;- balance %&gt;% filter(group == &quot;counterfactual&quot;) # # one-way ANOVA, comparing trial_1, trial_2, trial_3 anovaBF(duration ~ trial + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(counterfactual)) # filter rows corresponding to control group control &lt;- balance %&gt;% filter(group == &quot;control&quot;) # # one-way ANOVA, comparing trial_1, trial_2, trial_3 anovaBF(duration ~ trial + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(control)) 6.7 Further knowledge Within-subject errorbars Here I show you how to plot the means with within-subject error bars (following Morey, 2008) for the first dataset in the worksheet discriminate_long. I use summarySEwithin() from the Rmisc package for this. The functions from this package have similar names to other tidyverse functions, which can lead to problems/conflicts later. For that reason, I unload Rmisc after using the functions from it, then reload the tidyverse. # within-subjects error bars # load Rmisc package for the summarySEwithin() function library(Rmisc) # get within-subject intervals # can also specify between-subjects vars with &#39;betweenvars = &#39; intervals_RM &lt;- summarySEwithin(discriminate_long, measurevar = &quot;performance&quot;, withinvars = &quot;time&quot;, idvar = &quot;ppt&quot;) # look at what&#39;s produced # intervals_RM # select time, performance and se columns # rename &#39;performance&#39; to M intervals_RM &lt;- intervals_RM %&gt;% select(time, performance, se) %&gt;% dplyr::rename(M = performance, SE = se) # compare with between subject intervals (they should be larger) # summarySE(discriminate_long, measurevar = &quot;performance&quot;, groupvars = &quot;time&quot;) # unload the Rmisc package due to conflicts with other packages detach(&quot;package:Rmisc&quot;, unload = TRUE) # now use the M and within-subjects SE to create a plot of means intervals_RM %&gt;% ggplot(aes(x = time, y = M)) + geom_point() + geom_errorbar(aes(ymin = M - SE, ymax = M + SE), width=.2, position = position_dodge(.9)) + theme_classic() Figure 6.2: Mean proportion correct at each time point. Error bars represent within-subjects standard error of the mean 6.8 Summary To analyse repeated measures data with ANOVA, the data must be in long format. Convert from wide to long format using pivot_longer(). Convert the columns containing the participant identifier (e.g., ppt) and independent variables to factors using factor(). Add ppt as a predictor in anovaBF(), and specify as a random factor using whichRandom = \"ppt\". Bayes factors given by anovaBF() compare each model against a null model with ppt only (as a random factor), rather than the intercept-only model (as was the case in between-subjects designs). Conduct follow-up tests using anovaBF() if there's evidence for an interaction. 6.9 References Chang, M., Ando, H., Maeda, T., Naruse, Y. (2021). Behavioral effect of mismatch negativity neurofeedback on foreign language learning. PLoS ONE, 16(7): e0254771. https://doi.org/10.1371/journal.pone.0254771 Hammell, C., Chan, A.Y.C. (2016). Improving physical task performance with counterfactual and prefactual thinking. PLoS ONE, 11(12): e0168181. doi:10.1371/journal.pone.0168181 Kreysa, H., Kessler, L., Schweinberger, S.R. (2016). Direct Speaker Gaze Promotes Trust in Truth-Ambiguous Statements. PLoS ONE, 11(9): e0162291. doi:10.1371/journal.pone.0162291 Morey, R. D. (2008). Confidence intervals from normalized data: A correction to Cousineau (2005). Reason, 4(2), 61-64. "],["prepost.html", "Session 7 Pre-post data, effect sizes, clinically significant change", " Session 7 Pre-post data, effect sizes, clinically significant change Chris Berry 2022 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} "],["assessment2022.html", "Session 8 Data Analysis Assessment 2022", " Session 8 Data Analysis Assessment 2022 Chris Berry 2022 The PDF instructions for the assessment are in the \"Analysis Assessment\" submission link, located in the \"Assessments\" section of the PSYC753 2021-22 DLE page. Answers to FAQs are on the following page. Slides for Rmd support "],["faqs.html", "Session 9 FAQs 9.1 What does it mean if RStudio prints out \"e-\" next to a number?", " Session 9 FAQs Chris Berry 2022 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} 9.1 What does it mean if RStudio prints out \"e-\" next to a number? If R prints \"1.4e-4\", in the output this actually means \"\\(1.4 \\times 10^{-4}\\)\", or \"0.00014\". It is a way of printing out very small (or very large) numbers. \"e-4\" means \"move the decimal point 4 places to the left\". Run the code below -- it should return \"0.00014\". 1.4e-4 So: \"e-5\" means move the decimal point 5 places to the left (e.g., 2.5e-5 is \"\\(2.5 \\times 10^{-5}\\)\", or 0.000025) \"e-10\" means move the decimal point 10 places to the left (e.g., 2.5e-10 is \"\\(2.5 \\times 10^{-10}\\)\", or 0.00000000025). \"e5\" means move the decimal point 5 places to the right (e.g., 2.5e5 is \"\\(2.5 \\times 10^{5}\\)\", 250000). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
