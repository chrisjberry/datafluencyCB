[["index.html", "PSYC761 Data Analysis Overview", " PSYC761 Data Analysis Chris Berry Overview This workbook contains worksheets for the eight PSYC761 sessions given by Chris Berry. These worksheets should be used in conjunction with the slides for each session. Session 1. 20/01/25. Simple regression Session 2. 27/01/25. Multiple regression 1: multiple continuous Session 3. 03/02/25. ANOVA 1 (10/02/25. No session) Session 4. 17/02/25. Multiple regression 2: one continuous, one categorical Session 5. 24/02/25. Multiple regression 3: hierarchical regression Session 6. 03/03/25. ANOVA 2 Session 7. 10/03/25. Pre-post data and clinically significant change Session 8. 17/03/25. Mixed models It also contains a link to the: the Analysis Assessment (50% of module grade) FAQs Recommended reading Use the left and right arrow keys to navigate this workbook These sessions build upon Ben Whalley's materials from PSYC760: https://benwhalley.github.io/datafluency/ The PSYC761 DLE page 2024-25: https://dle.plymouth.ac.uk/course/view.php?id=77541 "],["simple1.html", "Session 1 Simple Regression 1.1 Overview 1.2 Worked Example 1.3 Predicting 1.4 Residuals 1.5 Evaluating the model 1.6 Exercise 1.7 Further Exercises 1.8 Going further: p-values 1.9 Summary 1.10 References", " Session 1 Simple Regression Chris Berry 2025 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} 1.1 Overview Slides from the lecture part of the session: Download R Studio online Access here using University log-in In simple regression, we create a linear (straight line) model of the relationship between one outcome variable and one predictor variable The outcome variable is what we want to predict or explain (e.g., anxiety scores) The predictor variable is what we use to predict the outcome variable (e.g., average hours of screen time per week) Both the outcome and predictor variable are continuous variables Regression is actually a more general technique that underpins a wide variety of analyses. Simple regression is the most basic form of regression. The simple regression equation has the form Predicted outcome = a + b(Predictor) a is the intercept, it is the height of the line. More formally, it's the value of the outcome when the predictor is equal to zero. b is the slope (or coefficient for the predictor variable). It determines the steepness of the line, or, more formally, the amount of change in the outcome variable for a one unit increase in the predictor variable. The goal of simple regression is to obtain the values of the intercept (a) and slope (b) so that the line 'fits' or 'goes through' our data as closely as possible. The specific method of 'fitting' the line to the data is called the method of least squares (described in the lecture). R can do this all automatically for us (with lm()). 1.2 Worked Example Is screen time linked to mental health? Teychenne and Hinkley (2016) used regression to investigate the association between anxiety and daily hours of screen time (e.g., TV, computer, or device use) in 528 mothers with young children. Read in the data from their study to R and store in mentalh. The data are located at https://raw.githubusercontent.com/chrisjberry/Teaching/master/1_mental_health_data.csv. Type or copy-paste the code to R Studio (e.g., in an Rmd file) to keep a record for your studies. # ensure tidyverse is loaded library(tidyverse) # read the data to R using read_csv() mentalh &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/1_mental_health_data.csv&#39;) There are numerous variables in mentalh (use mentalh %&gt;% glimpse() to take a look). We will focus only on two here: anxiety_score: a score representing the number of anxiety symptoms experienced in the past week, and screen_time: hours per day of screen time use on average. (Note: the data are publicly available with the article, but I've changed some of the variable names for clarity.) First, create a scatterplot of the two variables. Put the predictor variable on the x-axis, and the outcome variable on the y-axis. # The code below takes mentalh and pipes it to ggplot() # aes() is used to tell ggplot() to put # screen_time on the x-axis, and anxiety_score on the y-axis # The settings in geom_smooth() tell ggplot() to # add the regression line (method = &#39;lm&#39;) and # not show confidence intervals (se = FALSE) mentalh %&gt;% ggplot(aes(x = screen_time, y = anxiety_score)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = FALSE) Figure 1.1: Scatterplot of anxiety score according to screen time (hours per day) Exercise 1.1 Describe the relationship between screen time and anxiety evident in the scatterplot (pick one option; green = correct): Individuals with lower levels of screen time tend to have higher anxiety scores No association between screen time and anxiety scores is apparent Individuals with higher levels of screen time tend to have higher anxiety scores Use lm() to run the simple regression and store the results in simple1: # conduct a simple regression to predict anxiety_score from screen_time # store the results in simple1 simple1 &lt;- lm(anxiety_score ~ screen_time, data = mentalh) Explanation: To specify the regression equation, we use outcome_variable ~ predictor_variable.The ~ symbol is a tilde. We use it to specify certain formulas in R. When you see ~, you can read it as \"as a function of\". So, outcome variable ~ predictor variable means \"outcome variable as a function of the predictor variable\". In our case, \"anxiety_score as a function of screen_time\". The intercept (a) and slope (b) are automatically calculated by R and stored in simple1: # look at the results simple1 ## ## Call: ## lm(formula = anxiety_score ~ screen_time, data = mentalh) ## ## Coefficients: ## (Intercept) screen_time ## 5.5923 0.1318 Exercise 1.2 The value of the intercept a is 5.590.13 The value of the slope b for the screen_time predictor is 5.590.13 The regression equation Predicted Outcome = a + b(Predictor) can therefore be written as what? Predicted screen time = 5.59 + 0.13(anxiety score) Predicted anxiety score = 5.59 + 0.13(screen time) Predicted anxiety score = 0.13 + 5.59(screen time) 1.3 Predicting The regression equation can be used for prediction. Suppose someone asked us what the anxiety_score would be for a new person whose screen_time score is 10 hours per week. By reading off from the regression line on the scatterplot from earlier, the anxiety_score looks to be around 7: Figure 1.2: Predicted anxiety score for a person with 10 hours screen time Using the regression equation, we can substitute 10 for screen_time, then calculate predicted anxiety_score more precisely. The augment() function in the broom package can be used to work out the prediction for new data automatically: # load the broom package library(broom) # store new scores as a tibble new_scores &lt;- tibble(screen_time = 10) # give new_scores to &#39;newdata&#39; option in augment() augment(simple1, newdata = new_scores) screen_time .fitted 10 6.91012 The predicted anxiety_score is in the .fitted column and is 6.91 Predictions for multiple individuals can also be made at once. Here we obtain the predictions for two people with screen_time scores of 10 and 15. # store the scores we want predictions for in new_scores new_scores &lt;- tibble(screen_time = c(10, 15)) # use augment() to obtain the predicted anxiety_scores augment(simple1, newdata = new_scores) screen_time .fitted 10 6.910120 15 7.569031 Each row shows one individual. Their predicted anxiety_scores are 6.91 and 7.57. 1.4 Residuals The residual for a given datapoint is its vertical distance from the regression line. It is the error in prediction of the outcome variable for that datapoint. Residual = Observed Score - Predicted Score or \\(Residual = Y - \\hat{Y}\\) where \\(Y\\) is the observed data point, and \\(\\hat{Y}\\) is the predicted data point. To view the residuals, again use the augment() function in the broom package, this time without including newdata. The residual for each observation is given in the column .resid # look at the residuals for simple1 (in .resid) # pipe to head() to only show the first 6 rows augment(simple1) %&gt;% head() anxiety_score screen_time .fitted .resid .hat .sigma .cooksd .std.resid 7 2.571429 5.931167 1.068833 0.0022003 3.514985 0.0001024 0.304677 10 1.428571 5.780559 4.219441 0.0029659 3.510454 0.0021534 1.203238 13 4.214286 6.147666 6.852334 0.0019133 3.502526 0.0036559 1.953016 13 7.285714 6.552426 6.447574 0.0039508 3.503969 0.0067111 1.839532 3 18.571430 8.039682 -5.039682 0.0402420 3.508118 0.0449817 -1.464784 2 1.500000 5.789972 -3.789972 0.0029045 3.511390 0.0017011 -1.080735 Exercise 1.3 What was the residual for a person with anxiety_score equal to 13, and screen_time score equal to 7.29? 6.556.856.45 For this person, was the anxiety score predicted by the model overpredicted (too high)fit exactlyunderpredicted (too low) Explain The person has an anxiety_score of 13 and screen_time score of 7.29. The predicted anxiety_score for this datapoint is 6.55 (in the .fitted column), so the model underpredicts the observed value of anxiety_score. An assumption underlying regression is that the residuals are like random noise. More specifically, the residuals are assumed to be normally distributed with a mean of zero, and not correlated with one another. When we plot the residual against the predicted values, there should also be no trend evident in the datapoints in the plot. We can use this plot for checking this assumption of regression. # Create a plot of the predicted values vs. residuals # Use the &quot;.fitted&quot; and &quot;.resid&quot; columns in augment() # Use geom_hline() to draw a black horizontal line at y = 0 # Use geom_smooth() to fit a general trend line augment(simple1) %&gt;% ggplot(aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(color=&quot;blue&quot;, se=F) Figure 1.3: Predicted anxiety score vs. the residual Explanation: If there's no trend in the residuals, we'd expect the points to look like a random cloud above and below the horizontal line (at y = 0). There should be no patterns, and the points should be pretty symmetrically distributed around a single point in the middle of the plot. There's some slight indication that the residuals tend to have lower values as the predicted values (.fitted) increase. In other words, there's some tendency for the model to overestimate anxiety_score as screen_time becomes more extreme. The residuals also seem more spread out above the horizontal at lower predicted values, but this doesn't look too serious and the plot seems okay. The blue line is the trend line drawn by RStudio, which also shows no systematic trend. Issues here can indicate that improvement in the model is possible. Check the assumption that the residuals are normally distributed by obtaining a histogram: # Create a histogram of the residuals using # ggplot(aes()) # and geom_histogram() augment(simple1) %&gt;% ggplot(aes(.resid)) + geom_histogram() Figure 1.4: Histogram of the residuals Explanation: Inspection of the histogram of residuals reveals that the distribution is approximately normal, satisfying this assumption. 1.5 Evaluating the model 1.5.1 R2 R2 is a statistic that describes how well our model explains the outcome variable. It ranges between 0 and 1 and can be interpreted as the proportion of variance in the outcome variable that is explained by the predictor variable. To obtain R2 for the model: # use glance() to obtain R-squared glance(simple1) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.0148346 0.0129617 3.511952 7.920493 0.0050709 1 -1411.456 2828.913 2841.72 6487.581 526 528 The column r.squared contains R2 for the model, and is equal to 0.0148. To report as a percentage, multiply by 100. This means that screen_time explains 1.48% of the variance in anxiety_score. In psychological research, this is a relatively small amount of variance to explain with a model. It may still be meaningful in some contexts though (e.g., where it may be better to have a model with some predictive power rather than none at all, or if a theory predicts a presence vs. absence of a relation). In simple regression, R2 is actually the squared value of the Pearson correlation (r) between the outcome and predictor variable: # load corrr package library(corrr) # obtain the Pearson correlation r between screen_time and anxiety_score mentalh %&gt;% select(screen_time, anxiety_score) %&gt;% correlate(method = &quot;pearson&quot;) term screen_time anxiety_score screen_time NA 0.1217973 anxiety_score 0.1217973 NA The correlation between screen_time and anxiety_score is r = 0.1217973. 0.1217973 * 0.1217973 = 0.0148, which is equal to the R2 value obtained with glance() 1.5.2 Bayes factor To further evaluate the model, we can obtain a Bayes factor (Rouder &amp; Morey, 2012). In simple regression, the Bayes factor tells us how much more likely the model is than one comprising the mean of the outcome variable only. We call this baseline model the intercept-only model. It is a model in which the regression line is a flat line (i.e., has a slope equal to zero), and the predictor does not predict the outcome at all. To obtain the Bayes factor, use lmBF() in the BayesFactor package: # load BayesFactor package library(BayesFactor) # Compute the Bayes factor lmBF(anxiety_score ~ screen_time, data = data.frame(mentalh)) ## Bayes factor analysis ## -------------- ## [1] screen_time : 4.465124 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS The Bayes Factor is 4.47. We'd report this as BF10 = 4.47. This BF means that a model consisting of screen_time alone as a predictor of anxiety_score is over four times more likely than an intercept-only model (in which screen_time has a zero-slope and so does not predict anxiety_score). In other words, there's sufficient evidence to say that screen_time predicts anxiety_score. Reporting the simple regression in a report: A simple regression was conducted to model the number of anxiety symptoms reported in the past week (anxiety score) from average hours of screen time usage per day (screen time). Screen time was found to have a positive association with anxiety scores, whereby individuals who reported greater levels of screen time also tended to have greater anxiety scores. The regression equation was \"Predicted anxiety score = 5.59 + 0.13(screen time)\", indicating that every hour of screen time use was associated with an increase in 0.13 in the anxiety score. Screen time explained only a small proportion of the variance in anxiety score, adjusted R2 value = 1.30%. The Bayes factor, comparing the model against an intercept-only model, was BF 10 = 4.47, indicating moderate evidence for the model, with it being over four times more likely than an intercept-only model. 1.6 Exercise Exercise 1.4 Is screen time predicted by age? In addition to screen time, Teychenne and Hinkley (2016) also asked participants their age in years, recorded in age in the mentalh dataset. Let's explore whether age predicts screen_time using simple regression. Adapt the code in this worksheet to do the following: Try to do each one on your own first, before looking at the hint (or the solution). 1. Produce a scatterplot of age vs. screen_time Hint Pipe mentalh to ggplot() and use geom_point() and geom_smooth(). Put the new predictor variable (age) on the x-axis and the outcome variable (screen_time) on the y-axis. Solution mentalh %&gt;% ggplot(aes(x = age, y = screen_time)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = FALSE) + xlab(&quot;Age&quot;) + ylab(&quot;Screen time (hours)&quot;) Describe the relationship between age and screen time in the scatterplot (pick one): Older individuals tend to have higher screen time scores Older individuals tend to have lower screen time scores No association between age and screen time appears to be present 2. Conduct a simple regression, with screen_time as the outcome variable, and age as the predictor variable Hint Use lm() to specify the simple regression Solution simple2 &lt;- lm(screen_time ~ age, data = mentalh) simple2 What is the value of the intercept a (to two decimal places)? What is the value of the slope b (to two decimal places)? What is the regression equation? Predicted screen time = 7.48 - 0.10(age) Predicted screen time = 0.10 - 7.48(age) Predicted screen time = 7.48 + 0.10(age) 3. Obtain R-squared Hint Make sure you have stored the regression results (e.g., in simple2), then use glance() with those results Solution glance(simple2) What proportion of variance in the screen time is explained by age? (Report the adjusted R-squared value, to two decimal places) Report the value of adjusted R-squared as a percentage, to two decimal places: The adjusted R2 value is equal to % 4. Obtain the Bayes factor for the model Hint Use lmBF() to specify the model Solution simple2_BF &lt;- lmBF(screen_time ~ age, data = data.frame(mentalh)) simple2_BF How many times more likely is the model with age as a predictor of screen_time, compared to an intercept-only model? (to two decimal places) 5. Produce a plot of the fitted (predicted) values against the residuals Hint Use augment() with ggplot() and geom_point() Solution augment(simple2) %&gt;% ggplot(aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0) + geom_smooth() What type of trend is evident between the predicted values and the residuals? Further interpretation No association is apparent, but the points above the line appear to be more spread out than the points below the horizontal line. This indicates that the model tends to underestimate some of the screen time scores. This could be because the screen time scores are positively skewed, e.g., see mentalh %&gt;% ggplot(aes(screen_time)) + geom_density(), and therefore taking the log transform of the scores prior to analysis may improve this plot (though may not necessarily change the outcome of the analysis). 6. On balance, does age seem to be a good predictor of a person's daily screen time use? No Yes Cannot determine Explanation Yes, the older the individuals were, the lower the screen time score tended to be. A model with age as a predictor of screen time explained only 1.68% of the variance in screen time scores (adjusted R2), but the Bayes factor (BF10 = 12.19) indicated strong evidence for this model compared to an intercept-only model. The regression equation was \"Predicted screen time = 7.48 - 0.10(age)\", indicating that an increase in age of one year was associated with a reduction of approximately 6 minutes (i.e., one tenth of 1 hour) of screen time per week. 1.7 Further Exercises For those feeling confident with everything so far. Exercise 1.5 Further Exercise The variable physical_activity in the mentalh dataset is a measure of moderate-to-vigorous physical activity, based on participant's self reported weekly activity. To what extent is participants' anxiety_score explained by their physical_activity? Investigate by producing the following: Scatterplot Correlation Simple regression model Adjusted R-squared value Bayes factor On balance, does the anxiety_score seem to be predicted by physical_activity? Solution: code # scatterplot mentalh %&gt;% ggplot(aes(x = physical_activity, y = anxiety_score)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = F) + xlab(&quot;Physical activity&quot;) + ylab(&quot;Anxiety score&quot;) + theme_classic() # correlation mentalh %&gt;% select(anxiety_score, physical_activity) %&gt;% correlate() # simple regression model model_activity &lt;- lm(anxiety_score ~ physical_activity, data = mentalh) # look at intercept and slope model_activity # look at plot of fitted values and residuals augment(model_activity) %&gt;% ggplot(aes(x=.fitted, y=.resid)) + geom_point() + geom_hline(yintercept = 0) # look at R-squared glance(model_activity) # calculate Bayes Factor lmBF(anxiety_score ~ physical_activity, data = mentalh) Solution: interpretation No, there's no evidence that the anxiety scores are predicted by self reported measures of moderate-to-vigorous levels of physical activity. The two measures showed virtually no correlation, r = -0.01. The regression equation was Predicted Anxiety Score = 6.23 - 0.0001(physical activity), and the model explained no variance in anxiety score with the adjusted R2 = -0.0017. The Bayes factor was equal to 0.10. Given that this value of the Bayes factor is less than 0.33, this indicates substantial evidence for the intercept-only model, compared to the simple regression model where physical activity is the sole predictor of anxiety scores. In other words, if we only had these two variables, the best predictor of anxiety scores would be the mean value of the anxiety scores. Interestingly, although there appears to be no relationship between anxiety and physical activity in this sample of individuals (mothers), other populations do apparently show reductions in anxiety with greater levels of vigorous physical activity (e.g., in adolescents, see Hrafnkelsdottir et al., 2018). 1.8 Going further: p-values An additional resource on using p-values in regression if you are curious (e.g., for your projects): p-values In keeping with our undergraduate curriculum, Bayes factors have been used as the main method of statistical inference here. Frequentist methods of statistical inference, which rely on p-values, are still widely used in the psychological research literature, however. To obtain the p-values for a simple regression, use summary(model_name). For the first simple regression in the worksheet: # obtain the p-values for the simple regression summary(simple1) ## ## Call: ## lm(formula = anxiety_score ~ screen_time, data = mentalh) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.5103 -2.7076 -0.1194 2.0782 13.0500 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.59230 0.23757 23.540 &lt; 2e-16 *** ## screen_time 0.13178 0.04683 2.814 0.00507 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.512 on 526 degrees of freedom ## Multiple R-squared: 0.01483, Adjusted R-squared: 0.01296 ## F-statistic: 7.92 on 1 and 526 DF, p-value: 0.005071 Explanation of the output: Residuals: provides an indication of the discrepancy between the values of anxiety_score predicted by the model (i.e., the regression equation) and the actual values of anxiety_score. Roughly speaking, if the model does a good job in predicting anxiety_score, the residuals should be relatively small. The difference between Min and Max gives us some idea of the range of error in the prediction of anxiety_Scores scores. The difference in 3Q and 1Q is the interquartile range. The median of the residuals is -0.12. Coefficients: contains tests of statistical significance for each of the coefficients. The values in the column headed Pr(&gt;|t|) are the p-values associated with the t-values for the coefficients for each predictor. The t-values test a null hypothesis that the coefficients are equal to zero. A p-value less than .05 indicates that a predictor is statistically significant. More specifically, it is the probability of obtaining a t-statistic at least as extreme as the one observed, if the null hypothesis is true. The row for the (intercept) reports a t-test for whether the value of the intercept differs from zero. We're not usually interested in this test (so wouldn't report it). The row for screen_time tests whether the value of its coefficient (0.13) differs from zero. A coefficient of zero would be expected if the predictor explained no variance in the outcome variable. The coefficient for entrex (0.13) is greater than zero in this case. We can report this by saying that screen_time is a statistically significant predictor of anxiety_score, b = 0.13, t(526) = 2.81, p &lt; .01. Multiple R-squared: This is \\(R^2\\), which, as before, is the proportion of variance in anxiety_score explained by screen_time. Here, \\(R^2\\) = 0.0148, or 1.48%. Adjusted R-squared: Again, this is an estimate of \\(R^2\\), but adjusted for the population. Despite the usefulness of this statistic, most studies still tend to report only the (unadjusted) \\(R^2\\) value. If reporting the Adjusted R-squared value, be sure to label it clearly as such. Here, Adjusted R-squared = 0.013, or 1.30%. F-statistic: This compares the variance in anxiety_score explained by the model with the variance that it does not explain (i.e., explained variance divided by unexplained variance). Higher values of F indicate that the model explains greater variance in an outcome variable. If the p-value associated with the F-statistic is less than .05, we can say that the model significantly predicts the outcome variable. Hence, we can say that a model consisting of screen_time alone is a significant predictor of anxiety_score, F(1, 526) = 7.92, p &lt; .01. Higher screen_time scores tend to be associated with higher anxiety_scores scores. If our model did not explain any variance in anxiety_score, we wouldn't expect this to be statistically significant. In simple regression, the null hypothesis being tested on the F-statistic is that the slope of the regression line in the population is equal to zero. You'll notice that this is actually equivalent to the t-test on the screen_time coefficient. So in simple regression, report the F-statistic for the overall regression or the t-test on the coefficient (not both). This equivalence between F and t does not hold true for multiple regression, as we shall see later. The results of the frequentist and Bayesian analyses can be reported together in an article, e.g., \"Hours of screen time significantly predicted anxiety score, b = 0.13, t(526) = 2.81, p &lt; .01, BF10 = 4.47.\" 1.9 Summary Simple regression can be used to model the relationship between an outcome and predictor variable, where both variables are continuous. Once obtained, the regression equation allows us to: precisely describe the relationship between the outcome and predictor variables (whether positive or negative). derive predictions for the outcome variable, given new values of the predictor variable. evaluate the model with R2 and use a Bayes factor to compare how much more likely it is to an intercept-only model. Key functions Visualise the data: ggplot() Simple regression: lm() R2: glance() Residuals: augment() Bayes Factor: lmBF() In the next session we will explore regression models with more than one continuous predictor variable. 1.10 References Hrafnkelsdottir S.M., Brychta R.J., Rognvaldsdottir V., Gestsdottir S., Chen K.Y., Johannsson E., et al. (2018) Less screen time and more frequent vigorous physical activity is associated with lower risk of reporting negative mental health symptoms among Icelandic adolescents. PLoS ONE 13(4): e0196286. https://doi.org/10.1371/journal.pone.0196286 Rouder, J. N., &amp; Morey, R. D. (2012). Default Bayes factors for model selection in regression. Multivariate Behavioral Research, 47(6), 877-903. https://doi.org/10.1080/00273171.2012.734737 Teychenne M, &amp; Hinkley T (2016) Associations between screen-based sedentary behaviour and anxiety symptoms in mothers with young children. PLoS ONE, 11(5): e0155696. https://doi.org/10.1371/journal.pone.0155696 "],["multiple1.html", "Session 2 Multiple regression: multiple continuous predictors 2.1 Overview 2.2 Worked example 2.3 Understanding the contribution of individual predictors 2.4 Multicollinearity 2.5 Exercise 2.6 Further exercises 2.7 Going further: p-values 2.8 Summary of key points 2.9 References", " Session 2 Multiple regression: multiple continuous predictors Chris Berry 2025 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} 2.1 Overview Slides from the lecture part of the session: Download R Studio online Access here using University log-in This worksheet assumes you have gone through the previous one on simple regression. When we want to determine the extent to which an outcome variable (e.g., psychological wellbeing) is predicted by multiple continuous predictors (e.g., both worry and mindfulness scores), we can use multiple regression. Having multiple predictors to a model may serve to improve the prediction of the outcome variable. It can also be a way to test specific theories or hypotheses. Simple vs. Multiple Regression Simple regression is a linear model of the relationship between one outcome variable and one predictor variable. For example, can we predict wellbeing on the basis of worry scores? Multiple regression is a linear model of the relationship between one outcome variable and more than one predictor variable. For example, can we predict wellbeing based on worry, mindfulness, and emotional intelligence scores? 2.2 Worked example Iani et al. (2019) looked at factors associated with psychological wellbeing and distress in 66 individuals with generalised anxiety disorder. For educational purposes, we'll focus on a subset of their data, namely whether wellbeing is predicted by worry and describing scores in a multiple regression. Describing is the mindfulness skill of being able to describe one's inner experiences and feelings with words. Read the data to R. The data are stored at: https://raw.githubusercontent.com/chrisjberry/Teaching/master/2_wellbeing_data.csv # First ensure tidyverse is loaded, i.e., &#39;library(tidyverse)&#39; # read in the data using read_csv(), store in wellbeing_data wellbeing_data &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/2_wellbeing_data.csv&#39;) # preview the data with glimpse() wellbeing_data %&gt;% glimpse() We'll use these three variables in the dataset: wellbeing: Higher scores indicate higher wellbeing. worry: Higher scores indicate higher levels of worry. describing: Higher scores indicate higher self-reported ability to describe one's inner experiences. (Note. The data are publicly available, but I've changed the variable names for clarity. As in Iani et al. (2019), missing values were replaced with the mean of the relevant variable.) Visualise the data with a scatterplot. Place the outcome variable wellbeing on the y-axis, the predictor worry on the x-axis, and let the size of each point represent the second predictor, the describing score: # Scatterplot of all three variables # Use alpha to alter transparency of points # to make overlap easier to see wellbeing_data %&gt;% ggplot(aes(x = worry, y = wellbeing, size = describing)) + geom_point(alpha = 0.6, colour = &quot;cornflowerblue&quot;) Figure 2.1: Scatterplot of wellbeing scores vs. worry and describing Exercise 2.1 From inspection of the scatterplot: Greater worry scores tend to be associated with lowerconstanthigher wellbeing scores. Explain A negative trend between worry and wellbeing is evident in the scatterplot Greater describing scores tend to be associated with lowerconstanthigher wellbeing scores. Explain The size of the describing points tend to be larger when wellbeing scores are higher. The above trends are also apparent in the Pearson correlations between variables: # load the corrr package first library(corrr) # select the relevant columns from wellbeing_data and # use correlate() to obtain the correlations wellbeing_data %&gt;% select(wellbeing, worry, describing) %&gt;% correlate(method = &quot;pearson&quot;) term wellbeing worry describing wellbeing NA -0.5419352 0.5356548 worry -0.5419352 NA -0.2477970 describing 0.5356548 -0.2477970 NA The correlation between wellbeing and worry (to 2 decimal places) is r = The correlation between wellbeing and describing (to 2 decimal places) is r = The correlation between the two predictors (worry and describing) (to 2 decimal places) is r = Alternative plot The describing scores could alternatively be represented by the colour density of each point instead: wellbeing_data %&gt;% ggplot(aes(x = worry, y = wellbeing, colour = describing)) + geom_point(size = 5, alpha = 0.9) Multiple regression using lm() To include more than one predictor in a regression model, use the + symbol when specifying the model with lm(): lm(outcome ~ predictor_1 + predictor_2 + predictor_3.... , data = mydata) This specifies a model of the form: \\(Predicted \\ outcome = a + b_1(Predictor \\ 1) + b_2(Predictor \\ 2) + b_3(Predictor \\ 3) ...\\) Note that we don't need to specify the intercept a in lm() since it is included automatically by R (as is the case with simple regression). # conduct a multiple regression, store it in multiple1 multiple1 &lt;- lm(wellbeing ~ worry + describing, data = wellbeing_data) # look at the coefficients multiple1 ## ## Call: ## lm(formula = wellbeing ~ worry + describing, data = wellbeing_data) ## ## Coefficients: ## (Intercept) worry describing ## 70.7306 -0.7708 1.2484 (Intercept) is the value of the intercept a in the regression equation. Type to two decimal places: worry is the value of the coefficient \\(b_1\\) for the worry predictor. describing is the value of the coefficient \\(b_2\\) for the describing predictor. The regression equation is therefore written as: \\(Predicted\\ wellbeing = 70.73 - 0.77(worry) + 1.25(describing)\\) 2.2.1 Residual plot We can obtain a plot of the predicted values vs. the residuals in the same way as for simple regression by using augment() in the broom package. # ensure the broom package is loaded (it contains augment()) library(broom) augment(multiple1) %&gt;% ggplot(aes(x = .fitted, y = .resid)) + geom_hline(yintercept = 0) + geom_point() Figure 2.2: Scatterplot of the fitted (predicted) values vs. residuals The points seem randomly and evenly distributed around the horizontal, in line with assumptions of homoscedasticity (equal variance of residuals at each predicted value), and independence of residuals. To add a (linear) trend line to the above, use + geom_smooth(method = \"lm\"). The trend line is a flat line! 2.2.2 Evaluating the model: Bayes factor The Bayes factor for a multiple regression model tells us how many more times likely the model is, relative to the intercept-only model. Use lmBF() to obtain the Bayes factor for the multiple regression model: # ensure the BayesFactor package is loaded library(BayesFactor) # store the BF for the model in multiple1_BF multiple1_BF &lt;- lmBF(wellbeing ~ worry + describing, data = data.frame(wellbeing_data)) # show the BF multiple1_BF ## Bayes factor analysis ## -------------- ## [1] worry + describing : 4190994 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS The Bayes factor for the model is . This tells us that the model with worry and describing is over four million times more likely than an Intercept only model (one with no predictors). Thus, there's extremely strong evidence that the multiple regression model explains wellbeing. 2.2.3 Evaluating the model: R2 As with simple regression, R2 is the proportion of variance in the outcome variable that is explained by the multiple regression model. Use glance() in the broom package to obtain the R2 for the model: glance(multiple1) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.4653263 0.4483526 9.393314 27.41444 0 2 -239.9547 487.9093 496.6679 5558.763 63 66 r.squared is R2, the proportion of variance in wellbeing explained by the model. Thus, the model explains 0.4653, or 46.53% of the variance in wellbeing. adj.r.squared is the Adjusted R2 value, which is R2 adjusted for the sample size and the number of predictors in the model. It is an estimate of R2 for the population (not merely the scores we have in the sample), and is always less than R2. You'll see researchers reporting either R2 or the adjusted R2 in the literature. If you're not sure which one to use, report the adjusted R2, and say so (e.g., \"adjusted R2 = 44.83%\"). The adjusted R2 value is 0.4483, so in a report we could say that a model with worry and describing explains 44.83% of the variance in wellbeing. 2.3 Understanding the contribution of individual predictors Because predictor variables are often correlated to a degree, some of the variance they explain in the outcome will be shared. A predictor's contribution to a model must therefore only be interpreted after the other predictors in the model have been taken into account. This is explored in more detail below. 2.3.1 R2 in simple vs. multiple regression In a simple regression of wellbeing ~ worry, the variance in wellbeing explained by worry is R2 = 29.37%: s1 &lt;- lm(wellbeing ~ worry, data = wellbeing_data) glance(s1) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.2936938 0.2826578 10.71152 26.61226 2.6e-06 1 -249.1416 504.2832 510.8522 7343.15 64 66 In a simple regression of wellbeing ~ describing, the variance in wellbeing explained by describing is R2 = 28.69% s2 &lt;- lm(wellbeing ~ describing, data = wellbeing_data) glance(s2) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.286926 0.2757842 10.76272 25.75226 3.6e-06 1 -249.4563 504.9126 511.4816 7413.512 64 66 So, by looking at the (non-adjusted) R2 values of our simple and multiple regressions so far, an interesting pattern is evident: In the simple regression of wellbeing ~ worry, R2 = 29.37% In the simple regression of wellbeing ~ describing, R2 = 28.69% Yet, in a multiple regression of wellbeing ~ worry + describing, R2 = 46.56%, which is less than the sum of R2 from the simple regressions (29.37 + 28.69 = 58.06%). The reason why the R2 values from the simple regressions don't add up to the same value as the R2 for the multiple regression is because worry and describing are correlated (r = -0.25, as you obtained earlier). This means that some of the variance that they explain in wellbeing is shared. 2.3.2 Venn Diagrams Venn diagrams are useful for understanding the variance that predictors explain in the outcome variable. They are especially useful for understanding R2 in multiple regression. Here, I use them to explain why the R2 values from simple regressions don't necessarily add up to R2 when the same predictors are in a multiple regression. Suppose the rectangle below represents all of the variance in wellbeing to be explained. The area of the circle below represents the variance in wellbeing explained by worry in the first simple regression. If this diagram were drawn to scale (it's not!), the area of the circle would be equal to the value of \\(R^2\\) (i.e., 29.37%). The part of the rectangle not inside the circle represents the variance in wellbeing that is not explained by the model (i.e., the unexplained or residual variance). We'll now add describing to the model with worry. We could represent this on a Venn diagram as follows: The correlation is represented as an overlap in the circles. Their total area (R2 for the multiple regression model = 46.56%) is less than the area they'd explain if there were no overlap (58.06%). This highlights a crucial point: Predictors are often correlated to some degree, so in multiple regression it only really makes sense to talk about the contribution a predictor makes in the context of the other predictors in the model. That is, a given predictor explains variance in the outcome variable only after the other predictors in the model have been taken into account. Given that the unique contributions of worry and describing are lower in a multiple regression model, we must ask whether there's evidence that each predictor makes a unique contribution, over and above the other predictor. For example, is there sufficient evidence for a unique contribution of worry, once describing has been taken into account? If not, we probably wouldn't include it in the model. 2.3.3 Using Bayes factors to assess the unique contribution of predictors We can compare Bayes factors of models to determine whether a given predictor in a multiple regression model makes a unique contribution to the prediction of the outcome variable. First, obtain the Bayes factor for the model in which predictor_1 has been left out of the full model. Then obtain the Bayes factor for model in which predictor_2 has been left out of the model. Repeat this for as many predictors you have in the model. In our example, this involves obtaining the BFs of the model of wellbeing ~ describing, and the BF of the model of wellbeing ~ worry: # BF for wellbeing ~ describing describing_BF &lt;- lmBF(wellbeing ~ describing, data = data.frame(wellbeing_data)) # BF for wellbeing ~ worry worry_BF &lt;- lmBF(wellbeing ~ worry, data = data.frame(wellbeing_data)) We also need the BF for the full model: # BF for wellbeing ~ worry + describing # (this is the same as multiple1_BF above, but # with a different name, which will help us see what&#39;s going on later) worry_describing_BF &lt;- lmBF(wellbeing ~ worry + describing, data = data.frame(wellbeing_data)) Comparing models We can use the following general formula to determine whether there's evidence for a more complex version of a model, relative to a simpler model: BF_more_complex_model / BF_simpler_model That is, we take the BF for the more complex model and divide it by the BF for the simpler one. This then tells us how many more times more likely the more complex model is, relative to the simpler one. For example, if BF_more_complex_model = 10 and BF_simpler_model = 2, then the more complex model is five times more likely than the simpler one (because 10 / 2 = 5). There'd be substantial evidence to prefer the more complex model. In our case, we can compare the Bayes factor of the full model (worry_describing_BF) with that of our simpler models (describing_BF) and (worry_BF) in order to determine whether each predictor makes a unique contribution to the full model or not. To determine if there's evidence for the unique contribution of worry to the model: # compare BFs for the full model and one in which worry is left out worry_describing_BF / describing_BF ## Bayes factor analysis ## -------------- ## [1] worry + describing : 981.4912 ±0% ## ## Against denominator: ## wellbeing ~ describing ## --- ## Bayes factor type: BFlinearModel, JZS The BF for the comparison is 981.49, indicating that there's substantial evidence that the addition of worry to a model containing describing improves the model. In other words, there's evidence for a unique contribution of worry to the prediction of wellbeing. To determine if there's evidence for the unique contribution of describing to the model: # compare the BFs for the full model and one in which describing is left out worry_describing_BF / worry_BF ## Bayes factor analysis ## -------------- ## [1] worry + describing : 738.4095 ±0% ## ## Against denominator: ## wellbeing ~ worry ## --- ## Bayes factor type: BFlinearModel, JZS The BF comparing the full model with one containing worry alone is 738.41. This indicates that the model with both worry and describing is over seven hundred times more likely than the model with worry alone. The BF is greater than 3, so this indicates substantial evidence for the unique contribution of describing to the prediction of wellbeing. Thus, in a multiple regression model, there's substantial evidence that both worry and describing predict wellbeing overall (BF10 = 4,190,994). There's also substantial evidence that worry (BF10 = 981.49) and describing (BF10 = 738.41) make unique contributions to this model. The model with both predictors included therefore seems justified. 2.4 Multicollinearity If the correlation between predictors is very extreme (greater than r = 0.8 or less than -0.8), this is known as multicollinearity. Multicollinearity can be a problem in multiple regression. Predictors may explain a large amount of variance in the outcome variable, but their 'unique' contribution in a multiple regression may be small. In an extreme scenario, neither predictor makes a unique contribution, even though the overall regression explains a large amount of variance in the outcome variable! On a Venn diagram, the circles representing the predictors would almost completely overlap: Multicollinearity should be avoided. Therefore, check for extreme correlations between your predictor variables before including them in a multiple regression. The correlation between the predictor variables in the model of wellbeing ~ worry + describing was , and multicollinearity was therefore of concernof no concernnot possible to determine. How many predictor variables should be included in a model? If adding predictors to the regression improves the prediction of the outcome variable, you may think that we can simply keep adding in variables to the model, until all the residual variance has been explained. This seems fine until we learn that if we were to add as many predictors to the model as there are rows in our data (e.g., 66 participants inwellbeing_data), then we'd perfectly predict the outcome variable, and have a (non-adjusted) \\(R^2\\) of 100%! This would be true even if the predictors consisted of random values. Our model would clearly be meaningless though. We ideally want to explain the outcome variable with relatively few predictors, or for our selection of predictors to be guided by specific hypotheses or theory. 2.5 Exercise Exercise 2.2 Main Exercise The wellbeing_data also contain these columns: gad : Severity of symptoms of GAD (Generalised Anxiety Disorder). Higher scores indicate greater severity of symptoms. brooding : Higher scores indicate higher levels of brooding (i.e., being preoccupied with depressing, morbid, or painful memories or thoughts). observing : Higher scores indicate greater tendency to notice things in one's environment (e.g., smells, sounds, visual elements). It's another measure of mindfulness. Use multiple regression to investigate the extent to which brooding and observing predict gad. Adapt the code in this worksheet to do the following things (and try not to look at the solution until you've attempted the question): 1. Visualise the relationship between gad, brooding and observing in a scatterplot. Hint Pipe wellbeing_data to ggplot() and use geom_point(). Use the size option to specify the second predictor variable in aes() Solution wellbeing_data %&gt;% ggplot(aes(x = brooding, y = gad, size = observing)) + geom_point() + xlab(&quot;Brooding&quot;) + ylab(&quot;GAD severity of symptoms score&quot;) + theme_classic() Figure 2.3: GAD vs. brooding and observing scores In the scatterplot, observing could have also been swapped around with brooding. Try both ways. Greater brooding scores tend to be associated with lowerno appreciable change inhigher gad scores. Greater observing scores tend to be associated with lowerno appreciable change inhigher gad scores. 2. Obtain the correlations between gad, brooding, and observing Hint Pipe the wellbeing_data to select() and use correlate() Solution wellbeing_data %&gt;% select(gad, brooding, observing) %&gt;% correlate(method = &quot;pearson&quot;) State the correlations to two decimal places: The correlation between the GAD score and brooding is r = The correlation between the GAD score and observing is r = The correlation between brooding and observing is r = Is multicollinearity a concern between the two predictor variables? yesnocannot determine Explanation The correlation between the predictor variables is r = -.18. Although they are weakly correlated, this does not exceed r = -.80, and therefore multicolinearity is not a concern. 3. Conduct a multiple regression, with gad as the outcome variable, and brooding and observing as the predictor variables Hint Use lm() to specify the simple regression. Store it in multiple2. Solution multiple2 &lt;- lm(gad ~ brooding + observing, data = wellbeing_data) multiple2 What is the value of the intercept a (to two decimal places)? What is the value of the coefficient for brooding (to two decimal places)? What is the value of the coefficient for observing (to two decimal places)? What is the regression equation? Predicted GAD score = 0.07 - 0.89(brooding) + 0.02(observing) Predicted GAD score = 0.07 + 0.02(brooding) - 0.89(observing) Predicted GAD score = 0.07 + 0.89(brooding) - 0.02(observing) 4. Obtain R2 for the model Hint Make sure you have stored the regression results (e.g., in mutliple2), then use glance() with that variable. Solution glance(multiple2) What proportion of variance in the GAD score is explained by the model containing brooding and observing? (Report the adjusted R-squared value as a proportion, to two decimal places) Report the value of adjusted R-squared as a percentage, to two decimal places. Hint: multiply the proportion given in the output by 100 to obtain the percentage value. The adjusted R2 value is equal to % 5. Obtain the Bayes factor for the model Hint Use lmBF() to specify the multiple regression model Solution multiple2_BF &lt;- lmBF(gad ~ brooding + observing, data = data.frame(wellbeing_data)) How many times more likely is the model with brooding and observing as a predictor of gad, compared to an intercept-only model? (to two decimal places) 6. Plot the predicted values against the residuals. Hint Use augment() with ggplot() and geom_point() Solution augment(multiple2) %&gt;% ggplot(aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0) What type of trend is evident between the predicted values and the residuals positive trendthere's no associationnegative association Further interpretation Although there's a slight negative association for lower predicted values, the assumptions of homoscedasticity and independence of residuals appear to be met. You could also try adding the code + geom_smooth(method = \"lm\", se = FALSE) to your plot to see what the (linear) trend line would actually be (a flat line, confirming there's no association). 7. Using lmBF(), determine whether there's evidence for a unique contribution of brooding and observing to prediction of gad Report your answer to two decimal places. The BF associated with the unique contribution of observing is The BF associated with the unique contribution of brooding is Hint 1 Use lmBF() to obtain the BF for both simple regressions and the multiple regression. Then use the general formula BF_more_complex_model / BF_simpler_model to obtain the BF for the unique contribution of each predictor. Hint 2 To get the BF for the two simple regressions: BF_brooding &lt;- lmBF(gad ~ brooding, data = data.frame(wellbeing_data)) BF_observing &lt;- lmBF(gad ~ observing, data = data.frame(wellbeing_data)) The multiple regression BF (same as multiple2_BF): BF_brooding_observing &lt;- lmBF(gad ~ brooding + observing, data = data.frame(wellbeing_data)) Then the BFs for relevant models need to be compared, using: BF_more_complex_model / BF_simpler_model Solution # BF for gad ~ brooding BF_brooding &lt;- lmBF(gad ~ brooding, data = data.frame(wellbeing_data)) # BF for gad ~ observing BF_observing &lt;- lmBF(gad ~ observing, data = data.frame(wellbeing_data)) # BF for full model BF_brooding_observing &lt;- lmBF(gad ~ brooding + observing, data = data.frame(wellbeing_data)) # BF for the unique contribution of observing BF_brooding_observing / BF_brooding # BF for the unique contribution of brooding BF_brooding_observing / BF_observing 8. On balance, is there evidence for a model containing both brooding and observing as predictors of GAD scores? Hint Look at BF for the unique contributions. If the BF is greater than 3, this indicates substantial evidence for the inclusion of that predictor in the model. If the BF is less than 0.33, then there's substantial evidence against its inclusion (the simpler model is preferable). Intermediate BFs are inconclusive, and the predictor should therefore be left out. Solution Together, brooding and observing explain 34.50% of the variance in GAD scores (adjusted R2), and there was strong evidence for this model, compared to an intercept only model, BF10 = 26889.69. There was, however, no evidence for a unique contribution of observing to the model (BF10 = 0.19), but there was substantial evidence for a unique contribution of brooding to the model, BF10 = 741841.28, whereby greater GAD scores were associated with higher brooding scores. In the interest of parsimony (not making models more complex than they need to be), the results suggest that only brooding predicts GAD and observing should not be included in the model that also contains brooding. 2.6 Further exercises Not essential. For those confident with everything. 2.6.1 regressionBF() When determining the BF for the unique contribution of a predictor, as a shortcut, we could have used regressionBF(), rather than using lmBF() multiple times. regressionBF() automatically calculates the BFs for all permutations of a set of predictors in a model. For our first model of wellbeing on the basis of worry and describing: # obtain BFs for all permutations of the model predictors all_BFs &lt;- regressionBF(wellbeing ~ worry + describing, data = data.frame(wellbeing_data)) # look at the BFs all_BFs ## Bayes factor analysis ## -------------- ## [1] worry : 5675.703 ±0% ## [2] describing : 4270.027 ±0% ## [3] worry + describing : 4190994 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS \"[1]\" is the BF for the model with worry as the sole predictor of wellbeing \"[2]\" is the BF for the model with describing as the sole predictor of wellbeing \"[3]\" is the BF for the model with worry and describing as predictors Comparing the BF for [3] and [2] will tell us whether there's evidence that worry makes a unique contribution. # compare multiple regression with simple regression 2 all_BFs[3] / all_BFs[2] ## Bayes factor analysis ## -------------- ## [1] worry + describing : 981.4912 ±0% ## ## Against denominator: ## wellbeing ~ describing ## --- ## Bayes factor type: BFlinearModel, JZS The BF for this comparison is 981.49, which matches the BF for the unique contribution of describing calculated earlier. Comparing the BF for [3] and [1] will therefore tell us whether there's evidence that describing makes a unique contribution to the model: # compare the BF for [3] and [1] all_BFs[3] / all_BFs[1] ## Bayes factor analysis ## -------------- ## [1] worry + describing : 738.4095 ±0% ## ## Against denominator: ## wellbeing ~ worry ## --- ## Bayes factor type: BFlinearModel, JZS The BF for this comparison is 738.41, which matches the BF for the unique contribution of describing calculated earlier. Thus, the BFs derived by this method are the same as when we used lmBF() in the main section of the worksheet. Exercise 2.3 Further exercise 1 Use regressionBF() to obtain the BFs for the unique contribution of brooding and observation in the model in the Main Exercise (i.e., gad ~ brooding + observation), and check that they are the same as the ones produced when you used lmBF() 2.6.2 Predicting new data As with simple regression, we can use augment() to predict what the outcome variable would be, given new data. For example, for a new individual with a worry score of 20 and describing score of 15: # specify the new data (for each predictor in the model) new_scores &lt;- tibble(worry = 20, describing = 15) # use augment() in the broom package to obtain the prediction library(broom) augment(multiple1, newdata = new_scores) worry describing .fitted 20 15 74.03992 .fitted contains the predicted wellbeing score, and is equal to 74.04. To derive predictions for several new participants, use c(score1, score2...) when specifying the new_scores. To add another individual with a worry score of 25 and describing score of 10 # specify the new data (for both predictors) new_scores &lt;- tibble(worry = c(20, 25), describing = c(15, 10)) # use augment() in the broom package for the predictions augment(multiple1, newdata = new_scores) worry describing .fitted 20 15 74.03992 25 10 63.94410 Each row contains the details for a new person. The .fitted column contains the predicted wellbeing scores: 74.04 and 63.94. Exercise 2.4 Predicting data For the model in the main exercise (i.e., gad ~ brooding + observing), what are the predicted values of gad for three new individuals with brooding scores of 10, 15, and 20, and observing scores of 3, 6 and 9, respectively. Predicted GAD of participant 1 (to one decimal place) = Predicted GAD of participant 2 (to one decimal place) = Predicted GAD of participant 3 (to one decimal place) = Solution new_scores &lt;- tibble(brooding = c(10, 15, 20), observing = c(3, 6, 9)) multiple2 &lt;- lm(gad ~ brooding + observing, data = wellbeing_data) augment(multiple2, newdata = new_scores) Exercise 2.5 Further exercise Not essential. Only for if you are really confident with everything. The variables attention, clarity and repair in the wellbeing_data are measures of emotional intelligence. To what extent do these three predictors uniquely explain wellbeing in a multiple regression? The BF for the unique contribution of attention = The BF for the unique contribution of clarity = The BF for the unique contribution of repair = Hint Obtain the BF for the full model and the BFs for the models in which each of the predictors have been left out. Compare the BF for the full model with that of the model with the predictor left out in order to obtain the BF for the unique contribution of that predictor. Solution Check correlations: wellbeing_data %&gt;% select(wellbeing, attention, clarity, repair) %&gt;% correlate(method = &quot;pearson&quot;)   Run multiple regression: multiple3 &lt;- lm(wellbeing ~ attention + clarity + repair, data = wellbeing_data) Adjusted R2 for the overall model: glance( multiple3 ) Using lmBF(): BF_attention_clarity_repair &lt;- lmBF(wellbeing ~ attention + clarity + repair, data = data.frame(wellbeing_data)) BF_attention_clarity &lt;- lmBF(wellbeing ~ attention + clarity, data = data.frame(wellbeing_data)) BF_attention_repair &lt;- lmBF(wellbeing ~ attention + repair, data = data.frame(wellbeing_data)) BF_clarity_repair &lt;- lmBF(wellbeing ~ clarity + repair, data = data.frame(wellbeing_data)) # unique contribution of attention BF_attention_clarity_repair / BF_clarity_repair # unique contribution of clarity BF_attention_clarity_repair / BF_attention_repair # unique contribution of repair BF_attention_clarity_repair / BF_attention_clarity Using regressionBF(): all_BFs &lt;- regressionBF(wellbeing ~ attention + clarity + repair, data = data.frame(wellbeing_data)) # look at output all_BFs # unique contribution of attention all_BFs[7] / all_BFs[6] # unique contribution of clarity all_BFs[7] / all_BFs[5] # unique contribution of repair all_BFs[7] / all_BFs[4] 2.7 Going further: p-values p-values As with simple regression, summary(model_name) can be used to obtain the results of the frequentist regression analyses. For the first model above: summary(multiple1) ## ## Call: ## lm(formula = wellbeing ~ worry + describing, data = wellbeing_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.8160 -5.0883 -0.9689 5.3367 23.7387 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 70.7306 7.6065 9.299 1.98e-13 *** ## worry -0.7708 0.1681 -4.585 2.21e-05 *** ## describing 1.2484 0.2776 4.497 3.02e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.393 on 63 degrees of freedom ## Multiple R-squared: 0.4653, Adjusted R-squared: 0.4484 ## F-statistic: 27.41 on 2 and 63 DF, p-value: 2.721e-09 The t-tests on the coefficients in a multiple regression assess the unique contribution of each predictor in the model. That is, they test the variance a predictor explains in an outcome variable, after the variance explained by the other predictors has been taken into account. The F-statistic tests whether the variance explained by the model as a whole is statistically significant or not. It is possible to think of the F-statistic and t-value in multiple regression in terms of the Venn diagram: The F-statistic compares the explained variance with the unexplained variance. The explained variance is represented by the outline of the two circles in the Venn diagram above. The unexplained variance is the remaining blue area of the rectangle. The t-value compares the unique variance a predictor explains with the remaining unexplained variance. For example, for project in the Venn diagram above, this would be the area in the orange crescent, relative to the remaining blue area in the rectangle. As with simple regression, the Bayes factors can be reported with the frequentist analyses as follows: \"Together, worry and describing explain 44.83% of the variance in wellbeing scores (adjusted R2), and there was strong evidence for this model, compared to an intercept only model, F(2,63) = 27.41, p &lt; .001, BF10 = 4,190,994. There's also substantial evidence that worry, b = -0.77, t(63) = -4.59, p &lt; .001, BF10 = 981.49 and describing, b = 1.25, t(63) = 4.50, p &lt; .001, BF10 = 738.41 make unique contributions to this model. The sign on the coefficients indicate that greater worry is associated with lower wellbeing. Greater mindfulness (describing) is associated with greater wellbeing.\" 2.8 Summary of key points Predictors can be added to a model in lm using the + symbol e.g., lm(wellbeing ~ worry + describing + predictor_3 + ....) Predictor variables are often correlated to some extent. This can affect the interpretation of individual predictor variables. Venn diagrams can help to understand the unique contributions of predictors In multiple regression, it's important to understand that each predictor makes a contribution to explaining the outcome variable only after taking into account the other predictors in the model. The Bayes factor for a multiple regression model tells us how much more likely it is, compared to an intercept-only (i.e., the null) model. To know whether there's evidence for the unique contribution of a predictor, we have to compare Bayes factors for the more complex model (which includes it), versus the simpler model (in which the predictor is left out). i.e., BF_more_complex_model / BF_simpler_model. Multicollinearity exists when predictors are highly correlated (r above 0.8 or below -0.8) and should be avoided by dropping one of the predictors. 2.9 References Iani, L., Quinto, R. M., Lauriola, M., Crosta, M. L., &amp; Pozzi, G. (2019). Psychological well-being and distress in patients with generalized anxiety disorder: The roles of positive and negative functioning. PloS one, 14(11), e0225646. https://doi.org/10.1371/journal.pone.0225646 "],["anova1.html", "Session 3 ANOVA: between-subjects designs 3.1 Overview 3.2 One-way between subjects ANOVA 3.3 Two-way between-subjects ANOVA 3.4 Exercise: one-way 3.5 Exercise: two-way 3.6 Summary 3.7 References", " Session 3 ANOVA: between-subjects designs Chris Berry 2025 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} 3.1 Overview Slides from the lecture part of the session: Download R Studio online Access here using University log-in So far we have used regression where both the outcome and predictor are continuous variables. When all the predictor variables in a regression are categorical, the analysis is called ANOVA, which stands for Analysis Of VAriance. Here we consider two types of ANOVA for between-subjects designs: one-way ANOVA and two-way ANOVA. We will consider other types in future sessions (e.g., for within-subjects/repeated measures designs). 3.2 One-way between subjects ANOVA A one-way between subjects ANOVA is used to compare the scores from a dependent variable across different groups of individuals. For example, do mood scores differ between three groups of individuals, where each group undergoes a different type of therapy as treatment? one-way means that there is one independent variable, for example, type of therapy. Independent variables are also called factors in ANOVA. A factor is made up of different levels. Type of therapy could have three levels: CBT (Cognitive Behavioural Therapy), EMDR (Eye Movement Desensitisation and Reprocessing), and Control. between subjects means that a different group of participants gives us mood scores for each level of the independent variable. For example, if type of therapy is manipulated between-subjects, one group receives CBT, another group receives EMDR, and another group are the control group. Each participant provides exactly one score. 3.2.1 Worked Example What is the effect of viewing pictures of different aesthetic value on a person's mood? To investigate, Meidenbauer et al. (2020) showed three groups of participants pictures of urban environments that were either very low in aesthetic value (n = 102), low in aesthetic value (n = 100), or high (n = 104). Participants' change in State Trait Anxiety Inventory (STAI: a measure of negative symptoms such as upset, tense, worried) as a result of viewing the pictures was measured. Exercise 3.1 Design check. What is the independent variable (or factor) in this design? change in STAI scoreaesthetic appeal of the pictures How many levels does the factor have? 1234 What is the dependent variable? VeryLow, Low, Highchange in STAI scoreaesthetic appeal of images What is the nature of the independent variable? categoricalcontinuous Is the independent variable manipulated within- or between-subjects? between-subjectswithin-subjects 3.2.2 Read in the data Read in the data at the link below and store in affect_data. Preview the data using head(). https://raw.githubusercontent.com/chrisjberry/Teaching/master/3_affect.csv (Note. The data in are taken from the Urban condition of Meidenbauer et al. (2020, Experiment 1). The data are publicly available through the links in their article. The variable names have been changed here for clarity.) # Read in the data affect_data &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/3_affect.csv&#39;) # look at the first 6 rows affect_data %&gt;% head() Key variables of interest in affect_data: group: the aesthetic value group, with levels VeryLow, Low and High score: the change in STAI score. Higher scores indicate fewer negative symptoms after viewing the images (i.e., improvement in mood). Variable labels Notice that the group column is by default read into R as a character variable (that's what the label &lt;chr&gt; means in the output). This is because the levels of group have been recorded in the dataset as words e.g., \"VeryLow\". 3.2.3 Convert the independent variable to a factor To enable us to use the group column in an ANOVA, we need to tell R that group is a factor. Use mutate() and factor() to convert group to a factor. # use mutate() to convert the group variable to a factor # store the changes back in affect_data # (i.e., overwrite what&#39;s already in affect_data) affect_data &lt;- affect_data %&gt;% mutate(group = factor(group)) Remember mutate() can be used to change variables or create new ones. The code mutate(group = factor(group)) means: group =: create a variable called group. Because group already exists in affect_data it will be overwritten. factor(group): convert the group variable to a factor. Tip If we'd have used: factor(group, levels = c(\"VeryLow\",\"Low\",\"High\")) instead of factor(group) this would mean that the levels of group will additionally be ordered according to the order in levels. This can be useful when plotting the data. Check Check that the variable label for group has now changed from &lt;chr&gt; to &lt;fct&gt; (i.e., a factor) by looking at the dataset again. affect_data 3.2.4 n of each group Use group_by() and count() to obtain the number of participants in each group: # use group_by() to group the output by &#39;group&#39; column, # then obtain number of rows in each group with count() affect_data %&gt;% group_by(group) %&gt;% count() How many participants were there in the VeryLow aesthetic value group? How many participants were there in the Low aesthetic value group? How many participants were there in the High aesthetic value group? 3.2.5 Visualise the data The way the data are distributed in each group can be inspected with histograms or density plots: # Histogram of scores in each group # Use facet_wrap(~group) to create a separate # panel for each group affect_data %&gt;% ggplot(aes(score)) + geom_histogram() + facet_wrap(~group) Figure 1.2: Histogram of scores in each aesthetic value group Tip To produce density plots, swap geom_histogram() with geom_density(). Comments on the histograms The spread of scores in each group appears relatively similar, suggesting the assumption of homogeneity of variance in ANOVA, may be met. The distributions are reasonably symmetrical, and, aside from the very high number of 0 scores in each group (indicative of zero change in STAI score in a large number of individuals), the data appear approximately normally distributed. In practice, ANOVA is considered reasonably robust against violations of the test's assumptions (see e.g., Glass et al. 1972, Schmider et al., 2010). 3.2.6 Plot the means Visualise the data further by obtaining a plot of the mean score in each group. The package ggpubr can produce high quality plots with ease. Using ggerrorplot() in ggpubr: # load the ggpubr package library(ggpubr) # plot the mean of each group # specify desc_stat = &quot;mean_se&quot; to # add error bars representing the standard error affect_data %&gt;% ggerrorplot(x = &quot;group&quot; , y = &quot;score&quot;, desc_stat = &quot;mean_se&quot;) + xlab(&quot;Aesthetic value group&quot;) Figure 3.1: Mean change in STAI score across aesthetic value groups (error bars indicate SE) From inspection of the means: Which aesthetic value group has the greatest improvement in STAI score? HighLowVeryLow Which aesthetic value group has the lowest improvement in STAI score? HighLowVeryLow Did STAI scores appear to worsen (i.e., be below zero) in any group as a result of viewing the images? High groupLow groupVeryLow group Developing the plot As with plots generated in ggplot(), the figure can be enhanced by adding further code, e.g., try adding the line: + ylab(\"Change in STAI (negative symptoms)\") Beneath the surface, ggpubr uses ggplot() to make graphs. Other types of plot are available in ggpubr, see e.g.: Errorplots: ?ggerrorplot() Boxplots: ?ggboxplot() Violin plots: ?ggviolin() I encourage you to play around to find clear and effective ways to visualise your data! To see more types of plot: help(package = ggpubr) The same plot in ggplot() It is of course possible to create the same figure using ggplot: # pipe data to ggplot # use stat_summary() to plot the mean # and again to plot errorbars affect_data %&gt;% ggplot(aes(x=group,y=score))+ stat_summary(fun=&quot;mean&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;) + theme_classic() ## Warning: Removed 3 rows containing missing values or values outside the scale range ## (`geom_segment()`). Figure 3.2: Mean change in STAI score across aesthetic value groups (error bars indicate SE) 3.2.7 Descriptives: Mean of each group Use summarise() and group_by() to obtain the mean (M) in each group: affect_data %&gt;% group_by(group) %&gt;% summarise(M = mean(score)) The mean score in the High group is (to 2 decimal places) The mean score in the Low group is (to 2 decimal places) The mean score in the VeryLow group is (to 2 decimal places) Tip - Standard Error The formula for the standard error of the mean is \\(SD / \\sqrt{n}\\). We can therefore obtain the standard error of the mean for each group as follows: affect_data %&gt;% group_by(group) %&gt;% summarise(SE = sd(score) / sqrt( n() )) To show the mean and SE in the same output: affect_data %&gt;% group_by(group) %&gt;% summarise( M = mean(score), SE = sd(score) / sqrt( n() ) ) 3.2.8 Bayes factor A Bayes factor can be obtained for the one-way ANOVA model using anovaBF(). The model we specify is of score on the basis of group (i.e., score ~ group). The BF will tell us how much more likely the model (with different three groups) is than an intercept-only model, in which all the scores are treated as coming from one large group. In other words, the BF will tell us whether we have evidence for an effect of aesthetic appeal on the STAI scores or not. To obtain the BF for the one-way ANOVA model, use anovaBF(): # ensure BayesFactor package is loaded # library(BayesFactor) # obtain the BF for the ANOVA model with lmBF() anovaBF( score ~ group, data = data.frame(affect_data) ) ## Bayes factor analysis ## -------------- ## [1] group : 66.65842 ±0.04% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS The Bayes Factor for the model is equal to BF = . This indicates that the model is over sixsixtysix hundred times more likely than an intercept-only model. There is therefore substantial evidence for an effect of the aesthetic value of urban images on changes in STAI scores. Tip - lmBF() lmBF() in the BayesFactor package can be used in place of lmBF() to produce exactly the same result: lmBF( score ~ group, data = data.frame(affect_data) ) Remember, this works because ANOVA is a special case of regression. 3.2.9 R2 R2 can be reported for ANOVA models as a measure of effect size. As with simple and multiple regression, R2 represents the proportion of variance explained by the model, where our model is that the scores come from distinct groups of individuals (three groups in our example) with different means. To obtain R2, first use lm() to specify the model, then use glance() from the broom package: # specify and store the anova anova_1 &lt;- lm(score ~ group, data = affect_data) # make sure broom package is loaded with library(broom), then # use glance() with anova_1 glance(anova_1) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.0523547 0.0460996 0.4743 8.369943 0.0002896 2 -204.4377 416.8754 431.7697 68.16302 303 306 Adjusted R2 (as a percentage, to two decimal places) = %, which represents the percentage of the variance in the change in STAI scores that is explained by the affect value of an image. (Remember, the values of R2 given by glance() are proportions, so need to be multiplied by 100 to get the percentage.) 3.2.10 Follow-up tests The Bayes factor (66.66) tells us that there's evidence that the means of the three groups differ from one another, but not which groups differ from which. With three groups, there are three possible pairwise comparisons that can be made: VeryLow vs. Low VeryLow vs. High Low vs. High To compare scores from two groups, we can use filter() to filter the affect_data so that it contains only the two groups we want to compare. Then use anovaBF() again to compare the scores across groups. The BF will tell us how many times more likely it is that there's a difference between means, compared to no difference. # Compare scores of VeryLow vs. Low groups # # Step 1. Filter affect_data for VeryLow and Low groups only # Store in &#39;groups_VeryLow_Low&#39; groups_VeryLow_Low &lt;- affect_data %&gt;% filter(group == &quot;VeryLow&quot; | group == &quot;Low&quot;) # # Step 2. Obtain BF for VeryLow vs. Low groups anovaBF(score ~ group, data = data.frame(groups_VeryLow_Low)) # Compare scores of VeryLow vs. High groups # # Step 1. Filter affect_data for VeryLow and High groups only # Store in &#39;groups_VeryLow_High&#39; groups_VeryLow_High &lt;- affect_data %&gt;% filter(group == &quot;VeryLow&quot; | group == &quot;High&quot;) # # Step 2. Obtain BF for VeryLow vs. Low groups anovaBF(score ~ group, data = data.frame(groups_VeryLow_High)) # Compare scores of Low vs. High groups # # Step 1. Filter affect_data to store Low and High groups only # Store in &#39;groups_Low_High&#39; groups_Low_High &lt;- affect_data %&gt;% filter(group == &quot;Low&quot; | group == &quot;High&quot;) # # Step 2. Obtain BF for VeryLow vs. Low groups anovaBF(score ~ group, data = data.frame(groups_Low_High)) ## Bayes factor analysis ## -------------- ## [1] group : 10.25237 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS ## ## Bayes factor analysis ## -------------- ## [1] group : 55.43484 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS ## ## Bayes factor analysis ## -------------- ## [1] group : 0.1774611 ±0.06% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS To two decimal places: The BF for the comparison of VeryLow vs. Low groups = The BF for the comparison of VeryLow vs. High groups = The BF for the comparison of Low vs. High groups = Interpretation: The one-way between subjects ANOVA indicated that there was evidence for an effect of aesthetic value on change in STAI scores (BF10 = 66.66). The scores in the VeryLow group were lower than those of the Low (BF10 = 10.25) and High groups (BF10 = 55.43), but scores in the Low and High groups did not differ (BF10 = 0.18). This indicates that STAI scores were lower (i.e., there were more negative symptoms) after viewing images that were very low in aesthetic value, compared to images that were low or high in aesthetic value. Viewing pictures that were low or high in aesthetic value resulted in similar changes in STAI scores. Further information on filter() The | symbol means \"or\". The == symbol (an equals sign typed twice) means \"is equal to\" So filter(group == \"VeryLow\" | group == \"Low\") means \"filter the rows of affect_data when the labels in group are equal to VeryLow OR the labels in group are equal to Low An equivalent approach: ttestBF() anovaBF() was used to conduct follow-up tests. Because each test had two groups, this is equivalent to a Bayesian t-test, and so the exact same BFs could have been obtained using ttestBF(): # Compare scores of VeryLow vs. Low groups ttestBF( x = affect_data$score[ affect_data$group==&quot;VeryLow&quot; ], y = affect_data$score[ affect_data$group==&quot;Low&quot; ] ) # compare scores of VeryLow vs. High groups ttestBF( x = affect_data$score[ affect_data$group==&quot;VeryLow&quot; ], y = affect_data$score[ affect_data$group==&quot;High&quot; ] ) # compare scores of Low vs. High groups ttestBF( x = affect_data$score[ affect_data$group==&quot;Low&quot; ], y = affect_data$score[ affect_data$group==&quot;High&quot; ] ) 3.3 Two-way between-subjects ANOVA In a two-way ANOVA, there are two categorical independent variables or factors. When there are multiple factors, the ANOVA is referred to as factorial ANOVA. For example, if the design has two factors, and each factor has two levels, then we refer to the design as a 2 x 2 factorial design. The first number (2) denotes the number of levels of the first factor. The second number (2) denotes the number of levels of the second factor. If, instead, the second factor had three levels, we'd say we have a 2 x 3 factorial design. 3.3.1 Worked example What is the role of resilience in the distress experienced from childhood adversities? Beutel et al. (2017) analysed the distress scores from 2,437 individuals who were either low or high in trait resilience and had experienced either low or high levels of childhood adversity. Exercise 3.2 Design check. What is the first independent variable (or factor) that is mentioned in this design? distress scorechildhood adversityresilience How many levels does the first factor have? 1234 What is the second independent variable (or factor)? distress scorechildhood adversitytrait resilience How many levels does the second factor have? 1234 What is the dependent variable? distress scorechildhood adversitytrait resilience What is the nature of the independent variables? categoricalcontinuous What type of design is this? 2 x 2 x 2 between subjects factorial design2 x 3 between subjects factorial design2 x 2 between subjects factorial designcorrelational design 3.3.2 Read in the data Read in the data at the link below and store in resilience_data: https://raw.githubusercontent.com/chrisjberry/Teaching/master/3_resilience_data.csv # read in the data resilience_data &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/3_resilience_data.csv&#39;) # preview head(resilience_data) distress resilience adversity sex partnership education income unemployed 0 high low 1 1 2 2 0 1 low low 1 1 1 2 0 0 high low 1 2 1 1 0 0 high low 1 1 1 2 0 1 low low 1 2 2 2 0 1 high low 1 1 1 2 0 distress contains the distress scores. Higher scores indicate greater levels of distress. resilience labels the levels of childhood adversity ( low or high) adversity labels the levels of trait resilience ( low or high) (Note. The data are publicly available, but I have changed some of the variable names for clarity.) 3.3.3 Convert the independent variables to factors To enable the factors to be used as such in ANOVA, we need to convert them to factors using factor(): # use mutate() and factor() to convert # resilience and adversity to factors resilience_data &lt;- resilience_data %&gt;% mutate(resilience = factor(resilience), adversity = factor(adversity)) 3.3.4 n in each group Obtain the number of participants in each group: # use count() to obtain the number of rows in the dataset, # group_by() both resilience AND adversity resilience_data %&gt;% group_by(resilience, adversity) %&gt;% count() resilience adversity n high high 106 high low 997 low high 284 low low 1050 In a between-subjects factorial design, participants are assigned to groups by crossing the levels of one variable with those of the second variable. Thus, each row labels the level of resilience and level of adversity for that group. The number of participants in the high resilience, high adversity group is The number of participants in the high resilience, low adversity group is The number of participants in the low resilience, high adversity group is The number of participants in the low resilience, low adversity group is 3.3.5 Visualise the data The way the data are distributed in each group can be inspected with histograms or density plots. # Use `facet_wrap(~ resilience * adversity)` # to plot scores for all combinations of # resilience x adversity levels # use &#39;labeller = label_both&#39; to label levels by factor name resilience_data %&gt;% ggplot(aes(distress)) + geom_density() + facet_wrap(~ resilience * adversity, labeller = label_both) Figure 3.3: Density plots showing distress scores in each group) Interpretation: The data in each group appear positively skewed - the tail of the distribution goes towards the right (i.e., towards more positive values of distress). Beutel et al. (2017) took no further action and analysed the scores as they were. 3.3.6 Plot the means Use ggbarplot() in the ggpubr package: library(ggpubr) # plot the mean of each group # use &#39;desc_stat = &quot;mean_se&quot; to add SE error bars # use &#39;position = position_dodge(0.3)&#39; so that points are spaced resilience_data %&gt;% ggerrorplot(x = &quot;resilience&quot;, y = &quot;distress&quot;, color = &quot;adversity&quot;, desc_stat = &quot;mean_se&quot;, position = position_dodge(0.3)) + xlab(&quot;Resilience&quot;) + ylab(&quot;Distress&quot;) Figure 3.4: Distress as a function of adversity and resilience (error bars indicate SE of the mean) Same plot using ggplot resilience_data %&gt;% ggplot(aes(x=resilience,y=distress,color=adversity))+ stat_summary(fun.data = mean_se,geom = &quot;errorbar&quot;, position = position_dodge(width=0.5))+ stat_summary(fun=&quot;mean&quot;, position = position_dodge(width=0.5)) + theme_classic() ## Warning: Removed 4 rows containing missing values or values outside the scale range ## (`geom_segment()`). Figure 3.5: Distress as a function of adversity and resilience (error bars indicate SE of the mean) In a two-way design, researchers look at three things: The main effect of factor 1: Overall, do scores differ according to the levels of factor 1? The main effect of factor 2: Overall, do scores differ according to the levels of factor 2? The interaction between the factors: Is the effect of one factor different at each level of the other factor? We can get some idea of the main effects and interaction by inspecting the plot of the means: The main effect of resilience: Overall, distress scores in the high resilience groups appear to be lower thanabout the same ashigher than those in the low resilience groups. The main effect of adversity: Overall, distress scores in the low adversity groups appear to be lower thanabout the same ashigher than those in the high adversity groups. The interaction between resilience and distress: When trait resilience is low rather than high, the effect of adversity on distress appears to be lowersimilargreater. (Hint: the effect of adversity is indicated by the difference between the red and blue points.) 3.3.7 Bayes factors Use anovaBF() in the BayesFactor package to obtain Bayes factors corresponding to the main effect of factor 1, the main effect of factor 2, and the interaction between factor 1 and factor 2. To specify the two-way ANOVA in anovaBF(), use dependent_variable ~ factor1 * factor2. For the resilience data: # Obtain the Bayes Factors for the ANOVA model anova2x2_BF &lt;- anovaBF( distress ~ resilience * adversity, data = data.frame(resilience_data) ) # look at the output anova2x2_BF ## Bayes factor analysis ## -------------- ## [1] resilience : 1.822053e+27 ±0% ## [2] adversity : 7.870878e+25 ±0% ## [3] resilience + adversity : 3.303448e+46 ±1.35% ## [4] resilience + adversity + resilience:adversity : 3.18965e+49 ±4.21% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS What does 1.82e+27 mean? It means 1.82 x 1027. Or 1820000000000000000000000000. A very large number! For more information see: FAQ (Opens a new tab.) Please take care to notice \"e+\" or \"e-\" in the output for your Bayes factors. As you can see, BF10 = 1.82 means something drastically different to BF10 = 1.82 x 1027, and you wouldn't want to make this kind of mistake when drawing statistical inferences! Each BF in the output compares how likely the model is, compared to an intercept only model: [1] resilience is the BF for the main effect of resilience. It's how much more likely a model with resilience alone is than an intercept-only model. [2] adversity is the BF for the main effect of adversity. It's how much more likely a model with adversity alone is than an intercept-only model. [3] resilience + adversity is the BF for the main effects of resilience and adversity. It's how much more likely a model with main effects of resilience and adversity is than an intercept-only model. [4] resilience + adversity + resilience:adversity is the BF for the main effects of resilience and adversity and also the interaction between them (resilience:adversity). It's how much more likely a model with main effects of resilience and adversity and also the interaction (resilience:adversity) is than an intercept-only model. [1] and [2] correspond to the main effects of resilience and adversity, respectively. To obtain the BF for the interaction, we need to divide [4] by [3]: # BF for the interaction anova2x2_BF[4] / anova2x2_BF[3] ## Bayes factor analysis ## -------------- ## [1] resilience + adversity + resilience:adversity : 965.5518 ±4.42% ## ## Against denominator: ## distress ~ resilience + adversity ## --- ## Bayes factor type: BFlinearModel, JZS This BF then tells us whether there's evidence for the addition of an interaction term to a model containing the main effects of each factor. This is the BF for the interaction. Record the Bayes factors below: The BF for the main effect of resilience is BF10 = x 1027. The BF for the main effect of adversity is BF10 = x 1025. The BF for the resilience and adversity interaction is approximately (to the nearest whole number). Remember, this is the BF you calculated by dividing [4] by [3]. BF10 = . Meaning of ±number% You'll notice that some of the BFs had ±1.03% or similar next to them in the output. This is the error associated with the BF. It's like saying my height is 185 cm, plus or minus a cm or so. It can be non-zero because generation of the BFs involves random sampling processes. Larger error values mean that the exact same value of the BF won't necessarily be output each time the line of code containing anovaBF() is run, so there's a chance that the BFs in your output differ slightly from those above (particularly for the interaction). This is why I asked you to give your answer to the nearest whole number. 3.3.8 R2 Once again, glance() can be used to obtain R2 for the ANOVA model. The model first needs to be specified with lm(). Using factor1 * factor2 when specifying the model is a shortcut, which will automatically specify the full model containing the interaction. Thus, we can use lm(distress ~ resilience * adversity), which is equivalent to lm(distress ~ resilience + adversity + resilience*adversity). # specify the ANOVA model using lm() anova2x2 &lt;- lm(distress ~ resilience * adversity, data = resilience_data) # R^2^ glance(anova2x2) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.0963021 0.0951878 2.142569 86.42379 0 3 -5312.959 10635.92 10664.91 11168.93 2433 2437 The adjusted R2 of the ANOVA model (to two decimal places, as a percentage) is %, which is the percentage of variance in distress explained by the model. Going further - R2 for each model component Rather than report the R2 for the overall model in an article, we often would rather report the R2 value associated with each of the main effects and interaction. It is easiest to obtain this by running the ANOVA using the afex package. We'll do this in more detail in Session 6. # load afex library(afex) # add a column to code the ppt in the dataset resilience_data &lt;- resilience_data %&gt;% mutate(ppt = c(1:n())) # use the afex package to run a two-way ANOVA # id is the column of participant identifiers (&quot;ppt&quot;) # dv is the dependent variable (&quot;distress&quot;) # between specifies the between-subjects factor anova2bet &lt;- afex::aov_ez(id = c(&quot;ppt&quot;), dv = c(&quot;distress&quot;), between = c(&quot;resilience&quot;,&quot;adversity&quot;), # two factors data = resilience_data) anova2bet The R2 values are referred to as generalised eta squared. The values for each main effect and the interaction term are in the column ges. 3.3.9 Follow-up comparisons Given that we have evidence for an interaction, anovaBF() can be used to conduct follow-up comparisons to explore the nature of the interaction. (Note, we would not do this if the BF did not show evidence for the interaction, i.e., if the BF was less than 3.) The interaction implies the effect of adversity is different in individuals with high resilience, and those with low resilience. To determine the evidence for the effect of adversity in individuals with high resilience: # 1. The effect of adversity in individuals with high resilience # First use filter() to store only # the data from the &#39;high&#39; resilience groups resilience_high &lt;- resilience_data %&gt;% filter(resilience == &quot;high&quot;) # Then use `anovaBF()` to look at effect of adversity in resilience_high anovaBF( distress ~ adversity, data = data.frame(resilience_high) ) ## Bayes factor analysis ## -------------- ## [1] adversity : 1.026598 ±0.02% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS To determine the evidence for the effect of adversity in individuals with low resilience: # 2. The effect of adversity in individuals with low resilience # First use filter() to store only # the data from the &#39;low&#39; resilience groups resilience_low &lt;- resilience_data %&gt;% filter(resilience == &quot;low&quot;) # Then use `anovaBF()` to look at effect of adversity in resilience_low anovaBF( distress ~ adversity, data = data.frame(resilience_low) ) ## Bayes factor analysis ## -------------- ## [1] adversity : 2.389569e+17 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS This confirms the interaction that is apparent in the plot: There's no evidence for an effect of adversity within high resilience individuals (BF10 = ), but there's substantial evidence for an effect of adversity within low resilience individuals (BF10 = x 1017), such that levels of distress are greatest when the level of adversity is highest. Interestingly, this suggests that resilience may have a buffering effect on the distress experienced as a result of childhood adversity (see Beutel et al., 2017, for further discussion). 3.4 Exercise: one-way Exercise 3.3 One-way ANOVA The data at the link below are from a study by Bobak et al. (2016). They looked at the face matching performance of individuals with superior face recognition abilities, so called \"super recognisers\". Their performance was compared to two control groups. In one control group, payment was linked to performance (labelled \"motivated_control\"). The individuals in the other control group were not paid (labelled \"control\"). The column face_group contains the labels for each group. https://raw.githubusercontent.com/chrisjberry/Teaching/master/3_super_data.csv Conduct a one-way ANOVA to compare the performance across the three groups. Adapt the code in this worksheet to do the following: Try not to look at the solutions before you've attempted them 1. Read in the data and store in super_data Hint Ensure the tidyverse package is loaded. See ?read_csv() Solution super_data &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/3_super_data.csv&#39;) # look at the raw data super_data 2. Convert the independent variable to a factor Hint See ?mutate() and ?factor() Solution super_data &lt;- super_data %&gt;% mutate( face_group = factor(face_group) ) 3. Obtain n in each group Hint Pipe the data to group_by() and count() Solution super_data %&gt;% group_by(face_group) %&gt;% count() n super recognisers = n motivated_control = n control = 4. Produce a histogram of scores in each group Hint Pipe the data to ggplot() and geom_histogram() and facet_wrap() Solution super_data %&gt;% ggplot(aes(performance)) + geom_histogram() + facet_wrap(~face_group) 5. Produce a plot of the means (with SEs) in each group Hint ?ggerrorplot() in the ggpubr package Solution # produce means plot super_data %&gt;% ggerrorplot(x = &quot;face_group&quot;, y =&quot;performance&quot;, desc_stat = &quot;mean_se&quot;) + xlab(&quot;Group&quot;) + ylab(&quot;Face matching performance&quot;) Face matching performance appears to be highest in the control groupmotivated control groupsuper_recogniser group, followed by the control groupmotivated control groupsuper_recogniser group, followed lastly by the control groupmotivated control groupsuper_recogniser group. 6. Obtain the mean and SE of each group Hint Use group_by() and summarise() Solution super_data %&gt;% group_by(face_group) %&gt;% summarise( M = mean(performance), SE = sd(performance) / sqrt( n() ) ) The mean score of the super_recogniser group is , SE = The mean score of the motivated_control group is , SE = The mean score of the control group is , SE = 7. Obtain the Bayes factor for the ANOVA model The Bayes factor for the model is equal to (to two decimal places) . This indicates that there is no evidencesubstantial evidence for an effect of the face group on matching performance. Hint ?anovaBF() Solution anovaBF(performance ~ face_group, data = data.frame(super_data)) 8. What is R2 for the model? The adjusted R2 for the effect of face group on matching performance is (as a percentage, to two decimal places) %. Hint obtain the model with lm(), then use glance() Solution super_anova &lt;- lm(performance ~ face_group, data = super_data) glance(super_anova) # multiply the adj.r.squared value by 100 to obtain the percentage 0.301*100 9. Conduct follow-up tests to compare the mean score of each group Hint Use filter() to create new variables containing the scores of two groups at a time Then use anovaBF() to compare the scores of each group. Solution using anovaBF() # super_recognisers vs. motivated_control super_vs_motivated_controls &lt;- super_data %&gt;% filter(face_group == &quot;super_recogniser&quot; | face_group == &quot;motivated_control&quot;) anovaBF(performance ~ face_group, data = data.frame(super_vs_motivated_controls)) # super_recognisers vs. control super_vs_controls &lt;- super_data %&gt;% filter(face_group == &quot;super_recogniser&quot; | face_group == &quot;control&quot;) anovaBF(performance ~ face_group, data = data.frame(super_vs_controls)) # motivated_control vs. control motivated_control_vs_control &lt;- super_data %&gt;% filter(face_group == &quot;motivated_control&quot; | face_group == &quot;control&quot;) anovaBF(performance ~ face_group, data = data.frame(motivated_control_vs_control)) Solution using ttestBF() # super_recognisers vs. motivated_control ttestBF(x = super_data$performance[super_data$face_group == &quot;super_recogniser&quot;], y = super_data$performance[super_data$face_group == &quot;motivated_control&quot;]) # super_recognisers vs. control ttestBF(x = super_data$performance[super_data$face_group == &quot;super_recogniser&quot;], y = super_data$performance[super_data$face_group == &quot;control&quot;]) # motivated_control vs. control ttestBF(x = super_data$performance[super_data$face_group == &quot;motivated_control&quot;], y = super_data$performance[super_data$face_group == &quot;control&quot;]) The Bayes factor comparing the super_recogniser and motivated_control groups is , indicating substantial evidence forsubstantial evidence against there being a difference between groups. Face matching performance in the super_recogniser group was the samelowerhigher than that of the motivated_control group. The Bayes factor comparing the super_recogniser and control groups is , indicating substantial evidence forsubstantial evidence against there being a difference between groups. Face matching performance in the super_recogniser group was the samelowerhigher than that of the control group. The Bayes factor comparing the motivated_control and control groups is , indicating that there was substantial evidence for ainsufficient evidence for asubstantial evidence for an absence of a difference between the scores of each group. 3.5 Exercise: two-way Exercise 3.4 Two-way ANOVA Horstmann et al. (2018) looked at whether the type of exchange that people had with a robot would affect how long it would then take for them to switch it off. The type of exchange was either 'functional' or 'social'. Additionally, the researchers looked at the effect of the type of objection that the robot made during the conversation. The robot either voiced an 'objection' to being switched off, or voiced 'no objection'. Design check. What is the first independent variable (or factor) that is mentioned in this design? type of objectiontime to switch offtype of exchange How many levels does the first factor have? 1234 What is the second independent variable (or factor)? type of objectiontime to switch offtype of exchange How many levels does the second factor have? 1234 What is the dependent variable? type of objectiontime to switch offtype of exchange What is the nature of the independent variables? categoricalcontinuous What is the nature of the dependent variable? categoricalcontinuous What type of design is this? 2 x 2 x 2 between subjects factorial design2 x 3 between subjects factorial design2 x 2 between subjects factorial designcorrelational design The robot_data are located at the link below: https://raw.githubusercontent.com/chrisjberry/Teaching/master/3_robot_data.csv Adapt the code in this worksheet to do the following: Conduct a two-way ANOVA to compare the effects of exchange and objection on time. Determine whether there's sufficient evidence for the following: The main effect of objection: BF10 (to two decimal places) = The main effect of exchange: BF10 (to two decimal places) = The interaction between exchange and objection: BF10 (nearest whole number) = Conduct any follow-up tests concerning the interaction (if necessary). The effect of exchange when the robot objected: BF10 (to two decimal places) = The effect of exchange when the robot did not object: BF10 (to two decimal places) = Interpret the main effects and interaction. What do they mean? Hint - code Read in the data to a variable called robot_data Convert the independent variables to factors using factor() Examine the distribution of time in each group using geom_histogram() or geom_density() in ggplot() Obtain summary statistics using group_by(), count() and summarise(mean()) Use a plot from the ggpubr package to create a plot of the means (e.g., ggerrorplot()) Use anovaBF() to obtain the Bayes factors to allow you to assess evidence for the main effects and interaction. If there's evidence for an interaction, then conduct follow-up tests using anovaBF() or ttestBF() Solution - code # ensure following packages have been loaded # library(tidyverse) # library(BayesFactor) # library(ggpubr) # read the data robot_data &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/3_robot_data.csv&#39;) # convert IVs to factors robot_data &lt;- robot_data %&gt;% mutate(exchange = factor(exchange), objection = factor(objection)) # inspect distributions robot_data %&gt;% ggplot( aes(time) ) + geom_histogram() + facet_wrap(~ exchange*objection) # obtain M and SE each group robot_data %&gt;% group_by(exchange, objection) %&gt;% summarise( M = mean(time), SE = sd(time)/sqrt(n()) ) # plot of means robot_data %&gt;% ggerrorplot(x = &quot;objection&quot;, y = &quot;time&quot;, color = &quot;exchange&quot;, desc_stat = &quot;mean_se&quot;, position = position_dodge(0.3)) + xlab(&quot;Type of objection&quot;) + ylab(&quot;Switch off time (seconds)&quot;) # 2x2 ANOVA BF_robot &lt;- anovaBF( time ~ exchange*objection, data = data.frame(robot_data) ) # look at BFs BF_robot # main effect objection BF_robot[1] # main effect exchange BF_robot[2] # interaction BF_robot[4] / BF_robot[3] # Given the evidence for an interaction, conduct further comparisons # 1. The effect of exchange when the robot objected (i.e., `objection`) # use filter() to store only # the data from the &#39;objection&#39; groups robot_objection &lt;- robot_data %&gt;% filter(objection == &quot;objection&quot;) # use `anovaBF()` to look at effect of exchange in the objection groups anovaBF( time ~ exchange, data = data.frame(robot_objection) ) # 2. The effect of exchange when the robot did not object # use filter() to store only # the data from the &#39;no_objection&#39; groups robot_no_objection &lt;- robot_data %&gt;% filter(objection == &quot;no_objection&quot;) # use `anovaBF()` to look at effect of exchange in the no objection groups anovaBF( time ~ exchange, data = data.frame(robot_no_objection) ) # For the interpretation of main effects: # # Obtain M and SE for the main effect of objection robot_data %&gt;% group_by(objection) %&gt;% summarise( M = mean(time), SE = sd(time)/sqrt(n()) ) # Obtain M and SE for the main effect of exchange robot_data %&gt;% group_by(exchange) %&gt;% summarise( M = mean(time), SE = sd(time)/sqrt(n()) ) Hint - interpretation To aid your interpretation of the main effects and interaction, look at the mean of the scores in each group. In which groups is the time taken to switch off the robot longer than others? How does time differ according to type of exchange? How does time differ according to type of objection? Does the effect of exchange seem to be the same at each level of objection? Solution - interpretation There was substantial evidence for a main effect of the type of objection on the time it took for a participant to switch off a robot (BF10 = 6.29). Participants took longer when the robot had previously mentioned that it objected to being switched off (M = 10.00 seconds, SE = 2.21), compared to when no objection was mentioned (M = 4.69, SE = 0.37). There was insufficient evidence for a main effect of the type of exchange, given that the Bayes factor was inconclusive (BF10 = 0.74). Thus, there was no evidence to suggest that the time to switch off a robot differed according to whether the type of exchange was functional (M = 8.69, SE = 2.00) or social (M = 5.54, SE = 0.57). The main effects should be viewed in light of the substantial evidence for an interaction between exchange and objection (BF10 = 3.28). This indicated that it took people longer to switch the robot off when the exchange had been functional, rather than social, but only if the robot had objected to being switched off (M functional = 14.40, SE = 4.11 vs. M social = 6.19, SE = 1.15). When the robot had not previously objected to being switched off, the times were similar following functional and social exchanges (M functional = 4.28, SE = 0.59, vs. M social = 5.05, SE = 0.48). Follow-up tests indicated insufficient evidence for the effect of exchange at each level of objection (i.e., BF10s were between 0.33 and 3): The Bayes factor for the effect of exchange when the robot objected was 1.55, and the Bayes factor for the effect of exchange when the robot did not object was 0.47. Thus, although there was evidence for an interaction, the pattern of differences within objection conditions was not confirmed by the follow-up tests. 3.6 Summary ANOVA is a special case of regression when the predictor variables are entirely categorical. A one-way between-subjects ANOVA has one independent variable, and separate groups of participants for each level of the independent variable. The test looks at whether the mean scores differ between groups. A two-way between-subjects ANOVA has two independent variables (factors). Each factor has levels. Researchers examine evidence for the main effect of each factor and their interaction. The interaction examines the effect of one factor at each level of the other factor. If there's evidence for an interaction, follow-up comparisons can be performed. Use anovaBF() to obtain Bayes factors for the main effects and interaction. 3.7 References Beutel M.E., Tibubos A.N., Klein E.M., Schmutzer G., Reiner I., Kocalevent R-D., et al. (2017) Childhood adversities and distress - The role of resilience in a representative sample. PLoS ONE, 12(3): e0173826. https://doi.org/10.1371/journal.pone.0173826 Glass G.V., Peckham P.D., Sanders J.R. (1972). Consequences of failure to meet assumptions underlying the fixed effects analyses of variance and covariance. Review of Educational Research. 42, 237–288. https://doi.org/10.3102%2F00346543042003237 Horstmann A.C., Bock N., Linhuber E., Szczuka J.M., Straßmann C., Kramer N.C. (2018) Do a robot’s social skills and its objection discourage interactants from switching the robot off? PLoS ONE, 13(7): e0201581. https://doi.org/10.1371/journal.pone.0201581 Meidenbauer, K. L., Stenfors, C. U., Bratman, G. N., Gross, J. J., Schertz, K. E., Choe, K. W., &amp; Berman, M. G. (2020). The affective benefits of nature exposure: What's nature got to do with it?. Journal of Environmental Psychology, 72, 101498. https://doi.org/10.1016/j.jenvp.2020.101498 Schmider E., Ziegler M., Danay E., Beyer L., Bühner M. (2010). Is it really robust? Methodology. 6, 147–151. https://doi.org/10.1027/1614-2241/a000016 "],["multiple2.html", "Session 4 Multiple regression: one continuous, one categorical 4.1 Overview 4.2 Worked example 4.3 Read in the data 4.4 Visualise the data 4.5 Evidence for the interaction 4.6 Simple slopes analysis 4.7 Exercises 4.8 Further exercise 4.9 Further knowledge 4.10 Summary 4.11 References", " Session 4 Multiple regression: one continuous, one categorical Chris Berry 2025 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} 4.1 Overview Slides from the lecture part of the session: Download R Studio online Access here using University log-in So far we have looked at simple and multiple regression with continuous variables. (And in the last session, we saw that between-subjects ANOVA was equivalent to a multiple regression in which all the predictors are categorical.) Here, we look at multiple regression with a mixture of continuous and categorical predictors. Specifically, one predictor is continuous, and the other is a dichotomous categorical variable (i.e., made up of two levels or groups). This type of analysis can be used to see whether the relationship between two variables is the same or different across groups of individuals. For example, adherence to a particular treatment (e.g., CBT) may be associated with attitudes towards the efficacy of the treatment, but the nature of the association may differ according to the type of psychological disorder a person has (e.g., depressed vs. not depressed). Thus, in this type of analysis, in addition to looking at the ability of individual predictors to explain an outcome variable, we're able to look at their combined effect in explaining the outcome. This is referred to as an interaction effect. The interaction can tell us whether the relationship between one of the predictors and the outcome differs as a function of the levels of the other predictor variable. 4.2 Worked example Scientists often research things that are of personal interest to themselves. Do we trust the researcher more when they have some personal investment in what they are researching? Altenmuller et al. (2021) looked at whether participants' trust in the researcher is related to 1) whether they are told that the researcher is personally affected by the research or not affected by their research, and 2) the participant's attitude towards the research topic. The data from Altenmuller et al.'s (2021) study (Experiment 2) are stored at the link below. https://raw.githubusercontent.com/chrisjberry/Teaching/master/4_trust_data.csv The key variables: trustworthiness: how trustworthy the participant finds the researcher. Higher scores indicate higher levels of trustworthiness. attitude: the participant's attitude towards the research topic. Higher scores indicate a more positive attitude. group: whether participants were told that the researcher is personally affected or not_affected by their own research. More about the data The data were made publicly available by the researchers and have been pre-processed (using the researcher's open R code). Only a subset of the data is used here for teaching purposes; variable names have been changed for clarity. Exercise 4.1 Design check. What is the outcome variable in this design? attitudetrustworthinessgroup What is the nature of the outcome variable? categoricalcontinuous What is the name of the continuous predictor variable? groupattitudetrustworthiness What is the name of the categorical predictor variable? groupattitudetrustworthiness How many levels of group are there? 123 When a variable has two levels it is called a continuousdichotomousquantitative variable. 4.3 Read in the data Read in the data to trust_data # ensure tidyverse is loaded # library(tidyverse) # read in the data trust_data &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/4_trust_data.csv&#39;) # look at the data trust_data %&gt;% head() ppt group attitude trustworthiness credibility evaluation 1 not_affected 5.714286 4.428571 4.428571 2.583333 2 affected 3.142857 4.285714 4.428571 3.250000 3 not_affected 5.285714 4.428571 4.428571 2.750000 4 not_affected 4.428571 4.642857 3.857143 2.416667 5 affected 6.000000 6.000000 5.285714 2.333333 6 not_affected 3.642857 5.500000 4.571429 2.500000 4.4 Visualise the data Use a scatterplot to look at the relationship between trustworthiness, attitude and group. Use different colours for each group by specifying colour = group in aes(): trust_data %&gt;% ggplot(aes(x = attitude, y = trustworthiness, colour = group )) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_light() Figure 1.1: Trustworthiness vs. participant attitude to research Exercise 4.2 Visual inspection of the scatterplot Does the relationship between attitude and trustworthiness appear to be the same within each group? yesno Within the group that were told that the researcher was personally affected by their research: The association between attitude and trustworthiness appears to be negativepositive A more positive attitude towards the research topic tends to be associated with lowerno changehigher values of trustworthiness. Within the group that were told that the researcher was personally not_affected by their research: The association between attitude and trustworthiness appears to be negativepositive A more positive attitude towards the research topic tends to be associated with lowerno changehigher values of trustworthiness. Go further with ggplot The scatterplot can be improved through further customisation. For example, to change the x- and y- labels, add the code: + xlab(\"Participants' attitude to the research\") + ylab(\"Trustworthiness of researcher\") Feel free to customise the plot further if you feel it can be improved! Further inspection It is good practice to inspect the distributions of the data prior to analysis, for example, with geom_histogram() or geom_density(). We can check if the data appear normally distributed, or positively or negatively skewed. We can also check for outliers using geom_boxplot(). Get to know your data! # look at the distribution of trustworthiness trust_data %&gt;% ggplot(aes(trustworthiness)) + geom_histogram() # look at the distribution of attitude trust_data %&gt;% ggplot(aes(attitude)) + geom_histogram() # obtain n in affected &amp; not_affected groups trust_data %&gt;% group_by(group) %&gt;% count() 4.5 Evidence for the interaction An interaction between the attitude and group predictors is suggested by the scatterplot. That is, the association between trustworthiness and attitude appears to be different in the affected and non_affected groups. We may therefore want to include the interaction term in our multiple regression model and determine whether we have evidence for the interaction or not using Bayes factors. We'll do this in three steps: Specify the model without the interaction Specify the model with the interaction Compare the model with and without the interaction 4.5.1 Model without the interaction The model without an interaction looks like a typical multiple regression model with two predictors (from Session 2): model &lt;- lm(outcome ~ predictor_1 + predictor_2, data = mydata) As we did in the previous session, we need to also convert the categorical variable (group) to a factor() to ensure it's treated as categorical by R. Thus, for our data: # Convert group to a factor trust_data &lt;- trust_data %&gt;% mutate(group = factor(group)) # Specify the model without the interaction without_interaction &lt;- lm(trustworthiness ~ attitude + group, data = trust_data) # R² for the model # load the broom package if you haven&#39;t already # library(broom) glance(without_interaction) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.0130486 0.0065555 0.7588684 2.009602 0.1358188 2 -349.3972 706.7944 721.7018 175.0679 304 307 How much of the variance in trustworthiness is explained by the model with attitude and group? The adjusted R2 (as a proportion, to two decimal places) = . Hint: rounding Did you know that R can round values to two decimal places for you? Use this code in your script: # round adjusted r-squared to 2.D.P round(0.0065555, 2) Next use lmBF() to obtain the BF for the model with no interaction: # library(BayesFactor) # obtain BF for the model BF_without_interaction &lt;- lmBF(trustworthiness ~ attitude + group, data = data.frame(trust_data)) # look at the BF BF_without_interaction ## Bayes factor analysis ## -------------- ## [1] attitude + group : 0.1028322 ±1.59% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS The BF for the model without the interaction is equal to (to 2 decimal places) . Meaning of ±number% You'll notice that the BF has ±4.03% or similar next to it in the output. This is the error associated with the BF. It's like saying my height is 185 cm, plus or minus 4 cm or so. It can be non-zero because generation of the BFs involves random sampling processes. Larger error values mean that the exact same value of the BF won't necessarily be output each time the line of code containing lmBF() is run, so there's a chance that the BFs in your output differ slightly from those above. It should be very low, and approximately 0.10 though. 4.5.2 Model with the interaction To specify an interaction between two predictors, use the term predictor1 * predictor2. The * symbol means multiply, so the interaction term is simply the predictors multiplied together. To specify the model with the interaction, add the interaction term predictor1 * predictor2 to the model without the interaction term: # Specify the model with the interaction with_interaction &lt;- lm(trustworthiness ~ attitude + group + attitude*group, data = trust_data) # R² for the model glance(with_interaction) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.0623098 0.0530257 0.740907 6.711482 0.0002128 3 -341.5378 693.0756 711.7098 166.3298 303 307 How much of the variance in trustworthiness is explained by the model with attitude,group, and the attitude*group interaction? The adjusted R2 (as a proportion, to two decimal places) = . Use lmBF() to obtain the BF for the model with the interaction: # obtain BF for the model BF_with_interaction &lt;- lmBF(trustworthiness ~ attitude + group + attitude*group, data = data.frame(trust_data)) # look at the BF BF_with_interaction ## Bayes factor analysis ## -------------- ## [1] attitude + group + attitude * group : 34.01636 ±6.49% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS The BF for the model with the interaction is equal to . (Note: yours may come out slightly different to that shown in the worksheet due to random sampling processes used in the BF generation; as long as the BF is around 30, that's okay.) 4.5.3 Compare the model with and without the interaction To determine whether there's evidence for the interaction, we need to compare the Bayes factor for the model with and without the interaction. Exercise 4.3 Bayes factors can be compared with the formula: \\(\\frac{Bayes \\ factor \\ more \\ complex \\ model}{Bayes \\ factor \\ simpler \\ model}\\) Dividing the BFs in this way will return another Bayes factor, which is a number that tells us how many times more likely the more complex model compared to the simpler model, given the data. So, if we perform this calculation: \\(\\frac{Bayes \\ factor \\ for \\ model \\ with \\ interaction}{Bayes \\ factor \\ for \\ model \\ without \\ interaction}\\) this will tell us how many times more likely the model with the interaction is than the model without the interaction. In other words, dividing the BF for the model with the interaction by the BF for the model without the interaction will tell us whether there's evidence for an interaction between the predictors. # compare BFs of models # to determine evidence for the interaction BF_with_interaction / BF_without_interaction ## Bayes factor analysis ## -------------- ## [1] attitude + group + attitude * group : 330.7947 ±6.69% ## ## Against denominator: ## trustworthiness ~ attitude + group ## --- ## Bayes factor type: BFlinearModel, JZS The Bayes factor for the comparison of the model with and without the interaction is approximately 300. (Yours may not be exactly equal to 300 because of the error associated with the generation of the Bayes factor; it should be roughly the same though!) Exercise 4.4 Assessing the interaction Adjusted R2: The adjusted R2 for the model without the interaction (that you noted earlier) was: The adjusted R2 for the model with the interaction (that you also noted earlier) was: What is the increase in adjusted R2 as a result of the addition of the interaction term to the model? Hint. Work out the difference between the two adjusted R2 values you noted above calculatoR Did you know that R can function like a calculator too? Simply type the formula next to &gt; in the console window and hit enter, e.g., &gt; 2 + 2 Or use code in your script: # work out difference in R-squared 0.05 - 0.01 Bayes factors According to comparison of BFs for the model with and without an interaction term included, which statement is true? There's substantial evidence for an absence of an interaction between attitude and group The model with the interaction is as likely as the model without the interaction, given the data There's substantial evidence for an interaction between attitude and group Explanation As a result of adding in the interaction term to the model, the adjusted R2 value increases by approximately 0.04 (i.e., from 0.01 to 0.05). The comparison of BFs for the model with and without the interaction term indicates that there's substantial evidence for the interaction between attitude and group - it's around 300 times more likely that there is an interaction than there isn't one, given the data. Thus, as indicated in the scatterplot, there's evidence that the association between trustworthiness and attitude is different in each group. Specifically, the association is positive in the affected group, whereas it appears to be negative in the not_affected group. 4.6 Simple slopes analysis Given evidence for the interaction, we can conduct follow-up analyses to further characterise it. In a simple slopes analysis the relationship between the outcome and first predictor is examined at each level of the second predictor. Another way of thinking about the interaction is that it implies that the slopes of the lines for the affected group (red line in the scatterplot) and not_affected group (blue line) are not the same. We'll conduct a simple slopes analysis by conducting two simple regressions. The first will be the regression of trustworthiness on the basis of attitude in the affected group. The second will be the regression of trustworthiness on the basis of attitude in the not_affected group. The simplest way to do this is to store the data for each group separately, then perform a simple regression with each dataset. First, filter trust_data for each group: # Filter the dataset for when group is equal to &quot;affected&quot; # store in affected_data affected_data &lt;- trust_data %&gt;% filter(group == &quot;affected&quot;) # Filter the dataset for when group is equal to &quot;not_affected&quot; # store in not_affected_data not_affected_data &lt;- trust_data %&gt;% filter(group == &quot;not_affected&quot;) Now run a simple regression of trustworthiness ~ attitude in each group. First, do the affected group: # affected group: simple regression coefficients lm(trustworthiness ~ attitude, data = affected_data) # affected group: BF lmBF(trustworthiness ~ attitude, data = data.frame(affected_data)) ## ## Call: ## lm(formula = trustworthiness ~ attitude, data = affected_data) ## ## Coefficients: ## (Intercept) attitude ## 3.9631 0.1839 ## ## Bayes factor analysis ## -------------- ## [1] attitude : 1483.223 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS Exercise 4.5 For the affected group (the red line in the scatter plot) The intercept of the regression line is = The slope of the regression line is = The BF for the model (to two decimal places) is This is inconclusivesubstantial evidence for a negativean absence of ana positive association between trustworthiness and attitude in the affected group. In this group, participants with more positive attitudes to the research topic perceived the researcher to be lessmore credible. For the not_affected group: # not_affected group: simple regression coefficients lm(trustworthiness ~ attitude, data = not_affected_data) # not_affected group BF lmBF(trustworthiness ~ attitude, data = data.frame(not_affected_data)) ## ## Call: ## lm(formula = trustworthiness ~ attitude, data = not_affected_data) ## ## Coefficients: ## (Intercept) attitude ## 5.09157 -0.09505 ## ## Bayes factor analysis ## -------------- ## [1] attitude : 0.5874073 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS Exercise 4.6 For the not_affected group (the blue line in the scatterplot) The intercept of the regression line is = The slope of the regression line is = The BF for the model is Although the scatterplot suggests a negativean absence of ana positive association between trustworthiness and attitude in the not_affected group, there was insufficient evidence for this association because the Bayes factor was inconclusivesubstantial In sum, this analysis has shown that when participants hold a more favourable attitude towards a research topic, they perceived researchers who were personally affected by their own research as being more trustworthy. Although the association appeared to be reversed for researchers who were not personally affected by their research, a Bayes factor analysis indicated that there was insufficient evidence of an association between trustworthiness and attitude in this group. 4.7 Exercises Exercise 4.7 Credibility In addition to asking about trustworthiness, Altenmuller et al. (2021) also asked participants how credible they found the researcher. The scores are stored in credibility in the trust_data; higher scores indicate greater perceived credibility. As with trustworthiness, the authors looked at whether attitude and group predicted credibility. Adapt the code in this worksheet to do the following: Create a scatterplot with credibility on the y-axis, attitude on the x-axis, and group as separate lines. Hint Pipe the data to ggplot() and use colour = group in aes Solution trust_data %&gt;% ggplot(aes(x = attitude, y = credibility, colour = group )) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_light() The slope for the affected group appears to be close to zeropositivenegative The slope for the not_affected group appears to be close to zeroweakly positivenegative 2. Obtain adjusted R2 and the BF for the model without an interaction Adjusted R2 (as a proportion, to 2 decimal places) for the model without an interaction is: The BF for the model without an interaction is Hint Specify the model with predictor1 + predictor2 with lm(), pass to glance(), then use lmBF() to get the BF. Solution # Specify the model without an interaction credibility_no_interaction &lt;- lm(credibility ~ attitude + group, data = trust_data) # library(broom) glance(credibility_no_interaction) # BF model BF_credibility_no_interaction &lt;- lmBF(credibility ~ attitude + group, data = data.frame(trust_data)) 3. Obtain adjusted R2 and the BF for the model with an interaction Adjusted R2 (as a proportion, to 2 decimal places) for the model with an interaction is: The BF for the model with an interaction is x 1012 Hint Specify the model with predictor1 + predictor2 + predictor1*predictor2 with lm(), pass to glance(), then use lmBF() to get the BF. Solution # Specify the model with an interaction credibility_with_interaction &lt;- lm(credibility ~ attitude + group + attitude*group, data = trust_data) # library(broom) glance(credibility_with_interaction) # BF model BF_credibility_with_interaction &lt;- lmBF(credibility ~ attitude + group + attitude*group, data = data.frame(trust_data)) What does x 10^12 mean? It means a very large number! For more information see: FAQ (Opens a new tab.) Please take care to notice \"e+\" or \"e-\" in the output for your Bayes factors. As you can see, BF10 = 1.82 means something drastically different to BF10 = 1.82 x 1027, and you wouldn't want to make this kind of mistake when drawing statistical inferences! 4. Compare the models with and without the interaction The increase in adjusted R2 as a result of including the interaction term in the model is (as a proportion to 2 decimal places) = The Bayes factor for the comparison of the models with and without the interaction = Is there substantial evidence for an interaction between attitude and group in the prediction of percieved credibility of the researcher? noyes Hint Work out the difference in adjusted R2 in the model with and without the interaction. Use BF_model_with_interaction / BF_model_without_interaction If the BF &gt; 3, then by convention we say there's substantial evidence for the interaction. Solution # Adj R-square difference 0.21 - 0.15 # Compare BFs BF_credibility_with_interaction / BF_credibility_no_interaction 5. Simple slopes analysis For the affected group: The intercept of the regression line is = The slope of the regression line is = The BF for the model is x 1015 This is inconclusivesubstantial evidence for a negativean absence of ana positive association between credibility and attitude in the affected group. In this group, participants with more positive attitudes to the research topic perceived the researcher to be lessbe more credible. For the not_affected group: The intercept of the regression line is = The slope of the regression line is = The BF for the model is This is inconclusivesubstantial evidence for a negativean absence of ana positive association between credibility and attitude in the not_affected group. Hint Use filter() to separate out the groups of each dataset. Conduct one simple regression for the affected group. Conduct one simple regression for the not_affected group. Code # Filter the dataset for when group is equal to &quot;affected&quot; affected_data &lt;- trust_data %&gt;% filter(group == &quot;affected&quot;) # Filter the dataset for when group is equal to &quot;not_affected&quot; not_affected_data &lt;- trust_data %&gt;% filter(group == &quot;not_affected&quot;) # affected group coefficients lm(credibility ~ attitude, data = affected_data) # affected group BF lmBF(credibility ~ attitude, data = data.frame(affected_data)) # not_affected group coefficients lm(credibility ~ attitude, data = not_affected_data) # not_affected group BF lmBF(credibility ~ attitude, data = data.frame(not_affected_data)) Exercise 4.8 Critical evaluation of the field Altenmuller et al. (2021) also asked participants to report how critical they were in their evaluation of the entire research field. The scores are stored in evaluation in the trust_data; higher scores indicate that the participant evaluated the field more critically. As with the other outcome variables we've considered, the authors looked at whether attitude and group predicted evaluation. Adapt the code in this worksheet to do the following: Create a scatterplot with evaluation on the y-axis, attitude on the x-axis, and group as separate lines. Hint Pipe the data to ggplot() and use colour = group in aes() Solution trust_data %&gt;% ggplot(aes(x = attitude, y = evaluation, colour = group )) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_light() The slope for the affected group appears to be close to zeropositivenegative The slope for the not_affected group appears to be close to zeropositivenegative 2. Obtain adjusted R2 and the BF for the model without an interaction Adjusted R2 (as a proportion, to 2 decimal places) for the model without an interaction is: The BF for the model without an interaction is x 1013 Hint Specify the model with predictor1 + predictor2 with lm(), pass to glance(), then use lmBF() to get the BF. Solution # Specify the model without an interaction evaluation_no_interaction &lt;- lm(evaluation ~ attitude + group, data = trust_data) # library(broom) glance(evaluation_no_interaction) # BF model BF_evaluation_no_interaction &lt;- lmBF(evaluation ~ attitude + group, data = data.frame(trust_data)) 3. Obtain adjusted R2 and the BF for the model with an interaction Adjusted R2 (as a proportion, to 2 decimal places) for the model with an interaction is: The BF for the model with an interaction is x 1018 Hint Specify the model with predictor1 + predictor2 + predictor1*predictor2 with lm(), pass to glance(), then use lmBF() to get the BF. Solution # Specify the model with an interaction evaluation_with_interaction &lt;- lm(evaluation ~ attitude + group + attitude*group, data = trust_data) # library(broom) glance(evaluation_with_interaction) # BF model BF_evaluation_with_interaction &lt;- lmBF(evaluation ~ attitude + group + attitude*group, data = data.frame(trust_data)) 4. Compare the model with and without the interaction The increase in adjusted R2 as a result of including the interaction in the model is (as a proportion to 2 decimal places) = The Bayes factor for the model with the interaction vs. without = Is there evidence for an interaction between attitude and group? noyes Hint Work out the difference in adjusted R2 in the model with and without the interaction. Use BF_more_complex_model / BF_simpler_model If the BF &gt; 3, then there's substantial evidence for the interaction. Solution # Adj R-square difference 0.27 - 0.20 # Compare BFs BF_evaluation_with_interaction / BF_evaluation_no_interaction 5. Simple slopes analysis For the affected group: The intercept of the regression line is = The slope of the regression line is = The BF for the model is x 1020 This is inconclusivesubstantial evidence for a negativean absence of ana positive association between evaluation and attitude in the affected group. In this group, participants with more positive attitudes to the research topic evaluated the research field lessmore critically For the not_affected group: The intercept of the regression line is = The slope of the regression line is = The BF for the model is This is substantialinsufficient evidence for an association between evaluation and attitude in the not_affected group. Hint Use filter() to separate out the groups of each dataset. Conduct one simple regression for the affected group. Conduct one simple regression for the not_affected group. Code # Filter the dataset for when group is equal to &quot;affected&quot; affected_data &lt;- trust_data %&gt;% filter(group == &quot;affected&quot;) # Filter the dataset for when group is equal to &quot;not_affected&quot; not_affected_data &lt;- trust_data %&gt;% filter(group == &quot;not_affected&quot;) # affected group lm(evaluation ~ attitude, data = affected_data) # affected group BF lmBF(evaluation ~ attitude, data = data.frame(affected_data)) # not_affected group lm(evaluation ~ attitude, data = not_affected_data) # not affected group BF lmBF(evaluation ~ attitude, data = data.frame(not_affected_data)) 4.8 Further exercise Exercise 4.9 No interaction Using the data from Teychenne and Hinkley (2016) that we used in Session 1, determine whether there is evidence for an interaction between anxiety_score and level of education in the prediction of screen_time. education is made up of groups 'No uni degree' and 'University degree'. The data are located at: https://raw.githubusercontent.com/chrisjberry/Teaching/master/1_mental_health_data.csv The increase in adjusted R2 associated with the addition of the interaction to the model is (as a proportion) The Bayes factor comparing the model with and without the interaction is (to two decimal places) There's substantial evidence that the relationship between anxiety_score and screen_time is the samedifferent in those who have a degree and those who don't. Given the lack of evidence for an interaction, in an additive model containing only anxiety_score and education as predictors of screen_time: Higher anxiety scores tended to be associated with fewerno change ingreater hours of screen time, but the Bayes factor indicated that there was insufficient evidence for this association, BF = . Individuals without a university degree tended to spend a similar amount of timegreater amounts of timelower amounts of time using screens (e.g., devices, TV, computer) each week than those with a university degree, BF = Solution code # read the data to R using read_csv() mentalh &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/1_mental_health_data.csv&#39;) # scatterplot mentalh %&gt;% ggplot(aes(x = anxiety_score, y = screen_time, colour = education )) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_light() # Specify the model without an interaction screen_time_no_interaction &lt;- lm(screen_time ~ anxiety_score + education, data = mentalh) # in library(broom) glance(screen_time_no_interaction) # adj R² = 0.06 # BF model BF_screen_time_no_interaction &lt;- lmBF(screen_time ~ anxiety_score + education, data = data.frame(mentalh)) # Specify the model with an interaction screen_time_with_interaction &lt;- lm(screen_time ~ anxiety_score + education + anxiety_score*education, data = mentalh) # library(broom) glance(screen_time_with_interaction) # adj R² = 0.06 # BF model BF_screen_time_with_interaction &lt;- lmBF(screen_time ~ anxiety_score + education + anxiety_score*education, data = data.frame(mentalh)) # Adj R-square difference 0.06 - 0.06 # Compare BFs BF_screen_time_with_interaction / BF_screen_time_no_interaction # There&#39;s no evidence for the interaction, # therefore assume additive model # # BF of unique contribution of anxiety_score in model with education # This is the BF for model with both anxiety AND education, # divided by the BF for the model where anxiety is left out BF_screen_time_no_interaction / lmBF(screen_time ~ education, data = data.frame(mentalh)) # BF of unique contribution of education in model with anxiety_score # This is the BF for model with both anxiety AND education, # divided by the BF for the model where education is left out BF_screen_time_no_interaction / lmBF(screen_time ~ anxiety_score, data = data.frame(mentalh)) Storing R2 values in variables It's possible to store R2 values in variables. This can be handy if referring to them again in calculations, or if you want greater precision. # Store adj r-sq for model with no interaction rsq_no &lt;- glance(screen_time_no_interaction)$adj.r.squared # look at rsq rsq_no # Store adj r-sq for model with interaction rsq_with &lt;- glance(screen_time_with_interaction)$adj.r.squared # look at rsq rsq_with # work out change in adj R^2^ as a result of adding the interaction rsq_with - rsq_no Interestingly the change in adjusted R2 is negative here. Remember that this is because the calculation of adjusted R2 adjusts R2 downwards, taking into account the additional predictor in the model. Performing the calculation with the non-adjusted R2 values returns a (very small) positive number. 4.9 Further knowledge 4.9.1 Moderation Moderation The analysis that we've performed with attitude and group in this session is sometimes referred to as a test of moderation (Baron &amp; Kenny, 1986). Moderation is when the relationship between two variables changes as a function of a third variable. Altenmuller et al. (2021) concluded that participants' attitude moderated \"the effect of a researcher disclosing being personally affected (vs. not affected ) by their own research on participants' trustworthiness ascriptions regarding the research\". That is, attitude moderates the effect of group on trustworthiness. It could also be said that the effect of a researcher saying that they are personally affected by their own research moderates the relationship between attitude and trustworthiness (or credibility). That is, the group moderates the relationship between attitude and trustworthiness. A relationship is present when the researcher says they are affected, but absent when they say they are not affected. Which variable is said to be the moderator variable appears to be down to the choice of the researcher and the context of the research. If we don't find evidence for the interaction term, then we don't have evidence that one variable moderates the relationship between the other two. Instead, researchers may say that there is an additive effect of both predictors on the outcome variable. The absence of an interaction indicates that the regression lines for each group do not differ (i.e., the lines for each group in a scatterplot are statistically parallel to one another). 4.9.2 Coefficients Only for those wanting a deeper understanding. Coefficients If we obtain the coefficients for the full regression model (with the interaction), we can write out the regression equation: lm(trustworthiness ~ attitude + group + attitude*group, data = trust_data) \\(Predicted\\ outcome = a + b_1X_1 + b_2X_2 + b_3X_1X_2\\) So, \\(Predicted\\ trustworthiness = 3.96 + 0.18(attitude) + 1.13(group) - 0.28(attitude\\times group)\\) From this equation, we can derive the simple regression equations for each group. As we saw in the previous session, behind the scenes, R uses dummy coding to code the two levels of the categorical variable (i.e., coding levels of a categorical variable with 0s and 1s). It uses 0s to code the affected group and 1s to code the not_affected group. It does it this way because it assigns 0s and 1s alphabetically. Thus, taking the regression equation and substituting group = 0 for the affected group: \\[ \\begin{align} Predicted\\ trustworthiness &amp;= 3.96 + 0.18(attitude) + (1.13\\times0) - 0.28(attitude\\times0)\\\\ &amp;= 3.96 + 0.18(attitude) + 0 - 0 \\\\ &amp;= 3.96 + 0.18(attitude) \\end{align} \\] The intercept (3.96) and slope (0.18) in this simple regression equation match those obtained for the affected group in the earlier simple slopes analysis. Next, taking the full regression equation and substituting group = 1 for the not_affected group: \\[ \\begin{align} Predicted\\ trustworthiness &amp;= 3.96 + 0.18(attitude) + (1.13\\times1) - 0.28(attitude\\times1)\\\\ &amp;= 3.96 + 0.18(attitude) + 1.13 - 0.28(attitude) \\\\ &amp;= 5.09 + 0.18(attitude) - 0.28(attitude) \\\\ &amp;= 5.09 -0.10(attitude) \\end{align} \\] The intercept (5.09) and slope (-0.10) in this simple regression correspond to those we obtained for the not_affected group in the earlier simple slopes analysis. In sum, when one of the predictors is dichotomous, it is possible to derive the simple regression equation for each level of that predictor from the regression equation for the model with the interaction term included. 4.9.3 Centering Again, only for those wanting deeper knowledge. Centering When testing for moderation effects, it is common to center the predictor variables prior to the analysis. (Indeed, Altenmuller et al. (2021) centered attitude prior to running their analyses.) Centering is where you subtract the mean of a variable from every score of that variable. For example, to center the attitude scores, we'd obtain the mean of attitude, and then subtract that value from each of our attitude scores. Thus, a participant with a score of 0 on the centered attitude score would therefore have an attitude value that is equal to the mean. Centering is usually performed to help increase the interpretability of the coefficients of the predictor variables in a model with an interaction. It can also help to reduce the chances of multicollinearity between the predictor variables and the interaction term. Multicollinearity can occur because the interaction term is derived from the individual predictors themselves (by multiplying them together). scale() can be used to center variables automatically. Set the option center = TRUE to center the variable. Setting the option scale = TRUE would also standardise the variable (i.e., divide each score by the standard deviation, to create z-scores), so scale = FALSE means that we won't also standardise the scores: # center the attitude scores # use mutate() to create a new variable in trust_data # called attitude_centered trust_data &lt;- trust_data %&gt;% mutate( attitude_centered = scale(attitude, center = TRUE, scale = FALSE)[,1] ) # compare the means to check that centered scores have mean of 0 trust_data %&gt;% summarise(mean(attitude), mean(attitude_centered)) Before centering, the mean of the attitude scores was 4.26. After centering, the mean (of attitude_centered) is 0, or close enough to zero, being 4.08 x 10-16, more precisely because there's some rounding error along the way. Now re-run the analysis using attitude_centered in place of attitude and look at adjusted R2 and the BF: # full model with attitude_centered full_centered &lt;- lm(trustworthiness ~ attitude_centered + group + attitude_centered*group, data = trust_data) # R-squared glance(full_centered) # BF lmBF(trustworthiness ~ attitude_centered + group + attitude_centered*group, data = data.frame(trust_data)) The adjusted R2 for the full model is , which is the same as we found previously when attitude was not centered. The BF for the model is approximately 33, which is the same as we found previously with the non-centered version of attitude. Thus, centering predictors does not affect the adjusted R2 or evidence for the model. This is because subtracting the mean from a variable is equivalent to a linear transformation and this does not affect the association. To go even further, centering affects the values of the coefficients of the individual predictors in the model, but has no effect on the coefficient for the interaction. When attitude wasn't centered, the regression equation was: \\(Predicted\\ trustworthiness = 3.96 + 0.18(attitude) + 1.13(group) - 0.28(attitude\\times group)\\) Using attitude_centered, the coefficients for the regression equation are obtained from: full_centered And so the regression equation is: \\(Predicted\\ trustworthiness = 4.75 + 0.18(attitude) -0.06(group) - 0.28(attitude\\times group)\\) Notice that the intercept has changed and so has the coefficient for group. As before, to obtain the simple regression equations for each group we can once again substitute in the dummy codes for group. Thus, for the affected group: \\[ \\begin{align} Predicted\\ trustworthiness &amp;= 4.75 + 0.18(attitude) - (0.06\\times0) - 0.28(attitude\\times0)\\\\ &amp;= 4.75 + 0.18(attitude) + 0 - 0 \\\\ &amp;= 4.75 + 0.18(attitude) \\end{align} \\] For the not_affected group: \\[ \\begin{align} Predicted\\ trustworthiness &amp;= 4.75 + 0.18(attitude) - (0.06\\times1) - 0.28(attitude\\times0)\\\\ &amp;= 4.75 + 0.18(attitude) -0.06 - 0.28(attitude) \\\\ &amp;= 4.69 + 0.18(attitude) - 0.28(attitude) \\\\ &amp;= 4.69 -0.10(attitude) \\end{align} \\] The values in the simple regression equations come out slightly differently when attitude is centered, but the direction on the sign of the coefficient for attitude is the same. In sum, centering of predictor variables is often performed in moderation analyses, but does not affect the variance explained by the model, nor the evidence (i.e., the BF) for the model, nor the direction (positive/negative) of the associations in the simple slopes analyses (because it doesn't change the slope). Centering can be desirable to aid interpretation of the coefficients of individual predictors and can reduce multicollinearity. 4.10 Summary A multiple regression model can consist of a mixture of continuous and categorical predictors, Predictors may have a combined effect in explaining the outcome variable. Evidence for this interaction effect can be examined by adding the interaction term to the model, i.e., with lm(outcome ~ predictor1 + predictor2 + predictor1*predictor2) With a dichotomous categorical variable, the interaction implies that the slopes of the simple regressions of outcome ~ continuous_predictor are different. This can be examined with a simple slopes analysis. The interaction implies that the relationship between the outcome and continuous predictor are different in each group. 4.11 References Altenmuller M.S., Lange L.L., Gollwitzer M. (2021). When research is me-search: How researchers’ motivation to pursue a topic affects laypeople’s trust in science. PLoS ONE 16(7): e0253911. https://doi.org/10.1371/journal.pone.0253911 Baron, R. M., &amp; Kenny, D. A. (1986). The moderator–mediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations. Journal of Personality and Social Psychology, 51(6), 1173. https://psycnet.apa.org/doi/10.1037/0022-3514.51.6.1173 "],["multiple3.html", "Session 5 Multiple regression: hierarchical regression 5.1 Overview 5.2 Worked example 1: wellbeing 5.3 Exercise 1 5.4 Worked Exercise 2: Controlling for categorical variables 5.5 Exercise 2 5.6 Further knowledge and exercises 5.7 Summary 5.8 References", " Session 5 Multiple regression: hierarchical regression Chris Berry 2025 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} 5.1 Overview Slides from the lecture part of the session: Download Slides for Rmd support R Studio online Access here using University log-in Hierarchical regression is another form of multiple regression analysis and can be used when we want to add predictor variables to a model in discrete steps or stages. The technique allows the unique contribution of the variables on each step to be separately determined. We can use it when we want to know whether a predictor variable (e.g., sense_of_belonging) predicts an outcome (e.g., social_interaction) after controlling for background variables that are categorical (e.g., gender, level of education) or continuous (e.g., age, score on a cognitive test) in nature. The variables entered on each step can also be determined by theoretical considerations, to test specific hypotheses. At each step in the analysis, the increase in the variance explained in the outcome variable (i.e., R2) and evidence for the unique contribution of the predictors (with Bayes factors) can be assessed. Hierarchical regression is sometimes called sequential regression. 5.2 Worked example 1: wellbeing In Session 2, we analysed some of the data from Iani et al. (2019) and found evidence that brooding and worry predicted wellbeing in a multiple regression. Iani et al. (2019) were actually primarily interested in whether mindfulness and emotional intelligence predicted psychological wellbeing scores, but after controlling for brooding and worry. The reason for asking the question in this way is because it's a well-established finding that brooding and worry explain wellbeing, but less is known about mindfulness and emotional intelligence. To control for brooding and worry, these variables are entered into a regression first. Next, mindfulness and emotional intelligence are added, and the change in R2 associated with their addition to the model can be evaluated. Bayes factors can also be used to assess the unique contribution of the predictors added at each step. 5.2.1 Read in the data Read the data to R, and store in pwb_data (to stand for Psychological WellBeing data). The data we'll use are located at: https://raw.githubusercontent.com/chrisjberry/Teaching/master/2_wellbeing_data.csv # First ensure tidyverse is loaded, i.e., &#39;library(tidyverse)&#39; # read in the data using read_csv(), store in pwb_data pwb_data &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/2_wellbeing_data.csv&#39;) (Note. The data are publicly available, but I've changed the variable names for clarity. As in Iani et al., missing values were replaced with the mean of the relevant variable.) Preview the data with head(): pwb_data %&gt;% head() describing observing acting nonreactivity nonjudging attention clarity repair brooding worry wellbeing gad 13 14 19 6 15 23 20 21 18 19 64 13 15 6 15 13 12 24 20 15 14 30 72 7 14 14 16 17 11 31 27 26 15 30 59 11 10 10 18 18 18 27 18 20 16 29 62 13 10 14 15 14 16 21 24 14 8 27 78 5 21 15 18 14 14 30 26 23 10 23 78 4 About the data: Mindfulness variables: describing: Higher scores indicate greater ability to describe one's inner experiences. observing: Higher scores indicate greater levels of observing. acting: Higher scores indicate greater levels of acting with awareness. nonreactivity: Higher scores indicate greater levels of nonreactivity. nonjudging: Higher scores indicate greater levels of nonjudging. Emotional intelligence variables: attention: Higher scores indicate greater skill in attending to their feelings and moods. clarity: Higher scores indicate greater skill in experiencing their feelings clearly. repair: Higher scores indicate greater skill in regulating unpleasant moods or prolonging pleasant ones. Negative functioning variables: brooding: Higher scores indicate greater levels of brooding. worry: Higher scores indicate greater levels of worry. Outcome variables: wellbeing: Higher scores indicate higher levels of psychological wellbeing in terms of self-acceptance, positive relations with others, autonomy, environmental mastery, purpose in life, and personal growth. gad: higher scores indicate greater severity of Generalised Anxiety Disorder. 5.2.2 Visualisation There are a number of continuous variables in the dataset that we can visualise with density plots or histograms. We've done this individually, variable by variable in past worksheets. Here I'd like to show you a more advanced way of plotting. The code below will create a density plot of every variable in a dataset that is numeric (continuous) in nature, using different facets: # Plot density plots of all the numeric (continuous) variables # The code below: # -Keeps only the numeric columns in a data frame # -Uses pivot_longer() to code each set of scores by its variable name # -Specifies the &#39;score&#39; to plot in aes() # -Uses facet_wrap() to plot each variable in a separate panel # -Uses scales = &quot;free&quot; to allow the range on the x- and y-axis to be different across panels pwb_data %&gt;% keep(is.numeric) %&gt;% pivot_longer(everything(), names_to = &quot;variable&quot;, values_to = &quot;score&quot;) %&gt;% ggplot(aes(score)) + facet_wrap(~ variable, scales = &quot;free&quot;) + geom_density() Figure 2.1: Density plots for each continous predictor in pwb_data 5.2.3 Correlations With many variables being used in a multiple regression, it is good practice to inspect the correlations between all continuous variables first to get an idea of the inter-relations and to check for multicollinearity between predictors. Obtain a correlation matrix of all of the numeric variables. Ensure the corrr package is loaded, then use correlate(): # library(corrr) # ensure this is loaded # correlations of all numeric variables # use mutate() with round() to round to 2 D.P. pwb_data %&gt;% keep(is.numeric) %&gt;% correlate(method = &quot;pearson&quot;) %&gt;% mutate(across(where(is.numeric), round, digits = 2)) Exercise 5.1 Before conducting the hierarchical regression, Iani et al. (2019) reported the Pearson correlations between wellbeing and gad and the mindfulness and emotional intelligence variables. We'll report a subset of those here to check you can read the correlation matrix that's output. Report the correlations below to two decimal places: describing and clarity, r = describing and wellbeing, r = repair and wellbeing, r = nonreactivity and brooding, r = nonreactivity and gad, r = Does multicollinearity seem an issue (check the correlations between the predictor variables for r &lt; -0.8 or r &gt; 0.8)? yesno 5.2.4 Hierarchical regression To conduct the hierarchical regression, variables will be entered to the multiple regression in progressive steps, and the change in R2 associated with the predictors added to the model at each step obtained. Likewise, Bayes factors can be used to determine whether there is evidence that the predictors added at each step make a unique contribution to the prediction of the outcome variable. Iani et al. (2019) wanted to explain variance in wellbeing (the outcome variable). They reported the non-adjusted R2, so that's what we'll do too. Due to theoretical considerations, the variables in each step were entered as follows: Step 1: brooding Step 2: brooding, worry Step 3: brooding, worry and mindfulness variables (describing, observing, acting, nonreactivity, nonjudging) Step 4: brooding, worry, mindfulness variables (describing, observing, acting, nonreactivity, nonjudging), and emotional intelligence variables (attention, clarity, repair). 5.2.4.1 Step 1: brooding First, use brooding to predict wellbeing. Use lm() and glance() to obtain the R2 for the model, then use lmBF() to obtain the BF for the model. # specify the model in the step step1 &lt;- lm(wellbeing ~ brooding, data = pwb_data) # R^2^ # ensure the broom package loaded, i.e., with &#39;library(broom)&#39; glance(step1) # store the BF # ensure the BayesFactor package is loaded, i.e., with &#39;library(BayesFactor) BF_step1 &lt;- lmBF(wellbeing ~ brooding, data = data.frame(pwb_data)) # look at BF BF_step1 r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.189052 0.1763809 11.4776 14.91998 0.0002642 1 -253.7007 513.4014 519.9704 8431.064 64 66 ## Bayes factor analysis ## -------------- ## [1] brooding : 95.76111 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS The R2 (non-adjusted, as a proportion, to 2 decimal places) for the model in Step 1 = . The BF for the model in Step 1 = 5.2.4.2 Step 2: brooding + worry Next, add worry to the model in Step 1, using + worry and look at R2 again and obtain the BF. We'll look at: whether the model in Step 2 explains more variance in wellbeing by looking at the amount that R2 increases. whether there's evidence for a contribution of worry after controlling for brooding, by dividing the Bayes factor for the model in Step 2 by the BF for the model in Step 1 # specify the model in the step step2 &lt;- lm(wellbeing ~ brooding + worry, data = pwb_data) # R-sq glance(step2) # store the BF BF_step2 &lt;- lmBF(wellbeing ~ brooding + worry, data = data.frame(pwb_data)) # compare the BFs for the models in Step 2 and Step 1 BF_step2 / BF_step1 r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.3288693 0.3075636 10.52393 15.43572 3.5e-06 2 -247.4558 502.9116 511.6702 6977.446 63 66 ## Bayes factor analysis ## -------------- ## [1] brooding + worry : 56.11175 ±0% ## ## Against denominator: ## wellbeing ~ brooding ## --- ## Bayes factor type: BFlinearModel, JZS The R2 for the model in Step 2 is The increase in R2 associated with the addition of worry to the model is Hint. To calculate this, take the R2 that you recorded before for the model in Step 2 and subtract the R2 for the model in Step 1. The BF for the contribution of worry to the model is . Hint. This is the BF produced by dividing the BF for the model in Step 2, by the BF for the model in Step 1. calculatoR Did you know that R can function like a calculator too? Simply type the formula next to &gt; in the console window and hit enter, e.g., &gt; 2 + 2 Or use code in your script: 0.33 - 0.19 Change in R-squared symbol In reports and articles, you will often see the change in R2 written as \\(\\Delta R^2\\). The \\(\\Delta\\) symbol means \"change\". For example \\(\\Delta R^2 = 0.33\\). 5.2.4.3 Step 3: brooding + worry + mindfulness variables Next, add the variables associated with mindfulness to the model. The mindfulness measures are observing, describing, acting, nonjudging, and nonreactivity. Add these five variables to the model all in the same step. As before, note R2 for the model and the BF. To determine whether there's evidence for the addition of the mindfulness variables, compare the BF for the model in Step 3 and the BF of the model in Step 2. # specify the model in the step step3 &lt;- lm(wellbeing ~ brooding + worry + observing + describing + acting + nonjudging + nonreactivity, data = pwb_data) # R-sq glance(step3) # store the BF BF_step3 &lt;- lmBF(wellbeing ~ brooding + worry + observing + describing + acting + nonjudging + nonreactivity, data = data.frame(pwb_data)) # compare the BFs for the models in step 3 and step 2 BF_step3 / BF_step2 r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.5330601 0.4767053 9.148738 9.458999 1e-07 7 -235.4846 488.9692 508.6761 4854.566 58 66 ## Bayes factor analysis ## -------------- ## [1] brooding + worry + observing + describing + acting + nonjudging + nonreactivity : 35.49308 ±0% ## ## Against denominator: ## wellbeing ~ brooding + worry ## --- ## Bayes factor type: BFlinearModel, JZS The R2 for the model in Step 3 is The increase in R2 associated with the addition of the mindfulness variables is Hint. Take the R2 for the model in Step 3 and subtract the R2 for the model in Step 2. The BF for the contribution of the mindfulness variables to the model is Hint. This is the BF produced by dividing BF_step3 by BF_step2. After controlling for brooding and worry, is there sufficient evidence for the contribution of mindfulness to the prediction of wellbeing? yes, BF&gt;3no Explain The BF comparing the models in Steps 3 and 2 was BF = 34.59, indicating that the model in Step 3 is more than thirty five times more likely than the model in Step 2, given the data. Thus, there's substantial evidence that the mindfulness variables contribute to the prediction of wellbeing after controlling for brooding and worry. The mindfulness variables explain an additional R2 = 0.20, or 20% of the variance in wellbeing, over and above brooding and worry. 5.2.4.4 Step 4: brooding + worry + mindfulness + emotional intelligence variables Next, add the variables associated with emotional intelligence to the model. The emotional intelligence variables are attention, clarity, and repair. These three variables are added to the model all in the same step. Once again, note R2 for the model and the BF. To determine whether there's evidence for the addition of the emotional intelligence variables, compare the BF for the model in Step 4 and the BF of the model in Step 3. # specify the model in step 4 step4 &lt;- lm(wellbeing ~ brooding + worry + observing + describing + acting + nonjudging + nonreactivity + attention + clarity + repair, data = pwb_data) # R-sq glance(step4) # store the BF BF_step4 &lt;- lmBF(wellbeing ~ brooding + worry + observing + describing + acting + nonjudging + nonreactivity + attention + clarity + repair, data = data.frame(pwb_data)) # compare the BFs for the models in step 4 and step 3 BF_step4 / BF_step3 r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.6045859 0.5326924 8.645487 8.409467 0 10 -229.9978 483.9956 510.2715 4110.944 55 66 ## Bayes factor analysis ## -------------- ## [1] brooding + worry + observing + describing + acting + nonjudging + nonreactivity + attention + clarity + repair : 2.150042 ±0% ## ## Against denominator: ## wellbeing ~ brooding + worry + observing + describing + acting + nonjudging + nonreactivity ## --- ## Bayes factor type: BFlinearModel, JZS The R2 for the model in Step 4 is The increase in R2 associated with the addition of the emotional intelligence variables is Hint. Take the R2 for the model in Step 4 and subtract the R2 for the model in Step 3. The BF representing the evidence for the unique contribution of the emotional intelligence variables to the model is After controlling for brooding, worry, and mindfulness, is there sufficient evidence for the contribution of emotional intelligence to the prediction of wellbeing? yes, BF &gt; 3no, BF &lt; 3 Explain The BF comparing the models in steps 4 and 3 was BF = 2.15. Although this indicates that the model in Step 4 is more than twice as likely than the model in Step 3, given the data, the BF is less than 3, and therefore falls short of the conventional level for declaring that there's substantial evidence for the addition of emotional intelligence. Thus, according to this Bayes factor analysis, there's insufficient evidence that the additional R2 = 0.07, or 7%, of the variance in wellbeing explained by emotional intelligence represents a genuine improvement in prediction. 5.3 Exercise 1 Exercise 5.2 Hierarchical regression Iani et al. (2019) were also interested in whether whether mindfulness and emotional intelligence predicted gad (anxiety symptoms) after controlling for brooding and worry. Repeat the analysis conducted above, but now with gad as the outcome variable. Step 1: brooding Step 2: brooding, worry Step 3: brooding, worry and mindfulness variables (describing, observing, acting, nonreactivity, nonjudging) Step 4: brooding, worry, mindfulness variables (describing, observing, acting, nonreactivity, nonjudging), and emotional intelligence variables (attention, clarity, repair). Step 1: brooding The R2 (non-adjusted, to two decimal places) for the model in Step 1 = The BF for the model in Step 1 = Hint Rounding can sometimes be unclear when the last digit ends in a 5, and you are unsure of the precision to which something has been printed in the output R gives. If in doubt, it's better to round from the more precise value. For R2, this can be obtained as follows from the glance function: # show R-squared to more decimal places, # by calling value from function specifically glance(gad1)$r.squared Solution - code # specify the model in step 1, store in gad1 gad1 &lt;- lm(gad ~ brooding, data = pwb_data) # R-sq glance(gad1) # store BF BF_gad1 &lt;- lmBF(gad ~ brooding, data = data.frame(pwb_data)) # show BF BF_gad1 Step 2: brooding + worry The R2 for the model in Step 2 is The increase in R2 associated with the addition of worry is The BF for the contribution of worry to the model is Hint Next, add worry to the model, using + worry. Look at R2 again and obtain the BF. To determine the R2 change for the model, take the R2 for the model in Step 2 and subtract the R2 value for the model in Step 1. To determine whether there's evidence for a contribution of worry after controlling for brooding, divide the Bayes factor for the model by the BF for the model in Step 1. Solution - code # specify the model in Step 2 gad2 &lt;- lm(gad ~ brooding + worry, data = pwb_data) # R sq glance(gad2) # store the BF BF_gad2 &lt;- lmBF(gad ~ brooding + worry, data = data.frame(pwb_data)) # compare the BFs for the models in Step 2 and Step 1 BF_gad2 / BF_gad1 Step 3: brooding + worry + mindfulness variables The R2 for the model in Step 3 is The increase in R2 associated with the addition of the mindfulness variables is The BF for the contribution of the mindfulness variables to the model is After controlling for brooding and worry, is there sufficient evidence for the contribution of mindfulness to the prediction of gad? yesno There's substantialinconclusive evidence for the model in Step 2, compared to Step 3, because the Bayes factor for the model in Step 3 divided by that of the model in Step 2 is less than 0.33equal to 1greater than 3. Hint Add the variables associated with mindfulness to the model. The mindfulness measures are observing, describing, acting, nonjudging, and nonreactivity. These five variables are added to the model all in the same step. To calculate the increase in R2, take the R2 for the model in Step 3 and subtract the R2 for the model in Step 2. To obtain the BF for the contribution of the mindfulness variables, take the BF for the model in Step 3 and divide it by the BF for the model in Step 2. If the BF is greater than 3, there's substantial evidence for the model in Step 3. If the BF &lt; 0.33, then there's substantial evidence for the model in Step 2. Intermediate BFs are inconclusive. Solution - code # specify the model in Step 3 gad3 &lt;- lm(gad ~ brooding + worry + observing + describing + acting + nonjudging + nonreactivity, data = pwb_data) # R-sq glance(gad3) # store the BF BF_gad3 &lt;- lmBF(gad ~ brooding + worry + observing + describing + acting + nonjudging + nonreactivity, data = data.frame(pwb_data)) # compare the BFs for the models in Step 3 and Step 2 BF_gad3 / BF_gad2 Step 4: brooding + worry + mindfulness + emotional intelligence variables The R2 for the model in Step 4 is The increase in R2 associated with the addition of the emotional intelligence variables to the model is . The BF associated with the contribution of the emotional intelligence variables to the model is After controlling for brooding, worry, and mindfulness, is there sufficient evidence for the contribution of emotional intelligence to the prediction of gad? yesno Hint Add the variables associated with emotional intelligence to the model. The emotional intelligence measures are attention, clarity, and repair. These three variables are added to the model all in the same step. To calculate the increase in R2, take the R2 for the model in Step 4 and subtract the R2 for the model in Step 3. To obtain the BF for the contribution of the emotional intelligence variables, take the BF for the model in Step 4 and divide it by the BF for the model in Step 3. If the BF is greater than 3, there's substantial evidence for the model in Step 4. If the BF &lt; 0.33, then there's substantial evidence for the model in Step 3. Intermediate BFs are inconclusive. Solution - code # specify the model in Step 4 gad4 &lt;- lm(gad ~ brooding + worry + observing + describing + acting + nonjudging + nonreactivity + attention + clarity + repair, data = pwb_data) # R-sq glance(gad4) # store the BF BF_gad4 &lt;- lmBF(gad ~ brooding + worry + observing + describing + acting + nonjudging + nonreactivity + attention + clarity + repair, data = data.frame(pwb_data)) # compare the BFs for the models in Step 3 and Step 2 BF_gad4 / BF_gad3 In summary, regarding the hypothesis of Iani et al. (2019): After controlling for brooding and worry, there's evidence for the contribution of mindfulness and emotional intelligence to the prediction of gad. noyes 5.4 Worked Exercise 2: Controlling for categorical variables All of the variables considered in the previous exercise were continuous (or at were least assumed to be). It is also possible to use hierarchical regression to control for the influence of variables that are categorical in nature. How do human-animal relationships affect mental health? Ratschen et al. (2020) looked at the impact of relationships with animals (e.g., pets) on mental health and loneliness in individuals during the Covid-19 pandemic. Data from their study are located at the link below. https://raw.githubusercontent.com/chrisjberry/Teaching/master/5_animal_data.csv More on the data The data are publicly available and variable names have been changed here for clarity. Missing data have been dealt with differently to the way the researchers dealt with them, so the data you are analysing (and therefore the results) are not identical to theirs. About the data: comfort: Comfort from companion animals. Higher scores indicate greater comfort from the companion animal. mental_health_pre: Mental health before lockdown. Higher scores indicate better mental health. mental_health_since: Mental health during lockdown. Higher scores indicate better mental health. wellbeing_since: Higher scores indicate better wellbeing. gender: Male, female, another option, or prefer not to say age: Age group. partner: Whether the participant lives with their partner or not. species: The type of animal companion. loneliness_pre: Loneliness before lockdown. Higher scores indicate greater loneliness. loneliness_since: Loneliness during lockdown. Higher scores indicate greater loneliness. 5.4.1 Visualisation Inspect the distributions of the continuous variables: # Read in the data animal_data &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/5_animal_data.csv&#39;) # Plot density plots of all the numeric (continuous) variables # The code below: # -Keeps only the numeric columns in a data frame # -Uses pivot_longer() to code each set of scores by its variable name # -Specifies the &#39;score&#39; to plot in aes() # -Uses facet_wrap() to plot each variable in a separate panel # -Uses scales = &quot;free&quot; to allow the range on the x- and y-axis to be different across panels animal_data %&gt;% keep(is.numeric) %&gt;% pivot_longer(everything(), names_to = &quot;variable&quot;, values_to = &quot;score&quot;) %&gt;% ggplot(aes(score)) + facet_wrap(~ variable, scales = &quot;free&quot;) + geom_histogram() Figure 5.1: Histogram plots for each continous predictor The code can be modified to obtain histograms of all of the categorical variables: # Plot histograms of all the categorical vars (i.e. count data) # The code: # -Keeps only the character columns in the dataset # -Uses pivot_longer() to code each set of scores by variable # -Specifies the &#39;score&#39; to plot in aes() # -Uses facet_wrap() to plot in separate panels # -Tells geom_histogram() to plot count data animal_data %&gt;% keep(is.character) %&gt;% pivot_longer(everything(), names_to = &quot;variable&quot;, values_to = &quot;score&quot;) %&gt;% ggplot(aes(score)) + facet_wrap(~ variable, scales = &quot;free&quot;) + geom_histogram(stat = &quot;count&quot;) Figure 5.2: Histograms for each categorical predictor By looking at the histograms of the categorical variables, answer the following: Age: The majority of people were in age group 18_2425_3435_4445_5455_6465_7070_over Gender: The majority of people were in another wayfemalemaleprefer not to say Partner: The majority of people were living with partnernot_living_with_partner Species: The most common animal companion was birdcatdogfishhorseotherreptilesmall The scores on the loneliness_pre variable range from 3 to 9, so there are only 7 possible scores. When there are many data points, this can create issues when visualising the variable in a scatterplot because there are only so many combinations of scores for the variable, so they overlap. The code below uses the gridExtra package to display two plots created with ggplot(). Each plot is stored in a separate variable panel1 and panel2, then placed side-by-side using grid.arrange() from the gridExtra package: # load library(gridExtra) # for displaying different plots on multiple panels library(gridExtra) # create a scatterplot of loneliness scores # without random jittering of points # store in panel1 panel1 &lt;- animal_data %&gt;% ggplot(aes(x=loneliness_pre, y = loneliness_since)) + geom_point() + geom_smooth(method=&quot;lm&quot;, se = F) + ggtitle(&quot;Without geom_jitter()&quot;) # create a scatterplot of loneliness scores # with random jittering of points using geom_jitter() # store in panel2 panel2 &lt;- animal_data %&gt;% ggplot(aes(x=loneliness_pre, y = loneliness_since)) + geom_jitter() + geom_smooth(method=&quot;lm&quot;, se = F)+ ggtitle(&quot;With geom_jitter()&quot;) grid.arrange(panel1, panel2, nrow = 1) Figure 2.3: Using geom_jitter(): loneliness_since vs. londliness_pre Using geom_jitter() instead of geom_point() means that the scores will be randomly jittered by a tiny amount. This reduces overlap, making it much easier to see how the scores are distributed. It's useful to use geom_jitter() when the response variable is on an ordinal scale, but the responses are discrete (e.g., 1, 2, 3, 4, 5), as is often the case with survey data and likert scales. Thus, if you ever create a scatterplot of survey data and it ends up looking like the plot on the left, try using geom_jitter() instead of geom_point(). The next step isn't strictly necessary because the categorical variables are stored as character variables (i.e., &lt;chr&gt;), but it is good practice to convert the categorical variables to factors before the analysis. # Convert categorical variables to factors # (remember, age is grouped, so is categorical here) animal_data &lt;- animal_data %&gt;% mutate(gender = factor(gender), age = factor(age), partner = factor(partner), species = factor(species)) 5.5 Exercise 2 Exercise 5.3 Hierarchical regression Using the animal_data, we'll look at whether comfort scores predict mental_health_pre after controlling for gender, age, partner, species and loneliness_pre. How many steps do you think there'll be in the hierarchical regression analyses? 1234567 Explain The variables we want to control for can go into the model in the Step 1. Then we can add comfort to the model in Step 2, to see whether it explains mental_health_pre over and above all the variables in Step 1. How many participants are in this dataset? Hint The number of participants is the same as the number of rows. One way of checking is as follows: animal_data %&gt;% count() The variables that are controlled for in a multiple regression are sometimes called covariates Step 1: Covariates only R2 for the model containing the covariates only is (to three decimal places; this level of precision is required for this answer) The BF for the model containing the covariates only is &lt; 0.33between 0.33 and 3&gt; 10000 Hint Run a regression to predict mental_health_pre from the covariates only. The covariates are the things we want to control for, and are gender, age, partner, species and loneliness_pre. Obtain R2 for the model using glance() in the broom package. Obtain the BF for the model using lmBF() in the BayesFactor package. Solution # store model with covariates only step1_covariates &lt;- lm(mental_health_pre ~ gender + age + partner + species + loneliness_pre, data = animal_data) # R-sq glance(step1_covariates) # Store the BF BF_step1_covariates &lt;- lmBF(mental_health_pre ~ gender + age + partner + species + loneliness_pre, data = data.frame(animal_data)) # View the BF for step 1 BF_step1_covariates Step 2: Covariates plus comfort The R2 for the model with the covariates and the comfort predictor is (report to three decimal places) The increase in R2 as a result of the addition of comfort to the model with the covariates is Hint: subtract the R2 for Step 1 from that of Step 2. It's necessary to use the R2 values to three decimal places because the increase is so small! The Bayes factor for the comparison of the model in Step 2 and Step 1 is &lt; 0.33between 0.33 and 3&gt; 3 Is there sufficient evidence that comfort from animal companions explains mental health levels before lockdown, after controlling for gender, age, partner, species of animal, and loneliness_pre? no, BF &lt; 0.33no, BF between 0.33 and 3yes, BF &gt; 3 Individuals who reported deriving greater comfort from animals also tended to have higherlower levels of mental health, as measured before lockdown. Hint Run a regression to predict mental_health_pre from the covariates and comfort. The covariates are the things we want to control for, and are gender, age, partner, species and loneliness_pre. Obtain R2 for the model using glance() in the broom package. Obtain the BF for the model using lmBF() in the BayesFactor package. Calculate the difference in R2 for the model in Step 2 and the model in Step 1 Divide the BF for the model in Step 2 by the BF for the model in Step 1 to obtain the BF representing the evidence for the contribution of comfort to the model, after controlling for the covariates. To determine whether the association between comfort and mental_health_pre is positive or negative, look at the sign on the coefficient for comfort by using step2_full. Solution - code # covariates + comfort step2_full &lt;- lm(mental_health_pre ~ comfort + gender + age + partner + species + loneliness_pre, data = animal_data) # R-sq glance(step2_full) # store BF BF_step2_full &lt;- lmBF(mental_health_pre ~ comfort + gender + age + partner + species + loneliness_pre, data = data.frame(animal_data)) # evidence for comfort, controlling for covariates BF_step2_full / BF_step1_covariates # look at the sign on the coefficient for comfort step2_full 5.6 Further knowledge and exercises 5.6.1 Standardised Coefficients Standardised coefficients Iani et al. (2019) also reported the values of the coefficients for the predictors at each step (see their Table 2). When a multiple regression has been performed using the raw data, the coefficients given by lm() are unstandardised. This means that they are in the same units as the predictor variable they correspond to. For example, if the coefficient for brooding is -1.57, this means that a 1 unit increase in brooding score is associated with a 1.57 decrease in wellbeing score. We'd like to be able to compare the coefficients of predictors to get some idea of their relative strength of the contribution to the model. The trouble is that predictors are often measured on different scales, with different ranges. For example, scores of brooding range from 5 to 20, and those of clarity range from 10 to 40. Use summary(pwb_data) to see this. Because the scales are so different, it doesn't make sense to directly compare the coefficients of the predictors. To compare the coefficients of predictor variables in a model, we need to compare the standardised regression coefficients. These are the coefficients derived from the data after the scores of each predictor have been standardised. To standardise the scores of a variable, subtract the mean value from each score, and then divide each score by the standard deviation of the scores. The transformed scores will have a mean of zero and standard deviation of 1, and so will all be on the same scale. The scale() function does this automatically for us. To standardise all the numeric variables in the pwb_data: # Store the result in std_pwb_data # Take pwb_data, pipe it to # mutate_if(). # Tell mutate_if() to standardise a column # using &#39;scale&#39; if the variable type &#39;is.numeric&#39; # (note, it&#39;s not possible to standardise non-numeric variables) std_pwb_data &lt;- pwb_data %&gt;% mutate_if(is.numeric, scale) Let's compare the mean and standard deviation before and after standardising the scores: # use summarise() and across() to obtain # the mean and sd of each column # see ?summarise() # see ?across() # before standardising pwb_data %&gt;% summarise(across(.cols = everything(), list(mean = mean, sd = sd))) %&gt;% glimpse() # after standardising std_pwb_data %&gt;% summarise(across(.cols = everything(), list(mean = mean, sd = sd))) %&gt;% glimpse() Now re-run Step 4 (i.e., the final model) of the hierarchical regression in Iani et al. (2019), but with std_pwb_data instead of pwb_data: # run the regression using standardised data std_step4 &lt;- lm(wellbeing ~ brooding + worry + observing + describing + acting + nonjudging + nonreactivity + attention + clarity + repair, data = std_pwb_data) # look at standardised coefficients std_step4 The standardised coefficients are called the beta coefficients, and have the symbol \\(\\beta\\). Beta coefficients range from -1 to +1, so the zero is usually omitted when reporting, e.g., \\(\\beta(brooding) = -.12\\) Make a note of the standardised (beta) coefficients for the model in Iani et al. (2019): brooding = worry = observing = describing = acting = nonjudging = nonreactivity = attention = clarity = repair = As with the unstandardised coefficients, the sign on the beta coefficient indicates the direction of the association with the outcome variable (i.e., positive or negative). Because the beta coefficients are now on the same scale, their magnitudes (i.e., their absolute size, ignoring the sign) can be compared to determine the relative \"importance\" of each predictor. Which predictor variable makes the strongest contribution to the prediction of wellbeing? broodingworryobservingdescribingactingnonjudgingnonreactivityattentionclarityrepair Which predictor variable makes the weakest contribution to the prediction of wellbeing? broodingworryobservingdescribingactingnonjudgingnonreactivityattentionclarityrepair Explain describing has the largest beta coeffcient (.38); it therefore makes the greatest contribution to the prediction of wellbeing in the full model. clarity makes the smallest contribution (beta = .02). Note, some of the beta coefficients differ slightly from those reported by Iani et al. (2019). This is most likely due to differences in rounding introduced during standardisation by different software packages. The values are very close though and the ordinal pattern in the beta coefficients is the same. 5.6.2 Prediction Prediction The final model can be used to predict new data points (as in earlier sessions). For the model with continuous predictors: # specify data for new ppt new_pwb &lt;- tibble( brooding = 6, worry = 12, observing = 12, describing = 18, acting = 17, nonjudging = 20, nonreactivity = 19, attention = 30, clarity = 27, repair = 27 ) # use augment() in broom package. # .fitted = predicted wellbeing value augment(step4, newdata = new_pwb) The predicted value of wellbeing for the new participant in new_pwb is . For the model with categorical predictors: # specify data for new ppt new_animal_dat &lt;- tibble( comfort = 45, gender = &#39;female&#39;, age = &#39;18_24&#39;, partner = &#39;living_partner&#39;, species = &#39;cat&#39;, loneliness_pre = 7) # use augment() in broom package. # .fitted = predicted value augment(step2_full, newdata = new_animal_dat) The predicted value of mental_health_pre for the new participant in new_animal_dat is . 5.6.3 Residuals Residuals As in earlier sessions, the residuals can be inspected in a plot of the predicted values vs. the residuals: # scatterplot of the predicted outcome values vs. residuals augment(step4) %&gt;% ggplot(aes(x=.fitted, y=.resid)) + geom_point()+ geom_hline(yintercept=0) We can also inspect the histogram of the residuals for normality: # histogram of the residuals augment(step4) %&gt;% ggplot(aes(x=.resid)) + geom_histogram() 5.7 Summary Hierarchical regression In hierarchical regression, predictors are added to a regression model in successive steps. It can be used to test particular theories or hypotheses. It can also be used to control the influence of particular variables (e.g., background variables) before analysing whether a predictor variable (or set of variables) of interest explains the outcome variable. As before, lm() and glance() can be used to obtain R2 for the model at each step. The change in R2 associated with each step can be obtained to determine the unique contribution of the predictors in each step. lmBF() can be used to obtain the Bayes factor for the model in each step. Bayes factors of models from successive steps can be compared to determine the evidence for the unique contribution of predictors in a step. Plotting tips: Use geom_jitter() to make scatterplots with overlapping points easier to interpret. Useful for survey data. Use keep() with pivot_longer(), ggplot() and facet_wrap() to plot lots of variables of a certain type (e.g., numeric or character) on separate panels. Use grid.arrange() in the gridExtra package to display figures on separate panels. 5.8 References Iani, L., Quinto, R. M., Lauriola, M., Crosta, M. L., &amp; Pozzi, G. (2019). Psychological well-being and distress in patients with generalized anxiety disorder: The roles of positive and negative functioning. PloS ONE, 14(11), e0225646. https://doi.org/10.1371/journal.pone.0225646 Ratschen E., Shoesmith E., Shahab L., Silva K., Kale D., Toner P., et al. (2020) Human-animal relationships and interactions during the Covid-19 lockdown phase in the UK: Investigating links with mental health and loneliness. PLoS ONE, 15(9):e0239397. https://doi.org/10.1371/journal.pone.0239397 "],["anova2.html", "Session 6 ANOVA: Repeated measures 6.1 Overview 6.2 One-way repeated measures ANOVA 6.3 Long format data 6.4 Worked Example 6.5 Two-way within subjects ANOVA 6.6 Exercise: two-way mixed ANOVA 6.7 Further knowledge - within-subject errorbars 6.8 Further knowledge - R-squared 6.9 Summary 6.10 References", " Session 6 ANOVA: Repeated measures Chris Berry 2025 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} 6.1 Overview Slides from the lecture part of the session: Download R Studio online Access here using University log-in Previously, we saw that when all of the predictor variables in a multiple regression are categorical then the analysis has the name ANOVA (in Session 3). Here we will analyse repeated measures (or within-subjects) designs with ANOVA. In a repeated measures design, the scores for the different levels of an independent variable come from the same participants, rather than separate groups of participants. We consider two types of ANOVA for within-subjects designs: one-way ANOVA and two-way factorial ANOVA. The methods are similar to those in Session 3, where we analysed between-subjects designs with ANOVA. The exercise at the end is for a mixed factorial design, where one of the factors is manipulated between-subjects, and the other is manipulated within-subjects. 6.2 One-way repeated measures ANOVA A one-way repeated measures ANOVA is used to compare the scores from a dependent variable for the same individuals across different time points. For example, do depression scores differ in a group of individuals across three time points: before CBT therapy, during therapy, and 1 year after therapy? One-way within-subjects ANOVA refers to the same analysis. The scores for each level of the independent variable manipulated within-subjects come from the same group of participants. For example, we can say that the independent variable of time is manipulated within-subjects. One-way means that there is one independent variable, for example, time. Independent variables are also called factors in ANOVA. A factor is made up of different levels. Time could have three levels: before, during and after therapy. In repeated measures/within-subjects designs, these levels are often referred to as conditions. Repeated measures means that the same dependent variable (depression score) is measured multiple times on the same participant. Here, it's measured at different time points. Each participant provides multiple measurements. 6.3 Long format data To conduct a repeated measures ANOVA in R, the data must be in long format. This is crucial. Exercise 6.1 Wide vs. long format When the data are in wide format, all of the data for a single participant is stored: on one row, across multiple columns on multiple rows, with each row representing a separate observation When the data are in long format, all of the data for a single participant is stored: on one row, across multiple columns on multiple rows, with each row representing a separate observation 6.4 Worked Example In an investigation of language learning, Chang et al., (2021) provided neuro-feedback training to 6 individuals attempting to discriminate particular sounds in a foreign language. Performance was measured as the proportion of correct responses in the discrimination task, and was measured at three time points relative to when the training was administered: before training was given (pre), after 3 days (day3), and after two months (month2). The data are at the link below: https://raw.githubusercontent.com/chrisjberry/Teaching/master/6a_discrimination_test.csv Exercise 6.2 Design check. What is the independent variable (or factor) in this design? proportion correcttime since training How many levels does the factor have? 1234 What is the dependent variable? proportion correcttime What is the nature of the independent variable? categoricalcontinuous Is the independent variable manipulated within- or between-subjects? between-subjectswithin-subjects 6.4.1 Read in the data # library(tidyverse) # read in the data discriminate_wide &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/6a_discrimination_test.csv&#39;) # look at the data discriminate_wide ppt pre day3 month2 1 0.56 0.63 0.65 2 0.71 0.79 0.76 3 0.53 0.86 0.92 4 0.56 0.81 0.77 5 0.49 0.85 0.84 6 0.55 0.84 0.84 Columns: ppt: the participant number pre: proportion correct before training day3: proportion correct 3 days after training month2: proportion correct 2 months after training Exercise 6.3 Data check How many participants are there in this dataset? 34618 Are the data in long format or wide format? longwide Explain The data for each of the 6 participants is on a different row in discriminate_wide, with the scores for each level of the independent variable time in different columns. The data are therefore in wide format. 6.4.2 Convert the data to long format Data must be in long format in order to be analysed using a repeated measures ANOVA. Use pivot_longer() to convert the data to long format: # The code below: # - stores the result in discriminate_long # - takes discriminate_wide and pipes it to # - pivot_longer() # - specifies which of the existing columns to make into a new single column # - specifies the name of the new column labeling the conditions # - specifies the name of the column containing the dependent variable discriminate_long &lt;- discriminate_wide %&gt;% pivot_longer(cols = c(&quot;pre&quot;, &quot;day3&quot;, &quot;month2&quot;), names_to = &quot;time&quot;, values_to = &quot;performance&quot;) # look at the first 6 rows discriminate_long %&gt;% head() ppt time performance 1 pre 0.56 1 day3 0.63 1 month2 0.65 2 pre 0.71 2 day3 0.79 2 month2 0.76 Exercise 6.4 Data check - long How many participants are there in discriminate_long? 24618 Are the data in discriminate_long in long format or wide format? longwide How many rows or observations are there in discriminate_long? 34618 Explain Each score is now on a separate row. There is a column to code the participant and condition. The data are therefore in long format. Each participant contributes 3 scores (one for pre, day3 and month2). In discriminate_long, what type of variable is ppt currently? factor character double (numeric) Hint. Look at the label at the top of each column, e.g., &lt;dbl&gt; In discriminate_long, what type of variable is time currently? factor character double (numeric) 6.4.3 Convert the participant and the categorical variable to factors As with the analysis of categorical variables in regression and ANOVA, each categorical variable must be converted to a factor before the analysis. Importantly, in repeated measures designs, the column of participant labels ppt must also be converted to a factor. Because the levels of time are ordered, we can additionally specify the way the levels should be ordered by using levels = c(\"level_1_name\", \"level_2_name\", \"level_3_name\") when using factor(). # use mutate() to transform # both ppt and time to factor # and overwrite existing variables discriminate_long &lt;- discriminate_long %&gt;% mutate(ppt = factor(ppt), time = factor(time, levels = c(&quot;pre&quot;, &quot;day3&quot;, &quot;month2&quot;) ) ) # check discriminate_long %&gt;% head() ppt time performance 1 pre 0.56 1 day3 0.63 1 month2 0.65 2 pre 0.71 2 day3 0.79 2 month2 0.76 Exercise 6.5 Data check - factors After using mutate() and factor() above: In discriminate_long, what type of variable is ppt? factor character double (numeric) In discriminate_long, what type of variable is time? factor character double (numeric) 6.4.4 Visualise the data Look at the distribution of the dependent variable in each condition. discriminate_long %&gt;% ggplot(aes(x = performance)) + facet_wrap(~ time) + geom_density() Figure 6.1: Density plots of performance at each time point The density plots look a little 'lumpy'. Remember that in our sample, there are only six data points per condition, so the plots are unlikely to be representative of the true (population) distribution of scores! 6.4.5 Plot the means # load the ggpubr package library(ggpubr) # use ggline() to plot the means discriminate_long %&gt;% ggline(x = &quot;time&quot; , y = &quot;performance&quot;, add = &quot;mean&quot;) + ylab(&quot;Performance&quot;) + ylim(c(0,1)) Figure 1.2: Mean proportion correct at each time point Error bars Previously, we used desc_stat = \"mean_se\" to add errorbars to the plot. Doing so here would add error bars representing the standard error of the mean to each of the points in the plot. These are appropriate for between-subjects designs, but for within-subjects designs, these are not as appropriate. Refer to the Further Knowledge section at the end of the worksheet to see how to add within-subject error bars (and see Morey, 2008, for more detail). the same plot with ggplot discriminate_long %&gt;% ggplot(aes(x = time , y = performance)) + stat_summary(fun = mean, geom = &quot;point&quot;) + stat_summary(aes(group = 1), fun = mean, geom = &quot;line&quot;) + ylab(&quot;Performance&quot;) + ylim(c(0,1)) + theme_classic() Figure 3.1: Mean proportion correct at each time point Exercise 6.6 Describe the trend shown in the line plot: Between pre and day3, performance appeared to improveshow no changedecline Between pre and month2, performance appeared to improveshow no changedecline Between day3 and month2, performance appeared to improveshow no changedecline 6.4.6 Descriptives: Mean of each condition Obtain the mean proportion correct at each time point using summarise(): discriminate_long %&gt;% group_by(time) %&gt;% summarise(M = mean(performance)) time M pre 0.5666667 day3 0.7966667 month2 0.7966667 6.4.7 Bayes factor The crucial difference in repeated measures designs compared to between-subjects ones concerns the column labeling the participant, ppt. ppt is entered into the model as another predictor variable, i.e., using ...+ ppt. ppt is specified as a random factor using whichRandom =. This means that the model comprising the other predictors in the model will be evaluated relative to the null model containing ppt alone. ppt is not individually evaluated. The Bayes factors can be obtained with anovaBF() but can also be obtained (less conveniently) using lmBF(). Using anovaBF(): # library(BayesFactor) anovaBF(performance ~ time + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(discriminate_long)) ## Bayes factor analysis ## -------------- ## [1] time + ppt : 331.3637 ±0.72% ## ## Against denominator: ## performance ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS In the output, the Bayes factor provided is for the model of performance ~ time + ppt vs. the model of performance ~ ppt. The latter is the null model containing only ppt as a predictor variable. The Bayes factor therefore represents evidence for the (unique) effect of time on performance, over and above a model containing ppt only. This differs to between-subject designs, where the null model (in the denominator) is the intercept-only model (representing the grand mean). Exercise 6.7 The Bayes factor for the effect of time on performance is This indicates that (select one): it is approximately 300 times more likely that there is difference between the means of the three time conditions, compared to there being no difference. it is approximately 300 times more likely that there is no difference between the means of the three time conditions, compared to there being a difference. On the basis of the Bayes factor analysis, does neuro-feedback training affect performance in the language learning task? yesno Equivalent BFs using lmBF() It is possible to obtain the same result as anovaBF() using lmBF(). Doing so can help us to understand what anovaBF() is doing. Again, we must tell lmBF() that ppt is a random factor, using whichRandom =. # Equivalent results with lmBF() # model with time and ppt only BF_time_ppt &lt;- lmBF(performance ~ time + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(discriminate_long)) # model with ppt only BF_ppt_only &lt;- lmBF(performance ~ ppt, whichRandom = &quot;ppt&quot;, data = data.frame(discriminate_long)) # compare model with vs. without time BF_time_ppt / BF_ppt_only ## Bayes factor analysis ## -------------- ## [1] time + ppt : 330.0359 ±0.56% ## ## Against denominator: ## performance ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS This produces a Bayes factor of approximately 330. This is equivalent to the BF obtained with anovaBF(), though note that because of the random data sampling methods used to generate the Bayes factors, the value may differ by a small amount. 6.4.8 Follow-up tests The Bayes factor obtained with anovaBF() above tells us that there's a difference between the means of the conditions, but not which conditions differ from which. To determine which conditions differ from which, filter the data for the conditions of interest, then use anovaBF() in exactly the same way as before, but this time with relevant filtered data. 6.4.8.1 Pre vs. day3 conditions # pre vs. day3 # filter the data for the relevant conditions pre_v_day3 &lt;- discriminate_long %&gt;% filter(time == &quot;pre&quot; | time == &quot;day3&quot;) # # compare the means of the conditions, using anovaBF() anovaBF(performance ~ time + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(pre_v_day3)) ## Bayes factor analysis ## -------------- ## [1] time + ppt : 79.96102 ±0.73% ## ## Against denominator: ## performance ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS The Bayes factor for the comparison of mean performance in the pre and day3 conditions is . Note. Your BF may differ slightly from the one above due to the random sampling methods anovaBF() uses. This indicates that there is substantialinconclusive evidence for no differencea difference between the performance in the pre and day3 conditions. Performance at day3 was higherlower than performance pre training. 6.4.8.2 Pre vs. month2 conditions # pre vs. month2 # filter the data for these conditions pre_v_month2 &lt;- discriminate_long %&gt;% filter(time == &quot;pre&quot; | time == &quot;month2&quot;) # # compare the means of the conditions, using anovaBF() anovaBF(performance ~ time + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(pre_v_month2)) ## Bayes factor analysis ## -------------- ## [1] time + ppt : 54.2467 ±0.89% ## ## Against denominator: ## performance ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS The Bayes factor for the comparison of mean performance in the pre and month2 conditions is This indicates that there is substantialinconclusive evidence for no differencea difference between the mean performance in the pre and month2 conditions. Performance at month2 was higherlower than performance pre training. 6.4.8.3 day3 vs. month2 conditions # day3 vs. month2 # filter the data for these conditions day3_v_month2 &lt;- discriminate_long %&gt;% filter(time == &quot;day3&quot; | time == &quot;month2&quot;) # # compare the means of the conditions, using anovaBF() anovaBF(performance ~ time + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(day3_v_month2)) ## Bayes factor analysis ## -------------- ## [1] time + ppt : 0.4534745 ±1.47% ## ## Against denominator: ## performance ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS The Bayes factor for the comparison of mean performance in the day3 and month2 conditions is This indicates that there is substantialinconclusive evidence for an absence of a differencea difference between the mean performance in the day3 and month2 conditions. Thus, there's no evidence of a difference in performance between the day3 and month2 conditions. Equivalent results with lmBF() As before, it's possible to conduct the same comparisons and obtain the same BFs using lmBF(): # pre vs. day3 lmBF(performance ~ time + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(pre_v_day3)) / lmBF(performance ~ ppt, whichRandom = &quot;ppt&quot;, data = data.frame(pre_v_day3)) # pre vs. month2 lmBF(performance ~ time + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(pre_v_month2)) / lmBF(performance ~ ppt, whichRandom = &quot;ppt&quot;, data = data.frame(pre_v_month2)) # day3 vs. month2 lmBF(performance ~ time + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(day3_v_month2)) / lmBF(performance ~ ppt, whichRandom = &quot;ppt&quot;, data = data.frame(day3_v_month2)) ## Bayes factor analysis ## -------------- ## [1] time + ppt : 83.16192 ±2.6% ## ## Against denominator: ## performance ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS ## ## Bayes factor analysis ## -------------- ## [1] time + ppt : 54.16511 ±0.8% ## ## Against denominator: ## performance ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS ## ## Bayes factor analysis ## -------------- ## [1] time + ppt : 0.4902542 ±5.23% ## ## Against denominator: ## performance ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS 6.5 Two-way within subjects ANOVA In a two-way repeated measures or within-subjects ANOVA, there are two categorical independent variables or factors. When there are multiple factors, the ANOVA is referred to as factorial ANOVA. For example, if the design has two factors, and each factor has two levels, then we refer to the design as a 2 x 2 factorial design. The first number (2) denotes the number of levels of the first factor. The second number (2) denotes the number of levels of the second factor. If, instead, the second factor had three levels, we'd say we have a 2 x 3 factorial design. The cells of the design are produced by crossing the levels of one factor with each level of the other factor. If both factors are manipulated within-subjects, then the scores from each cell of the design come from the same group of participants. For example, in a 2 x 2 factorial design, there will be four cells of the design. 6.5.1 Worked Example Kreysa et al. (2016) analysed RTs to statements made by faces that had different gaze directions - the eyes of the face were either averted to the left, averted to the right, or were looking directly at the participant when the statement was made. RTs were further broken down according to whether the participant had agreed with the statement or not (they responded 'yes' or 'no'). Exercise 6.8 Design check. What is the first independent variable (or factor) that is mentioned in this design? RTgaze directionagreement with statement How many levels does the first factor have? 1234 What is the second independent variable (or factor) that was mentioned? RTgaze directionagreement with statement How many levels does the second factor have? 1234 What is the dependent variable? RTgaze directionagreement with statement What is the nature of the independent variables? categoricalcontinuous What type of design is this? 2 x 2 x 2 repeated measures factorial design3 x 2 repeated measures factorial design2 x 2 repeated measures design 6.5.2 Read in the data Read in the data from the link below: https://raw.githubusercontent.com/chrisjberry/Teaching/master/6_gaze_data.csv gaze &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/6_gaze_data.csv&#39;) gaze %&gt;% head() ppt gaze_direction agreement RT logRT 1 averted_left no 725.6667 6.553344 1 averted_left yes 787.6667 6.615148 1 averted_right no 621.6667 6.389811 1 averted_right yes 752.6667 6.568404 1 direct no 1001.3333 6.829217 1 direct yes 688.8333 6.477116 Columns: ppt: the participant number gaze_direction: the gaze-direction of the face. agreement: whether the responses made to the statements were 'yes' or 'no' RT: mean response times logRT: the log transform of the RT column, i.e., log(RT) Are the data in long format or wide format? wide formatlong format Explain You can tell this is a repeated measures design and that the data are in long format because a given participant's scores are on multiple rows, and there are multiple scores on the dependent variable (RT) for each participant. 6.5.3 Visualise the data Reaction time data tend to be positively skewed. As result, Kreysa et al. (2016) analysed the RT data after it had been log transformed. The log transformed data are in the column logRT. To see the positive skew, inspect the (untransformed) RTs in RT: gaze %&gt;% ggplot(aes(x=RT)) + facet_wrap(~ gaze_direction * agreement) + geom_density() Figure 5.1: Mean RT according to gaze direction and agreement Now inspect the log transformed RTs in logRT: gaze %&gt;% ggplot(aes(x = logRT)) + facet_wrap(~ gaze_direction * agreement) + geom_density() Figure 5.2: Mean logRT according to gaze direction and agreement Does the positive skew in the distributions appear to be reduced by the log transformation? noyes 6.5.4 Plot the means ggline() in the ggpubr package can be used to plot the mean of each condition. # load the ggpubr package #library(ggpubr) # use ggline to plot the means gaze %&gt;% ggline(x = &quot;gaze_direction&quot; , y = &quot;logRT&quot;, add = &quot;mean&quot;, color= &quot;agreement&quot;) + ylab(&quot;Mean log RT&quot;) Figure 2.3: Mean logRT according to gaze direction and agreement It appears as if people took longer to respond when the gaze was direct and they said 'no' to the statement that was made. The same plot in ggplot gaze %&gt;% ggplot(aes(x = gaze_direction, y = logRT, color = agreement)) + stat_summary(fun = mean, geom = &quot;point&quot;) + stat_summary(aes(group = agreement), fun = mean, geom = &quot;line&quot;) + theme_classic() Figure 6.2: Mean logRT according to gaze direction and agreement 6.5.5 Descriptive statistics - mean To obtain the mean of each condition, use group_by() to group the results of summarise() by the two factors in the design. Since the log RT values are analysed, we'll obtain the mean of these. # take the data in gaze # pipe to group_by() # group by the gaze_direction and agreement factors # create a table of the mean of each condition # and label the column &#39;M&#39; gaze %&gt;% group_by(gaze_direction, agreement) %&gt;% summarise(M = mean(logRT)) gaze_direction agreement M averted_left no 6.461225 averted_left yes 6.476511 averted_right no 6.364156 averted_right yes 6.420004 direct no 6.645607 direct yes 6.456804 The mean log RT when participants disagreed with a statement made by a face looking directly at them was (to 2 decimal places) . 6.5.6 Convert participant and categorical variables to factors Convert the ppt, gaze_direction, and agreement variables to factors so that R will treat them as such in the ANOVA. # use mutate() to transform # ppt, gaze_direction and agreement to factors # and overwrite existing variables gaze &lt;- gaze %&gt;% mutate(ppt = factor(ppt), gaze_direction = factor(gaze_direction), agreement = factor(agreement)) # check changes gaze %&gt;% head() ppt gaze_direction agreement RT logRT 1 averted_left no 725.6667 6.553344 1 averted_left yes 787.6667 6.615148 1 averted_right no 621.6667 6.389811 1 averted_right yes 752.6667 6.568404 1 direct no 1001.3333 6.829217 1 direct yes 688.8333 6.477116 As before, you could double check that the relevant variable labels in gaze have changed to &lt;fct&gt;. After using mutate() and factor() above: In gaze, what type of variable is ppt? factor character double (numeric) In gaze, what type of variable is gaze_direction? factor character double (numeric) In gaze, what type of variable is agreement? factor character double (numeric) 6.5.7 Bayes factor As with between-subjects two-way designs, researchers are typically interested in obtaining three things in a two-way repeated measures ANOVA: the main effect of factor1 the main effect of factor2 the interaction between factor1 and factor2, i.e., the factor1*factor2 interaction term. Obtain the Bayes factors for each of the above using anovaBF(). To specify the full model with the interaction, use factor1 * factor2. This will automatically specify a model with the main effects of factor1 and factor2, in addition to the interaction term: As before, add ppt to the model using ...+ ppt, and tell R it is a random factor using whichRandom = ppt. # obtain BFs for the anova BFs_gaze &lt;- anovaBF(logRT ~ gaze_direction*agreement + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(gaze) ) # look at BFs BFs_gaze ## Bayes factor analysis ## -------------- ## [1] gaze_direction + ppt : 26.36845 ±0.77% ## [2] agreement + ppt : 0.2650302 ±0.96% ## [3] gaze_direction + agreement + ppt : 7.405545 ±1.42% ## [4] gaze_direction + agreement + gaze_direction:agreement + ppt : 43.07602 ±1.64% ## ## Against denominator: ## logRT ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS Interpretation of the output is similar as with two-way between-subjects ANOVA (Session 3). Notice, however, that ppt is included in each of models, and the comparison of each model is not against the intercept-only model as it was with between-subjects factorial ANOVA. Because this is a repeated measures design, each model is compared against a null model containing only ppt as a predictor. 6.5.7.1 Main effect of gaze_direction The main effect of gaze direction: BFs_gaze[1] ## Bayes factor analysis ## -------------- ## [1] gaze_direction + ppt : 26.36845 ±0.77% ## ## Against denominator: ## logRT ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS RTs tended not to differto differ between gaze_direction conditions, BF = . 6.5.7.2 The main effect of agreement BFs_gaze[2] ## Bayes factor analysis ## -------------- ## [1] agreement + ppt : 0.2650302 ±0.96% ## ## Against denominator: ## logRT ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS RTs tended not to differto differ between agreement conditions, BF = . 6.5.7.3 The gaze_direction x agreement interaction BFs_gaze[4] / BFs_gaze[3] ## Bayes factor analysis ## -------------- ## [1] gaze_direction + agreement + gaze_direction:agreement + ppt : 5.816725 ±2.17% ## ## Against denominator: ## logRT ~ gaze_direction + agreement + ppt ## --- ## Bayes factor type: BFlinearModel, JZS There was substantial evidence forwas insufficient evidence forwas evidence of an absence of an interaction between gaze_direction and agreement, BF = . 6.5.8 Follow up comparisons Given the evidence for an interaction, Kreysa et al. (2016) conducted pairwise comparisons to compare \"yes\" and \"no\" responses in each gaze direction condition. This can be achieved by filtering the data for each gaze direction condition, and then using anovaBF() to compare scores in each agreement condition. If there were no evidence for the interaction, we wouldn't conduct further comparisons. 6.5.8.1 averted_left # filter data for averted_left condition averted_left &lt;- gaze %&gt;% filter(gaze_direction == &quot;averted_left&quot;) # use anovaBF() to compare the agreement conditions anovaBF(logRT ~ agreement + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(averted_left)) ## Bayes factor analysis ## -------------- ## [1] agreement + ppt : 0.249462 ±1.48% ## ## Against denominator: ## logRT ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS There was substantial evidence for a differencean absence of a difference between the mean log RTs made to \"yes\" and \"no\" responses in the averted_left condition, BF = . 6.5.8.2 averted_right # filter data for averted right condition averted_right &lt;- gaze %&gt;% filter(gaze_direction == &quot;averted_right&quot;) # use anovaBF() to compare the agreement conditions anovaBF(logRT ~ agreement + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(averted_right)) ## Bayes factor analysis ## -------------- ## [1] agreement + ppt : 0.347839 ±0.73% ## ## Against denominator: ## logRT ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS The Bayes factor comparing the mean log RTs to \"yes\" and \"no\" responses in the averted_right condition was inconclusive, BF = , although there was more evidence for the null hypothesis of there being no difference between conditions. 6.5.8.3 direct # filter data for direct condition direct &lt;- gaze %&gt;% filter(gaze_direction == &quot;direct&quot;) # use anovaBF() to compare the agreement conditions anovaBF(logRT ~ agreement + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(direct)) ## Bayes factor analysis ## -------------- ## [1] agreement + ppt : 33.71397 ±0.84% ## ## Against denominator: ## logRT ~ ppt ## --- ## Bayes factor type: BFlinearModel, JZS There was substantial evidence for a differencean absence of a difference between the mean log RTs made to \"yes\" and \"no\" responses in the direct condition, BF = . RTs tended to be longer in the direct condition when participants made a noyes response. 6.6 Exercise: two-way mixed ANOVA Exercise 6.9 Mixed ANOVA When one of the factors is manipulated within-subjects and the other is manipulated between-subjects, the design is said to be a mixed design. It is also sometimes called a split-plot design. Hammel and Chan (2016) looked at the duration (in seconds) that participants were able to balance on a balance-board on three successive occasions (trials). There were three groups of participants: one group (counterfactual) engaged in counterfactual thinking, another group (prefactual) engaged in prefactual thinking, and another group (control) engaged in an unrelated thinking task. The data (already in long format) are located at the link below: https://raw.githubusercontent.com/chrisjberry/Teaching/master/6_improve_long.csv 1. Create a line plot of trial vs. duration. Represent each group as a separate line. Balance duration seems highest in the counterfactualprefactualcontrol group. Balance duration seems lowest in the counterfactualprefactualcontrol group. Balance duration generally appears to become shorterremain constantbecome longer across trials. Hints convert ppt and the columns labeling the levels of the independent variables to factors. use ggline() in the ggpubr package to create the line plot. Solution - code # read in the data balance &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/6_improve_long.csv&#39;) # plot # # library(ggpubr) balance %&gt;% ggline(x = &quot;trial&quot;, y = &quot;duration&quot;, color = &quot;group&quot;, add = &quot;mean&quot;) 2. Bayes factors The main effect of group: BF = The main effect of trial: BF = The group x trial interaction: Note. It may take a short while for anovaBF() to determine the BFs. The processing time increases for more complex designs, especially those with repeated measures. Solution - code # convert ppt and independent variables to factors balance &lt;- balance %&gt;% mutate(ppt = factor(ppt), group = factor(group), trial = factor(trial)) # anovaBF #library(BayesFactor) bfs &lt;- anovaBF(duration ~ ppt + group + trial, whichRandom = &quot;ppt&quot;, data = data.frame(balance)) # main effect of group bfs[1] # main effect of trial bfs[2] # group*trial interaction bfs[4] / bfs[3] # You&#39;ll notice there is a large amount of error on the interaction term # If you have time to wait, then we can recompute the BF with a larger number of iterations, # which should reduce the error. The BFs come out very similarly, but with smaller error. # Try this: # bfs2 &lt;- recompute(bfs, iterations = 100000) # bfs2[1] # bfs2[2] # bfs2[4]/bfs2[3] 4. Follow up comparisons. Conduct follow-up comparisons to investigate the interaction further. Note, we wouldn't usually do this because there was evidence against there being an interaction between trial and group. In statistical terms, this means that the effect of trial is the same in each group, so there's no need to dig deeper into the data. Instead, the following is a pedagogical exercise only. Assuming there were an interaction, one approach to investigating it is, for each group, to conduct a one-way ANOVA to determine the effect of trial on duration. This is called a simple effects analysis. This is where we assess the effect of trial at each level of the second factor, in this case group. Conduct three one-way repeated measures ANOVAs, comparing duration across the three types of trial in each group. Prefactual group: There's substantial evidence for the effect of trial on duration, BF = . Counterfactual group: There's substantial evidence for the effect of trial on duration, BF = . Control group: There's insufficient evidence for the effect of trial on duration, BF = . Hint Use filter() to subset the data for each group. Use anovaBF() to conduct a one-way repeated measures ANOVA for each group. Solution - code # filter rows corresponding to prefactual group prefactual &lt;- balance %&gt;% filter(group == &quot;prefactual&quot;) # # one-way ANOVA, comparing trial_1, trial_2, trial_3 anovaBF(duration ~ trial + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(prefactual)) # filter rows corresponding to counterfactual group counterfactual &lt;- balance %&gt;% filter(group == &quot;counterfactual&quot;) # # one-way ANOVA, comparing trial_1, trial_2, trial_3 anovaBF(duration ~ trial + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(counterfactual)) # filter rows corresponding to control group control &lt;- balance %&gt;% filter(group == &quot;control&quot;) # # one-way ANOVA, comparing trial_1, trial_2, trial_3 anovaBF(duration ~ trial + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(control)) 6.7 Further knowledge - within-subject errorbars Within-subject errorbars Here I show you how to plot the means with within-subject error bars (following Morey, 2008) for the first dataset in the worksheet discriminate_long. I use summarySEwithin() from the Rmisc package for this. The functions from this package have similar names to other tidyverse functions, which can lead to problems/conflicts later. For that reason, I unload Rmisc after using the functions from it, then reload the tidyverse. # within-subjects error bars # load Rmisc package for the summarySEwithin() function library(Rmisc) # get within-subject intervals # can also specify between-subjects vars with &#39;betweenvars = &#39; intervals_RM &lt;- summarySEwithin(discriminate_long, measurevar = &quot;performance&quot;, withinvars = &quot;time&quot;, idvar = &quot;ppt&quot;) # look at what&#39;s produced # intervals_RM # select time, performance and se columns # rename &#39;performance&#39; to M intervals_RM &lt;- intervals_RM %&gt;% select(time, performance, se) %&gt;% dplyr::rename(M = performance, SE = se) # compare with between subject intervals (they should be larger) # summarySE(discriminate_long, measurevar = &quot;performance&quot;, groupvars = &quot;time&quot;) # unload the Rmisc package due to conflicts with other packages detach(&quot;package:Rmisc&quot;, unload = TRUE) # now use the M and within-subjects SE to create a plot of means intervals_RM %&gt;% ggplot(aes(x = time, y = M)) + geom_point() + geom_errorbar(aes(ymin = M - SE, ymax = M + SE), width=.2, position = position_dodge(.9)) + theme_classic() Figure 6.3: Mean proportion correct at each time point. Error bars represent within-subjects standard error of the mean 6.8 Further knowledge - R-squared One-way within-subjects ANOVA It's possible to obtain R2 for the ANOVA model as we have done previously, by specifying the model with lm(), and then using glance(): For the one-way repeated measures ANOVA: # specify full model with ppt and time using lm() ppt_and_time &lt;- lm(performance ~ ppt + time, data = discriminate_long) # look at R-squared glance(ppt_and_time) This produces an R2 value, which is relatively high, R2 = (non-corrected, as a proportion, to two decimal places). This value of R2 is, however, for the full ANOVA model, which includes the random factor ppt too. Given that we are interested in the R2 associated with time only, we need to work out the change in R2 as a result of adding time to the model containing ppt. To do this, obtain R2 for the model with ppt alone: # specify model with ppt only using lm() ppt_alone &lt;- lm(performance ~ ppt, data = discriminate_long) # make sure the broom package is loaded #library(broom) # look at R-squared glance(ppt_alone) R2 for the model with ppt alone = The R2 associated with the time factor can be calculated as the difference in R2 values for the models: glance(ppt_and_time)$r.squared - glance(ppt_alone)$r.squared Thus, R2 for the time factor = The value of R2 that we have calculated for time is the one that we would provide in a journal article, and could be reported as a measure of effect size. In the literature, this R2 value actually goes by the name generalised eta-squared, and is the recommended measure of effect size for repeated measures designs (see Bakeman, 2005, for more details). Two-way repeated measures ANOVA To calculate generalised eta-squared for each factor and the interaction term in a repeated measures ANOVA, each model in the design must be separately specified with lm(). The change in R2 value for each factor and the interaction must then be separately determined, by subtracting the relevant R2 value: # specify each model in the design gaze_ppt_only &lt;- lm(logRT ~ ppt, data = gaze) gaze_ppt_agreement &lt;- lm(logRT ~ ppt + agreement, data = gaze) gaze_ppt_gaze &lt;- lm(logRT ~ ppt + gaze_direction, data = gaze) gaze_ppt_agreement_gaze &lt;- lm(logRT ~ ppt + agreement + gaze_direction, data = gaze) gaze_ppt_agreement_gaze_interaction &lt;- lm(logRT ~ ppt + agreement + gaze_direction + agreement*gaze_direction, data = gaze) # work out change in R-squared (equivalent to Generalised Eta Squared or ges) glance(gaze_ppt_agreement)$r.squared - glance(gaze_ppt_only)$r.squared # ges for agreement glance(gaze_ppt_gaze)$r.squared - glance(gaze_ppt_only)$r.squared # ges for gaze_direction glance(gaze_ppt_agreement_gaze_interaction)$r.squared - glance(gaze_ppt_agreement_gaze)$r.squared # ges for interaction This is clearly quite cumbersome, and potentially error prone. For this reason, you may wish to use a package that will determine generalised eta-squared automatically, such as the afex package, which stands for Analysis of Factorial EXperiments. The package performs an ANOVA using frequentist methods, but provides generalised eta squared by default in the output. library(afex) # use the afex package to run a 2 x 2 ANOVA # id is the column of participant identifiers (&quot;ppt&quot;) # dv is the dependent variable (&quot;logRT&quot;) # within specifies the within-subjects factors anova2way &lt;- afex::aov_ez(id = c(&quot;ppt&quot;), dv = c(&quot;logRT&quot;), within = c(&quot;agreement&quot;,&quot;gaze_direction&quot;), # two factors data = gaze) The ges column provides the generalised eta squared values for each effect. One-way between-subjects ANOVA The afex package can also be used to obtain generalised eta squared for one-way between-subjects ANOVA designs. For the one-way between-subjects ANOVA performed on the affect_data in Session 3: # Read in the data affect_data &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/3_affect.csv&#39;) # use mutate() to convert the group variable to a factor # store the changes back in affect_data # (i.e., overwrite what&#39;s already in affect_data) affect_data &lt;- affect_data %&gt;% mutate(group = factor(group)) # use the afex package to a one-way ANOVA # id is the column of participant identifiers (&quot;ppt&quot;) # dv is the dependent variable (&quot;logRT&quot;) # between specifies the between-subjects factor anova1way &lt;- afex::aov_ez(id = c(&quot;ppt&quot;), dv = c(&quot;score&quot;), between = c(&quot;group&quot;), # one factor data = affect_data) # look at output anova1way The ges column provides the generalised eta squared value for the effect of group on score. The value (0.052) matches the value of R2 we found using lm() and glance(). Generalised eta squared is the same as R2 in this design. Two-way between-subjects ANOVA The afex package can also be used to obtain generalised eta squared for two-way between-subjects ANOVA designs. For the two-way between-subjects ANOVA performed on the resilience_data in Session 3: # read in the data resilience_data &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/3_resilience_data.csv&#39;) # use mutate() and factor() to convert # resilience and adversity to factors # add a ppt column resilience_data &lt;- resilience_data %&gt;% mutate(resilience = factor(resilience), adversity = factor(adversity), ppt = c(1:n())) # use the afex package to a two-way ANOVA # id is the column of participant identifiers (&quot;ppt&quot;) # dv is the dependent variable (&quot;logRT&quot;) # between specifies the between-subjects factor anova2bet &lt;- afex::aov_ez(id = c(&quot;ppt&quot;), dv = c(&quot;distress&quot;), between = c(&quot;resilience&quot;,&quot;adversity&quot;), # two factors data = resilience_data) anova2bet The generalised eta squared values are in the column ges. 6.9 Summary To analyse repeated measures data with ANOVA, the data must be in long format. Convert from wide to long format using pivot_longer(). Convert the columns containing the participant identifier (e.g., ppt) and independent variables to factors using factor(). Add ppt as a predictor in anovaBF(), and specify as a random factor using whichRandom = \"ppt\". Bayes factors given by anovaBF() compare each model against a null model with ppt only (as a random factor), rather than the intercept-only model (as was the case in between-subjects designs). Conduct follow-up tests using anovaBF() if there's evidence for an interaction. 6.10 References Bakeman, R. (2005). Recommended effect size statistics for repeated measures designs. Behavior Research Methods, 37(3), 379-384. https://doi.org/10.3758/BF03192707 Chang, M., Ando, H., Maeda, T., Naruse, Y. (2021). Behavioral effect of mismatch negativity neurofeedback on foreign language learning. PLoS ONE, 16(7): e0254771. https://doi.org/10.1371/journal.pone.0254771 Hammell, C., Chan, A.Y.C. (2016). Improving physical task performance with counterfactual and prefactual thinking. PLoS ONE, 11(12): e0168181. doi:10.1371/journal.pone.0168181 Kreysa, H., Kessler, L., Schweinberger, S.R. (2016). Direct Speaker Gaze Promotes Trust in Truth-Ambiguous Statements. PLoS ONE, 11(9): e0162291. doi:10.1371/journal.pone.0162291 Morey, R. D. (2008). Confidence intervals from normalized data: A correction to Cousineau (2005). Reason, 4(2), 61-64. Rouder, J. N., Morey, R. D., Speckman, P. L., &amp; Province, J. M. (2012). Default Bayes factors for ANOVA designs. Journal of Mathematical Psychology, 56(5), 356-374. "],["prepost.html", "Session 7 Pre-post data, clinically significant change 7.1 Overview 7.2 Approach 1 - Difference scores 7.3 Approach 2 - Mixed ANOVA 7.4 Approach 3 - ANCOVA 7.5 Which approach should you use? 7.6 Clinical Significance 7.7 Exercises 7.8 Further Knowledge 7.9 Summary 7.10 References", " Session 7 Pre-post data, clinically significant change Chris Berry 2025 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} 7.1 Overview Slides from the lecture part of the session: Download R Studio online Access here using University log-in To investigate the effect of an intervention, researchers may measure a dependent variable before and after the intervention. For example, a researcher might look at the impact of a particular treatment (or therapy) on the severity of depression symptoms. The data produced is sometimes referred to as pre-post data. Pre-post data in the treatment group is often compared with that of a control group. There are numerous ways to analyse the data from such designs. Three approaches are presented here, alongside an introduction to methods for calculating clinically significant change. 7.1.1 Worked example Francis et al. (2019) investigated whether a shift to a healthy diet impacted on depression in a group of individuals that had diets that fell short of the criteria for healthy eating, as defined by the Australian Guide to Healthy Eating. There were 76 participants. All had moderate to severe symptoms of depression. Half of the participants received instruction on guidance regarding their diet. Severity of depression symptoms were measured at two time points: before the intervention, and again three weeks later. Exercise 7.1 Design Check What is the name of the first independent variable mentioned? severity of depression symptomsdietary intervention Is the first independent variable manipulated between- or within-subjects? between-subjectswithin-subjects How many levels does the first independent variable have? 124 What is the name of the second independent variable mentioned in the study? timeseverity of depression symptoms Is the second independent variable manipulated between- or within-subjects? between-subjectswithin-subjects How many levels does the second independent variable have? 124 What is the dependent variable? severity of depression symptomsdietary intervention 7.1.2 Read in the data # read in diet &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/7_diet_depression.csv&#39;) # preview diet %&gt;% head() ppt group baseline week3 1 diet_change 18.00000 11.00000 2 diet_change 34.00000 11.00000 3 diet_change 28.42105 3.00000 4 diet_change 25.00000 14.00000 5 diet_change 40.00000 49.00000 6 diet_change 15.00000 14.73684 Exercise 7.2 Data Check How many participants are there in diet? 6386676 Are the data in diet in long format or wide format? longwide Explain Each participant's data is on a separate row. There is a column to code the group membership. The scores for each time condition, baseline and week3, are in separate columns. Therefore the data are in wide format. You can inspect all the data in a separate data viewer with the code View(diet). (Note that although there are 76 participants on 76 rows, the ppt identifiers go up to 77; this is because the data for participant 50 were incomplete and were therefore excluded.) What do the scores in the baseline and week3 columns represent? severity of depression symptomstimedietary change 7.1.3 Plot the means To plot the means, we need the data to be in long format: # The code below: # - stores the result in diet_long # - takes diet and pipes it to # - pivot_longer() # - specifies which of the existing columns to make into a single new column (&#39;cols&#39;) # - specifies the name of the new column labeling the conditions (&#39;names_to&#39;) # - specifies the name of the column containing the dependent variable (&#39;values_to&#39;) diet_long &lt;- diet %&gt;% pivot_longer(cols = c(baseline, week3), names_to = &quot;time&quot;, values_to = &quot;symptoms&quot;) # preview diet_long %&gt;% head() ppt group time symptoms 1 diet_change baseline 18.00000 1 diet_change week3 11.00000 2 diet_change baseline 34.00000 2 diet_change week3 11.00000 3 diet_change baseline 28.42105 3 diet_change week3 3.00000 Now use diet_long with ggdotplot() in the ggpubr package. library(ggpubr) diet_long %&gt;% ggdotplot(x = &quot;time&quot;, y = &quot;symptoms&quot;, color = &quot;group&quot;, add = &quot;mean_se&quot;, ylim = c(0,65), position_dodge = 0.1) Figure 2.1: Mean depression symptoms score. Solid circles denote the mean, error bars the SE. Each dot represents one participant's data. The solid circles indicate the mean severity of depression symptoms in each condition (with between-subject error bars added by using add = mean_se). (See Session 6 for further details on producing within-subject error bars, which may be more appropriate here.) Higher scores indicate greater severity. In the diet_change group, from baseline to week 3, the mean severity of symptoms appeared to lessenworsenstay about the same. In the habitual_diet group, from baseline to week 3, the mean severity of symptoms appeared to lessenworsenstay about the same. 7.2 Approach 1 - Difference scores One approach to analysing pre-post data is to compare the change in symptom scores from baseline to week3 across groups. This is achieved by 1) calculating the difference in scores between baseline and week3, and then 2) comparing the difference scores between the two groups using a between-subjects ANOVA (or, equivalently, a t-test). To calculate the change in severity score for each participant, it's simplest to use the wide format data since we can easily subtract scores in one column (week3) from another (baseline). # using the wide format data # calculate the difference in baseline and week3 scores # store in a new column called &#39;change&#39; diet &lt;- diet %&gt;% mutate(change = week3 - baseline) # look at new column diet %&gt;% head() ppt group baseline week3 change 1 diet_change 18.00000 11.00000 -7.0000000 2 diet_change 34.00000 11.00000 -23.0000000 3 diet_change 28.42105 3.00000 -25.4210526 4 diet_change 25.00000 14.00000 -11.0000000 5 diet_change 40.00000 49.00000 9.0000000 6 diet_change 15.00000 14.73684 -0.2631579 The column change now contains the change in symptom severity score from baseline to week3. A negative difference indicates an improvement in symptoms (i.e., the score at week3 was lower). To compare the change scores between the two groups, use anovaBF(). The functions lmBF() or ttestBF() could also be used. They all give equivalent results because there are only two groups being compared. # Convert the grouping variable &#39;group&#39; to a factor diet &lt;- diet %&gt;% mutate(group = factor(group)) # anovaBF(): between-subjects ANOVA anovaBF(change ~ group, data = data.frame(diet)) ## Bayes factor analysis ## -------------- ## [1] group : 2.262527 ±0.01% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS Alternatively with lmBF and ttest # lmBF(): between-subjects ANOVA lmBF(change ~ group, data = data.frame(diet)) # ttestBF: between-subjects ttestBF(x = diet$change[ diet$group == &quot;habitual_diet&quot; ], y = diet$change[ diet$group == &quot;diet_change&quot; ]) ## Bayes factor analysis ## -------------- ## [1] group : 2.262527 ±0.01% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS ## ## Bayes factor analysis ## -------------- ## [1] Alt., r=0.707 : 2.262527 ±0.01% ## ## Against denominator: ## Null, mu1-mu2 = 0 ## --- ## Bayes factor type: BFindepSample, JZS The Bayes factor comparing the change in severity of depression symptoms between baseline and week 3 is (to two decimal places) . Although there is noweaksubstantial evidence for a difference in the change score between groups, the Bayes factor was inconclusivesubstantial. Thus, by the method of comparing change scores between groups, can we conclude that the dietary intervention led to a reduction in depression symptom severity? yesno 7.3 Approach 2 - Mixed ANOVA Given that we have two independent variables, each with two levels, and where one is manipulated between-subjects and the other within-subjects, it is also possible to treat the design as a 2 x 2 mixed factorial design and analyse the data using anovaBF(). Because one of the factors is manipulated within-subjects, we need to add ppt to the model and specify it as a random factor using whichRandom = \"ppt\" (see Session 6). We also need to convert ppt to a factor, along with the other independent variables, group and time. As with repeated measures ANOVA in Session 6, the long format data needs to be used for the analysis. # convert ppt and IVs to factors diet_long &lt;- diet_long %&gt;% mutate(ppt = factor(ppt), group = factor(group), time = factor(time)) # anova model bfs &lt;- anovaBF(symptoms ~ group + time + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(diet_long)) # look at the bfs bfs In this analysis, we are less interested in the main effects of group and time and are more interested in the interaction between the factors. The interaction will tell us whether the change in symptom severity differs between groups. As with previous two-way ANOVAs, the BF for the interaction can be obtained by dividing the BF in [4] by the BF in [3] (see Sessions 3 and 6). # bf for the interaction bfs[4] / bfs[3] The Bayes factor representing evidence for the interaction term is . The Bayes factor for the interaction should come out at around 2.25, but because anovaBF() calculates the BF using random sampling methods, your value may not match this exactly (note the large error associated with the BF in the output). For greater precision, it's possible to recompute() the Bayes factor with a greater number of random samples, e.g., 1,000,000 (note, this could take a short while): # recompute bfs, but with a million iterations bfs_more &lt;- recompute(bfs, 1000000) # re-obtain the BF for the interaction, using bf_more bfs_more[4] / bfs_more[3] The Bayes factor for the interaction, using bf_more is . Although there is noweaksubstantial evidence for a difference in the change score between groups, the Bayes factor was inconclusivesubstantial. As a result of using a mixed ANOVA, can we conclude that the dietary intervention led to a reduction in depression symptom severity? yesno 7.4 Approach 3 - ANCOVA A third approach to analysing pre-post data is to use ANCOVA, which stands for Analysis of COvariance. This approach is particularly appropriate if our focus were solely on comparing the scores at the second time point between groups, but controlling for baseline symptom severity. A covariate is a background variable that may be associated with the outcome variable, but is not of primary interest. In ANCOVA, the variance explained by a covariate is first accounted for, before examining whether the dependent variable differs between groups or conditions. Because some of the error variance is explained by the covariate, ANCOVAs can have the advantage of being more powerful tests of the predictor of interest. The ANCOVA is essentially a one-way between subjects ANOVA, with symptom severity at week3 as the dependent variable, group as the independent variable, and symptoms at baseline as the covariate. Equivalently, the analysis can be conceptualised is a multiple regression of week3 scores on the basis of group and baseline. Assessing the unique contribution of group will tell us whether there's a difference in symptom severity at week3, after accounting for the variance explained by baseline scores (see Sessions 2 and 4)). Note that because effects of time are not analysed in this approach, there is no repeated measures factor, and ppt does not need to be included as a random factor. # The model with both group and baseline scores full &lt;- lmBF(week3 ~ group + baseline, data = data.frame(diet)) # The model with baseline scores only (for comparison) baseline &lt;- lmBF(week3 ~ baseline, data = data.frame(diet)) # The BF representing evidence for the unique contribution of group full / baseline ## Bayes factor analysis ## -------------- ## [1] group + baseline : 6.124479 ±0.83% ## ## Against denominator: ## week3 ~ baseline ## --- ## Bayes factor type: BFlinearModel, JZS The Bayes factor for the unique contribution of group is . There is noweaksubstantial evidence for a difference in the week3 scores between groups, after controlling for baseline scores. As a result of using the ANCOVA approach, can we conclude that the dietary intervention led to a reduction in depression symptom severity? yesno R-squared In Approach 3 R2 for the effect of group can be determined using methods from earlier sessions. The method is equivalent to obtaining R2 for the unique contribution of a predictor. # ensure broom is loaded # library(broom) # specify the full model with lm() full_model &lt;- lm(week3 ~ group + baseline, data = diet) # specify the model with baseline only baseline_only &lt;- lm(week3 ~ baseline, data = diet) # R-squared full model glance(full_model)$r.squared # R-squared baseline only glance(baseline_only)$r.squared # Unique contribution of group # (effect size associated with difference in groups) glance(full_model)$r.squared - glance(baseline_only)$r.squared # R-sq = 0.05817 7.5 Which approach should you use? Which method you use depends on the research question you are asking. If your question is whether the change in the dependent variable across timepoints differs between the two groups, then Methods 1 or 2 are appropriate. If your question is whether one group has a higher mean at the second timepoint, then Method 3 seems most appropriate. Indeed, ANCOVA is often recommended for pre-post designs (see e.g., O'Connel et al., 2017). An advantage of the ANCOVA method is that is has greater statistical power. It is the approach that Francis et al. (2019) used. 7.6 Clinical Significance The measure of depressive symptoms used by Francis et al. (2019) was the Centre for Epidemiological Studies Depression scale-Revised (CESD-R; Radloff, 1977). The scale ranges from 0-60, and the criterion for being the elevated range of depressive symptoms is a score equal to 16 or above. Re-plot the data, but with a horizontal line to indicate this criterion. diet_long %&gt;% ggdotplot(x = &quot;time&quot;, y = &quot;symptoms&quot;, color = &quot;group&quot;, add = &quot;mean_se&quot;, ylim = c(0,65), position_dodge = 0.1) + geom_hline(yintercept = 16, colour = &quot;red&quot;, lty = 2) Figure 7.1: Mean depression symptom severity score in each condition. The dotted line indicates the criterion for the elevated range of depressive symptoms At baseline, was the mean of the diet_change group above or below the criterion for clinical significance in the severity of depression symptoms? abovebelow After 3 weeks, was the mean of the diet_change above or below the criterion for clinical significance in the severity of depression symptoms? abovebelow Note that these statements apply to the conditions as a whole, rather than the individuals in each condition. For instance, there may be individuals whose symptoms did not improve. To learn more about whether a given individual can be said to have improved or not, see the Further Knowledge section below. 7.7 Exercises Exercise 7.3 Pre-post self-efficacy scores Francis et al. (2019) also measured self-efficacy before and after the dietary intervention. Self-efficacy refers to a person's belief in their ability to exert control over their behaviours and circumstances they find themselves in. Higher scores indicate greater levels of self-efficacy. Read in the data at the link below to self_efficacy and answer the questions. https://raw.githubusercontent.com/chrisjberry/Teaching/master/7_diet_self_efficacy.csv 1. Create a plot of the mean self-efficacy before and after the intervention in each group Hint Read the data using read_csv() Convert to long format Use a suitable package to plot the data. For example, ggdotplot() in the ggpubr package. Solution # read in self_efficacy &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/7_diet_self_efficacy.csv&#39;) # convert to long format self_efficacy_long &lt;- self_efficacy %&gt;% pivot_longer(cols = c(&quot;baseline&quot;, &quot;week3&quot;), names_to = &quot;time&quot;, values_to = &quot;score&quot;) # plot self_efficacy_long %&gt;% ggdotplot(x = &quot;time&quot;, y = &quot;score&quot;, color = &quot;group&quot;, add = &quot;mean_se&quot;, ylab = &quot;Self-efficacy score&quot;) 2. Evidence for the dietary intervention on self-efficacy Using Approach 1, determine the Bayes factor representing the evidence for a difference in the change in self-efficacy score between the diet_change and habitual_diet groups. BF = Using Approach 2, determine the Bayes factor, representing the evidence for a difference in the change in self-efficacy score score between groups. BF = Using Approach 3, determine the Bayes factor, representing the evidence for a difference in self-efficacy scores between groups at week 3, after controlling for self-efficacy scores at baseline. BF = Is there substantial evidence for an effect of the dietary intervention on self-efficacy scores according to any of the approaches? Approach 1: ANOVA of difference scores. yesno Approach 2: Mixed ANOVA. yesno Approach 3: ANCOVA. yesno Hint - Approach 1 Use the data in wide format. Create a column of difference scores using mutate() Compare the difference scores between groups using anovaBF() (or lmBF() or ttestBF()). Hint - Approach 2 Convert the data to long format using pivot_longer(). Convert the columns labelling the participant and independent variables to factors using factor(). Use anovaBF() to run the 2 x 2 ANOVA, with the participant column as a random factor. Obtain the Bayes factor for the interaction. Hint - Approach 3 Use the data in wide format. Use lmBF() with the scores at week3 as the outcome variable and group and baseline as predictors. Use lmBF() with the scores at week3 as the outcome variable and baseline as the predictor. Divide the BF for the model with both group and baseline by the BF for the model with baseline alone in order to determine the evidence that the scores at week3 differ between group after controlling for baseline. Solution - Approach 1 # Approach 1 - difference self_efficacy &lt;- self_efficacy %&gt;% mutate(change = week3 - baseline, group = factor(group)) # using anovaBF anovaBF(change ~ group, data = data.frame(self_efficacy)) # or lmBF lmBF(change ~ group, data = data.frame(self_efficacy)) # or ttest ttestBF(self_efficacy$change[self_efficacy$group==&quot;habitual_diet&quot;], self_efficacy$change[self_efficacy$group==&quot;diet_change&quot;]) Solution - Approach 2 # Approach 2 - Mixed ANOVA # set factors self_efficacy_long &lt;- self_efficacy_long %&gt;% mutate(time = factor(time), ppt = factor(ppt), group = factor(group)) # get bfs BFs &lt;- anovaBF(score ~ group + time + ppt, whichRandom = &quot;ppt&quot;, data = data.frame(self_efficacy_long)) BFs[4] / BFs[3] # recompute BFs2 &lt;- recompute(BFs, 1000000) BFs2[4] / BFs2[3] Solution - Approach 3 # Approach 3 - ANCOVA # wide format # convert to factors self_efficacy &lt;- self_efficacy %&gt;% mutate(group = factor(group)) # specify group + baseline BF_group_baseline &lt;- lmBF(week3 ~ group + baseline, data = data.frame(self_efficacy)) # specify baseline only BF_baseline &lt;- lmBF(week3 ~ baseline, data = data.frame(self_efficacy)) # unique contribution group BF_group_baseline / BF_baseline 7.8 Further Knowledge Reliable Change RC index 7.8.1 Reliable Change (RC) index In the main part of the worksheet, the mean symptom severity in the dietary intervention group moved from being above the criterion for the elevated range before the intervention to being below the criterion afterwards. One way to determine whether a particular individual shows a clinically significant change involves calculating a Reliable Change (RC) index (Christensen &amp; Mendoza, 1986; Jacobson &amp; Trurax, 1991). The RC index can be calculated for each individual as follows: \\(RC = \\frac{X_1 - X_2}{SE_D}\\) where \\(X_1\\) is an individual's pretest score, \\(X_2\\) is the posttest score, and \\(SE_D\\) is the standard error of the difference scores. \\(SE_D\\) is calculated with the equation: \\(\\sqrt{2(SD_{Pre}\\sqrt{(1-\\alpha)})^2}\\) where \\(SD_{pre}\\) is the standard deviation of the pre-test scores and \\(\\alpha\\) is the reliability of the measure (e.g., Cronbach's alpha). \\(\\alpha\\) is necessary for the calculation of \\(SE_D\\), but was not reported by Francis et al. (2019). The CESD-R scale is widely used though, and Busch et al. (2011) derived an estimate of \\(SE_D\\) to be 5.05, using an \\(\\alpha\\) of .914 and \\(SD_{Pre}\\) score of 12.187. For the purposes of illustration, we'll assume the value of \\(SE_D\\) is the same as in Busch et al. (2011) (i.e., 5.05). If RC &gt; 1.96 (or RC &lt; -1.96, depending on the way the difference is calculated), then the individual can be said to show a reliable change. Thus, to calculate RC for each individual in the diet_change group: # filter the data for the diet_change group only diet_change &lt;- diet %&gt;% filter(group == &quot;diet_change&quot;) # SE of differences se_diff &lt;- 5.05 # from Busch et al. (2011) # Reliable Change index calculation diet_change &lt;- diet_change %&gt;% mutate(RC = change / se_diff) # preview diet_change %&gt;% head() ppt group baseline week3 change RC 1 diet_change 18.00000 11.00000 -7.0000000 -1.3861386 2 diet_change 34.00000 11.00000 -23.0000000 -4.5544554 3 diet_change 28.42105 3.00000 -25.4210526 -5.0338718 4 diet_change 25.00000 14.00000 -11.0000000 -2.1782178 5 diet_change 40.00000 49.00000 9.0000000 1.7821782 6 diet_change 15.00000 14.73684 -0.2631579 -0.0521105 RC scores less than -1.96 indicate that the individual's score showed a reliable improvement in depressive symptoms (i.e., reliably lower scores on the CESD-R). To see all the individuals who showed such a reduction: improved_ppts &lt;- diet_change %&gt;% filter(RC &lt; -1.96) improved_ppts ppt group baseline week3 change RC 2 diet_change 34.00000 11 -23.00000 -4.554455 3 diet_change 28.42105 3 -25.42105 -5.033872 4 diet_change 25.00000 14 -11.00000 -2.178218 8 diet_change 33.00000 9 -24.00000 -4.752475 23 diet_change 43.00000 30 -13.00000 -2.574257 28 diet_change 22.00000 8 -14.00000 -2.772277 33 diet_change 50.52632 23 -27.52632 -5.450756 34 diet_change 28.00000 17 -11.00000 -2.178218 36 diet_change 49.00000 14 -35.00000 -6.930693 37 diet_change 23.00000 10 -13.00000 -2.574257 7.8.1.1 Scatterplot with RC bounds All the data can be represented on a plot with bounds for reliable change as follows: diet_change %&gt;% ggplot(aes(x = baseline, y = week3)) + geom_point() + geom_abline() + geom_abline(slope = 1, intercept = -1.96*se_diff, lty = 2) + geom_abline(slope = 1, intercept = 1.96*se_diff, lty = 2) Figure 3.4: Baseline vs. week 3 depressive symptom severity in the diet change group. Dotted lines denoted Reliable Change index bounds The black dashed lines are those plotted with geom_abline() and are created by multiplying se_diff by +1.96 and -1.96. The points falling above the upper black dashed line indicate participants showing reliable worsening of depressive symptoms at the posttest compared to the the pretest. The points falling below the lower black dashed line are individuals showing a reliable improvement (i.e., fewer depressive symptoms) at the posttest. Francis et al. (2019) reported that 10 individuals showed a reliable improvement in the diet change group, as indicated by their RC scores. Only 4 individuals in the habitual diet group were reported to show a reliable improvement. Count how many individuals fall below the lower black dashed line on the scatterplot, and can therefore be considered to meet the criteria for showing reliable improvement, as defined by the RC index? 89101112 The same process could be followed for the habitual_diet group to compare the number of individuals showing reliable change according to the RC index between groups. The code for this group is below. With the value of se_diff we've used here, a fifth person in this group has an RC index (just) exceeding -1.96. There's still half as many participants showing a reliable change, compared to the diet_change group. The results obtained with RC index converge with those of the ANCOVA, and support the notion that a healthy diet can lead to improvement in the severity of depression symptoms. Habitual diet group # filter the data for the habitual_diet group only habitual_diet &lt;- diet %&gt;% filter(group == &quot;habitual_diet&quot;) # SE of differences se_diff &lt;- 5.05 # from Busch et al. (2011) # Reliable Change index calculation habitual_diet &lt;- habitual_diet %&gt;% mutate(RC = change / se_diff) # RC plot habitual_diet %&gt;% ggplot(aes(x = baseline, y = week3)) + geom_point() + geom_abline() + geom_abline(slope = 1, intercept = -1.96*se_diff, lty = 2) + geom_abline(slope = 1, intercept = 1.96*se_diff, lty = 2) Figure 3.5: Baseline vs. week 3 depressive symptom severity in the habitual diet group. Dotted lines denoted Reliable Change index bounds 7.9 Summary There are three main approaches for measuring pre-post data when there is a treatment and control group: Analysis of the difference scores in a between-subjects ANOVA. A 2 x 2 mixed ANOVA, with time (pre, post) as the within-subjects factor, and group (treatment, control) as the between-subjects factor. An ANCOVA comparing posttest scores between groups, after accounting for pretest scores as a covariate. This is equivalent to a multiple regression of posttest scores on the basis of pretest scores and group (i.e., one continuous and one categorical predictor). A result can be statistically significant, but not necessarily clinically significant. Various methods exist for establishing clinical significance, including using established scale cutoffs and using the Reliable Change Index. 7.10 References Busch, A. M., Wagener, T. L., Gregor, K. L., Ring, K. T., &amp; Borrelli, B. (2011). Utilizing reliable and clinically significant change criteria to assess for the development of depression during smoking cessation treatment: The importance of tracking idiographic change. Addictive Behaviors, 36(12), 1228-1232. Christensen, L. &amp; Mendoza, J. L. (1986). A method of assessing change in a single subject: an alteration of the RC index. Behavior Therapy. 17, 305-308. Francis H.M., Stevenson R.J., Chambers J.R., Gupta D., Newey B., Lim C.K. (2019) A brief diet intervention can reduce symptoms of depression in young adults – A randomised controlled trial. PLoS ONE, 14(10): e0222768. https://doi.org/10.1371/journal.pone.0222768 Jacobson, N. S., &amp; Truax, P. (1991). Clinical significance: A statistical approach to defining meaningful change in psychotherapy research. Journal of Consulting and Clinical Psychology, 59(1), 12–19. https://doi.org/10.1037/0022-006X.59.1.12 O'Connell, N. S., Dai, L., Jiang, Y., Speiser, J. L., Ward, R., Wei, W., ... &amp; Gebregziabher, M. (2017). Methods for analysis of pre-post data in clinical research: a comparison of five common methods. Journal of Biometrics &amp; Biostatistics, 8(1), 1. https://doi.org/10.4172/2155-6180.1000334 Radloff, L. S. (1977). The CES-D scale: A self-report depression scale for research in the general population. Applied Psychological Measurement, 1(3), 385-401. "],["mixed.html", "Session 8 Mixed Models 8.1 Overview 8.2 Types of mixed model 8.3 lme4 8.4 Fixed and Random effects 8.5 Specifying the model 8.6 The analysis: Do RTs differ across modality conditions? 8.7 Inspecting the model 8.8 Going further: Multiple factors 8.9 Summary 8.10 References", " Session 8 Mixed Models Chris Berry 2025 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} 8.1 Overview Slides from the lecture part of the session: Download R Studio online Access here using University log-in The use of mixed models is becoming more and more popular in the analysis of psychological experiments. This session is introductory, and for more detailed overviews, students should consult the Reference list. Mixed effects models or mixed models are an extension of multiple regression that allow the dependencies among groups of data points to be explicitly accounted for via additional random effects parameters. Dependency can arise because responses come from the same participant, stimulus set, or cluster (e.g., classroom or hospital). This can also be useful in experiments where researchers measure responses across a set of items. Rather than deriving a single score across all items or trials (e.g., an accuracy measure, or a mean), all the responses can be entered into the analysis and the variance associated with the responses to each item across participants explicitly modelled. This technique can also circumvent the need to discard participants or trials with missing responses from the analysis (provided data are missing at random). 8.2 Types of mixed model When the outcome variable is continuous (e.g., response time) the model is called a Linear Mixed Model (LMM). If the outcome is binary (e.g., correct vs. incorrect), the model is called a Generalised Linear Mixed Model (GLMM) and a different approach must be used to the one in this worksheet. (See the References for more detail.) 8.3 lme4 Mixed models can be run using the lme4 package (Bates et al., 2015). # load lme4 library(lme4) We will use data from Brown (2021): The data are from a response time (RT) experiment in which 53 participants were presented with a word on each trial. Each word was presented either auditorily (audio-only condition), or both auditorily and visually (they saw each word spoken in a video clip) (audiovisual condition). There were 543 words in total. The participant's task was to listen to and repeat back each word while simultaneously performing a response time task. Each participant was presented with words in both conditions. The data is publicly avaialble via the Open Science Framework at https://osf.io/download/53wc4/. Load the data and store it in rt_data. # load data from Brown (2021), located on OSF rt_data &lt;- read_csv(&quot;https://osf.io/download/53wc4/&quot;) Take a look at the first six rows of the data: # preview the data using head() rt_data %&gt;% head() PID RT modality stim 301 1024 Audio-only gown 301 838 Audio-only might 301 1060 Audio-only fern 301 882 Audio-only vane 301 971 Audio-only pup 301 1064 Audio-only rise About the data in each column: PID contains the unique identifier for each participant. RT contains the response time for each trial. modality contains the label for each condition (Audio-only or Audiovisual) stim contains the word that was presented on each trial Exercise 8.1 Design Check What is the name of the dependent variable? PIDRTmodalitystim Is modality of presentation manipulated within- or between-subjects? betweenwithin How many levels does modality have? 124 How many participants were there? 21,679535434 How many unique stimuli (words) were there? 21,679535434 What format is the data in rt_data in? wide formatlong format Hint: count() A simple way to obtain the number of participants in long format data is as follows: # n ppts # take rt_data, group_by ppt, then look at number of rows in tibble rt_data %&gt;% group_by(PID) %&gt;% count() Likewise, for the number of items: # n items # take rt_data, group_by item, then look at number of rows in tibble rt_data %&gt;% group_by(stim) %&gt;% count() 8.4 Fixed and Random effects Mixed models comprise fixed and random effects. Fixed effect: A population-level effect, assumed to persist across experiments with different participants or items. They are used to model average trends. Typically the independent variable is the fixed effect, for example, levels of difficulty in an experiment, manipulated by an experimenter. Random effect: Represents the extent to which the fixed effect randomly varies across a grouping variable (e.g., participants, or items). For example, the behaviour of individual participants or items may differ from the average trend (some may do better than others). The random effects are always categorical variables. They should have at least 5 or 6 levels. Exercise 8.2 For the Brown (2021) dataset: What would be the fixed effect? PIDRTmodalitystim What would be one random effect? PIDRTmodalitystim What would be a second random effect? PIDRTmodalitystim In this design, we are justified in including both participants and items as random effects because you can think of modality as having been manipulated both within-subjects and within-items: Within-subjects: The participants provided responses in both conditions. Within-items: Each item was also presented in both conditions. That is, for some participants a word was presented in the Audio-only condition and for other participants it appeared in the Audiovisual condition. In other words, we have scores for each participant in each condition, and also scores for each item in each condition. If the items in each condition were different, it wouldn't make sense to include item as a random effect. 8.5 Specifying the model A mixed model approach allows for the variability across items and participants to be modelled via random intercepts and random slopes. For each, you can have by-participant and by-item effects. Random intercepts: by-participant: accounts for individual differences in participants' RTs. by-item: accounts for differences in RTs to particular words. Random slopes: by-participant: accounts for individual differences in the effect of modality on RT. by-item: accounts for differences in the effect of modality on particular words. Note that if random slopes and intercepts are included, correlations between them will also be estimated. A correlation could occur, for example, if slower participants show larger effects of modality. Models with different combinations of random effects can be specified and compared to test whether there's evidence for particular components or not. 8.5.1 Fixed effect model with random intercepts The simplest random effects model has random intercepts only. The general formula is: lmer(outcome ~ predictor + (1|participant) + (1|item), data = data) Note that fixed effects are specified outside parentheses: predictor means include the fixed effect of predictor Random effects are specified inside parentheses: Things to the left of | vary according to things on the right. (1|participant) means the intercept (represented with '1') varies according to participant. (1|item) means the intercept (1) varies according to item. So, in words, the formula means \"model the outcome variable as a function of the fixed effect of the predictor, with random intercepts for participant and item.\" 8.5.2 Fixed effect model with random intercepts AND slopes The effect of modality can be allowed to differ across participants and items by including random slopes. The general formula is: lmer(outcome ~ predictor + (1 + predictor|participant) + (1 + predictor|item), data = data) Where: (1 + predictor|participant) means the intercept (1) and fixed effect (predictor) varies according to participant. (1 + predictor|item) means the intercept (1) and fixed effect (predictor) varies according to item. 8.5.3 Multiple fixed effects, random intercepts AND slopes For multiple fixed effects, e.g., predictor1 and predictor2, an example of the formula used could be: lmer(outcome ~ predictor1 + predictor2 + (1 + predictor1 + predictor2|participant) + (1 + predictor1|item), data = data) Where: predictor1 means include the fixed effect of predictor1. predictor2 means include the fixed effect of predictor2. (1 + predictor1 + predictor2|participant) means include random intercepts (1) for participant and also by-participant random slopes for predictor1 and predictor2. (1 + predictor1|item) means include random intercepts for item and by-item random slopes for predictor1. 8.6 The analysis: Do RTs differ across modality conditions? The researchers wanted to know whether RTs differ according to the modality condition. As covered in the lecture, the first step is to specify the maximal (full) model permitted by the design. Here, it's one that includes random intercepts and slopes for participant and stimulus. # Specify the full model and store in rt_full_model rt_full_model &lt;- lmer(RT ~ 1 + modality + (1 + modality|PID) + (1 + modality|stim), data = rt_data) This \"maximal\" model for the data includes: The fixed effect of modality By-participant and by-item random intercepts By-participant and by-item random slopes For comparison, we need to specify the reduced model, which is identical except it does not contain the fixed effect. # Specify the reduced model and store in rt_reduced_model rt_reduced_model &lt;- lmer(RT ~ 1 + (1 + modality|PID) + (1 + modality|stim), data = rt_data) To compare the full and reduced models use anova(model_1, model_2), that is, # compare the reduced and full models anova(rt_reduced_model, rt_full_model) This tests a null hypothesis that the full and reduced models are statistically equivalent. Specifically, a likelihood ratio test is performed, which involves obtaining the likelihood of each model and then comparing the likelihoods. The p-value is the probability of obtaining a test statistic at least as extreme as the one observed, if the null hypothesis is true. If the p-value for the test is less than 0.05, then, by convention, we say the test is statistically significant. The p-value is shown in the Pr(&gt;ChiSq) column and is a very small number (e.g., 1.26e-08), being clearly less than 0.05. This means that the full model fits the data significantly better than the reduced model, and we have evidence for the fixed effect. 8.7 Inspecting the model Now we know that we have evidence for the full model containing the fixed effect, we can look at it in detail: # display results from the full model summary(rt_full_model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: RT ~ 1 + modality + (1 + modality | PID) + (1 + modality | stim) ## Data: rt_data ## ## REML criterion at convergence: 302385.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.3646 -0.6964 -0.0141 0.5886 5.0003 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## stim (Intercept) 304.0 17.44 ## modalityAudiovisual 216.9 14.73 0.16 ## PID (Intercept) 28559.0 168.99 ## modalityAudiovisual 7709.0 87.80 -0.17 ## Residual 65258.8 255.46 ## Number of obs: 21679, groups: stim, 543; PID, 53 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 1044.14 23.36 52.12 44.700 &lt; 2e-16 *** ## modalityAudiovisual 83.18 12.57 52.10 6.615 2.02e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## modltyAdvsl -0.178 In the output: REML stands for Restricted Maximum Likelihood. This is the method that was used to fit the model to the data. The formula entered for the mixed model is given again. Information about the variance in the residuals is given. Random effects: The values in Std. Dev are the estimates of the variance in the random intercepts and slopes in participants (PID) and items (stim). The values in Corr are the estimates of the correlation between the random intercept and random slopes (by-participant and by-item). The correlation of 0.16 means that the effect was larger for items that took longer to respond to. The correlation of -0.17 means that the effect of modality was smaller in participants who took longer to respond. To interpret the fixed effects, we need to consider how the levels of modality were coded. R uses dummy coding (0s and 1s) to code the levels of a factor (or a character variable). modality has levels Audio-only and Audiovisual R assigns the 0s and 1s in dummy coding alphabetically. Audio-only is therefore coded with 0s. Audiovisual is therefore coded with 1s. Fixed effects: The estimate of the intercept is given as 1044.14. This represents the average RT in the Audio-only group (the condition coded with zeros). The estimate of the fixed effect of modality is given as 83.18. This represents how much greater the mean is in the Audiovisual condition (coded with 1s), once participant and items have been taken into account. The researchers in this experiment thought RTs would be slower in Audio-visual condition than the audio-only condition. Seeing a word in addition to hearing it may be associated with greater cognitive effort. Are the researchers correct or not? NoYes Explain Because the fixed effect of modality is 83.18, and because Audio-visual was (dummy) coded with 1s and Audio-only was coded with 0s, this means that the mean of the Audiovisual condition is 83.18 greater than that of the Audiovisual condition. The outcome variable is RT, which means that responses in the Audiovisual condition were 83 ms slower, on average, as expected by the researchers. 8.7.1 Convergence issues It is common when running LMMs for the model to not converge successfully, meaning that R was unable to estimate all the parameters of the model. This is especially common with more complex models, such as those with multiple fixed and random effects and their interactions. If you receive a warning saying that the model did not converge, or that the fit is singular, you should not report the results from the model. One solution is to explore all the fitting options in lmer() that do allow for convergence. Handily, this can be done for you automatically using the allFit() function in the afex package: allFit(rt_full_model) ## bobyqa : [OK] ## Nelder_Mead : [OK] ## nlminbwrap : [OK] ## optimx.L-BFGS-B : [OK] ## nloptwrap.NLOPT_LN_NELDERMEAD : [OK] ## nloptwrap.NLOPT_LN_BOBYQA : [OK] ## original model: ## RT ~ 1 + modality + (1 + modality | PID) + (1 + modality | stim) ## data: rt_data ## optimizers (6): bobyqa, Nelder_Mead, nlminbwrap, optimx.L-BFGS-B,nloptwrap.NLOPT_LN_NELDERME... ## differences in negative log-likelihoods: ## max= 0.00015 ; std dev= 6.08e-05 This will provide a list of the optimisers (e.g., bobyqa, Nelder_Mead) in lmer() that produce convergence warnings or singular fits or do successfully converge [OK]. An optimiser from this list can then be manually specified using lmerControl when running the model: rt_full_model &lt;- lmer(RT ~ 1 + modality + (1 + modality|PID) + (1 + modality|stim), data = rt_data, control = lmerControl(optimizer = &quot;bobyqa&quot;)) Other things that may improve convergence are: increase the number of iterations the optimisation routine performs (with control = lmerControl(optCtr = list(maxfun = 1e9))) remove the derivative calculations that occur after the model has reached a solution (via. control = lmerControl(calc.derivs = FALSE) ) If the model still does not converge, then the complexity of the model should be systematically reduced. For example, remove a component such as a random slope term and re-run it, repeating the process of removing more and more components until it converges. Ideally, the random effects you include in the model should be psychologically principled and guided by theory. 8.7.2 Coefficients For learning purposes only, let's see for ourselves that each participant in the final model has their own estimate of the intercept and slope, using coef() # get the random intercept and slope for each participant coef(rt_full_model)$PID %&gt;% head() (Intercept) modalityAudiovisual 301 1024.0672 -16.936244 302 1044.1377 1.843072 303 882.8292 57.790549 304 1232.7548 -27.919783 306 1042.3427 33.886178 307 1111.3621 -9.938361 See also the random intercepts and slopes for each item: # get random intercept and slope for each stimulus coef(rt_full_model)$stim %&gt;% head() (Intercept) modalityAudiovisual babe 1038.919 82.11841 back 1050.914 86.52439 bad 1041.122 81.12280 bag 1042.892 86.41081 bake 1039.394 81.75528 balk 1042.558 84.17974 You wouldn't report these, but looking at them helps to make it concrete that each participant and stimulus has its own intercept and effect of modality (slope) in this model. 8.7.3 Graph The emmeans() package can be used to obtain the means from the model for a figure. EMM stands for Estimated Marginal Means. These are means for conditions or contrasts, estimated by the model that was fit to the data. Obtain the estimates of mean RT in the two modality conditions: # load emmeans package library(emmeans) # obtain the emms from the model, for each level of modality emms &lt;- emmeans(rt_full_model, ~ modality) If you get a warning that tells us that it can't calculate the degrees of freedom, then don't worry since we don't need these right now. To see the EMMs: emms ## modality emmean SE df asymp.LCL asymp.UCL ## Audio-only 1044 23.4 Inf 998 1090 ## Audiovisual 1127 24.5 Inf 1079 1175 ## ## Degrees-of-freedom method: asymptotic ## Confidence level used: 0.95 The EMMs are in the emmean column. Associated standard errors are in SE. The values in the asymp.LCL column are the lower 95% confidence interval limits of the means. The values in the asymp.UCL column are the upper 95% confidence interval limits of the means. The estimated mean of the Audio-only condition is 104423.41127 The estimated mean of the Audiovisual condition is 1079112723.41044 To create a figure showing these means in ggplot(): # use emms, convert to tibble for ggplot, # plot means and errorbars as_tibble(emms) %&gt;% ggplot(aes(x = modality, y = emmean)) + geom_point(group = 1, size = 3) + geom_errorbar(aes(ymin = asymp.LCL, ymax = asymp.UCL, width = 0.2)) + ylab(&quot;Estimated marginal mean RT&quot;) + xlab(&quot;Condition&quot;) Figure 5.2: Estimated marginal mean RT in each modality condition, controlling for participant and stimulus. Error bars denote the 95% confidence interval. These means in the figure are technically based on estimates of parameters from the model. That is, they are the mean of the levels of the fixed effect after accounting for the random effects, and so won't necessarily be the same as the means you may calculate directly from the data. Traditional analysis For learning purposes only, let's compare what the means would have been when calculated in the traditional way (without accounting for the random effects). # average RTs across participants in each condition # then work out the mean across participants rt_data %&gt;% group_by(PID, modality) %&gt;% summarise(M = mean(RT)) %&gt;% ungroup() %&gt;% group_by(modality) %&gt;% summarise(M_RT = mean(M)) In this case, the means come out the same. Now see if the modality effect would be significant in a traditional paired t-test. Again, this is for learning purposes only - most LMMs would be more complex than only having a fixed factor with two levels! Ms &lt;- rt_data %&gt;% group_by(PID, modality) %&gt;% summarise(M = mean(RT)) %&gt;% pivot_wider(names_from=modality,values_from=M) t.test(Ms$`Audio-only`, Ms$Audiovisual, paired = TRUE) The pattern of significance is the same in the t-test and LMM (there's a significant effect of condition). 8.8 Going further: Multiple factors The analysis above was for a very simple scenario where the fixed factor had two levels. It can be extended to include an additional fixed factor and interaction between the factors. Load the extended data from Brown (2021) at \"https://osf.io/download/vkfzn/\" and store in rt_data_interaction. # load data with additional fixed factor rt_data_interaction &lt;- read_csv(&quot;https://osf.io/download/vkfzn/&quot;) ## Rows: 21679 Columns: 5 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): SNR, modality, stim ## dbl (2): PID, RT ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Exercise 8.3 The name of the column containing the new fixed factor is modalitystimSNRPIDRT The levels of the new factor are Easy/HardAudio-only/Audiovisual SNR stands for Signal to Noise Ratio. This manipulation involved changing the background noise to affect how difficult it was to hear the word on each trial. In the Easy condition, it was easy to hear the word against background noise. In the Hard condition, it was difficult to hear the word against the background noise. To specify the full model with the interaction between the two fixed factors: # Specify the LMM with an interaction term rt_int.mod &lt;- lmer(RT ~ 1 + modality + SNR + modality*SNR + (1 + modality + SNR|stim) + (1 + modality + SNR|PID), data = rt_data_interaction) ## boundary (singular) fit: see help(&#39;isSingular&#39;) Notice that the interaction was specified by adding modality:SNR. We could have alternatively used modality*SNR to add the interaction. Notice that the model includes random intercepts and slopes for each factor (by participant and by-item). Random effects for the interaction term have not been included. Doing so often leads to an oversaturated model and/or convergence issues. The researcher didn't include them. The model failed to converge and is singular. Use allFit() from the afex package to see if another optimizer will work. # This code will take a while to run # as allFit() tries each different optimizer! allFit(rt_int.mod) To help convergence, Brown (2021) removed components of the model relating to the random effects of items. Specifically: The by-item random slopes associated with each fixed factor were removed. The component (1 + modality + SNR| stim) becomes (1|stim). The correlation between the random intercept for stim and the by-stimulus random slope for modality was also removed using + (0 + modality|stim). They weren't interested in that correlation. The allFit() function indicated that the bobyqa optimizer led to convergence. Thus, their final model is: rt_int.mod &lt;- lmer(RT ~ 1 + modality + SNR + modality:SNR + (0 + modality|stim) + (1|stim) + (1 + modality + SNR|PID), data = rt_data_interaction, control = lmerControl(optimizer = &#39;bobyqa&#39;)) ## boundary (singular) fit: see help(&#39;isSingular&#39;) This model still doesn't converge here for us though! To help convergence, we'll try removing the + (0 + modality|stim) term: rt_int.mod &lt;- lmer(RT ~ 1 + modality + SNR + modality:SNR + (1|stim) + (1 + modality + SNR|PID), data = rt_data_interaction, control = lmerControl(optimizer = &#39;bobyqa&#39;)) This model now contains random intercepts (by-item and by-participant), and also by-participant random slopes for the effects of modality and SNR. The model converges! Now we can look at the model results using summary(): summary(rt_int.mod) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: RT ~ 1 + modality + SNR + modality:SNR + (1 | stim) + (1 + modality + ## SNR | PID) ## Data: rt_data_interaction ## Control: lmerControl(optimizer = &quot;bobyqa&quot;) ## ## REML criterion at convergence: 301138.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.5317 -0.6951 -0.0044 0.5970 4.8640 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## stim (Intercept) 395 19.88 ## PID (Intercept) 25527 159.77 ## modalityAudiovisual 8046 89.70 -0.03 ## SNRHard 10357 101.77 0.02 -0.47 ## Residual 61269 247.53 ## Number of obs: 21679, groups: stim, 543; PID, 53 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 998.829 22.218 52.745 44.957 &lt; 2e-16 *** ## modalityAudiovisual 98.510 13.190 58.894 7.469 4.39e-10 *** ## SNRHard 92.346 14.792 58.015 6.243 5.39e-08 *** ## modalityAudiovisual:SNRHard -29.556 6.756 21346.939 -4.374 1.22e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) mdltyA SNRHrd ## modltyAdvsl -0.063 ## SNRHard -0.015 -0.354 ## mdltyA:SNRH 0.074 -0.247 -0.233 Significance tests on the main effects and interaction are conducted using Satterthwaite's method, which is a way of estimating degrees of freedom to enable a hypothesis test to be conducted. There are significant main effects of modality, t(58.89) = 44.96, p &lt; .001, and SNR, t(58.01) = 6.24, p &lt; .001, and there's also a significant interaction between the factors, t(21346.94) = -4.37, p &lt; .001. It is possible to interpret the main effects and interaction by looking at the values of the coefficients (see Brown, 2021, p.14). An alternative way, however, is to obtain and then plot the EMMs of each condition: # obtain the EMMs for each condition emms_interaction &lt;- emmeans(rt_int.mod, ~modality*SNR) # look at the emms emms_interaction ## modality SNR emmean SE df asymp.LCL asymp.UCL ## Audio-only Easy 999 22.2 Inf 955 1042 ## Audiovisual Easy 1097 25.1 Inf 1048 1147 ## Audio-only Hard 1091 26.5 Inf 1039 1143 ## Audiovisual Hard 1160 26.1 Inf 1109 1211 ## ## Degrees-of-freedom method: asymptotic ## Confidence level used: 0.95 Now plot the means using ggplot(): # use ggplot to plot the emms as points, with line connectors, and errorbars as_tibble(emms_interaction) %&gt;% ggplot(aes(x = modality, y = emmean, color = SNR)) + geom_point(size = 2) + geom_line(aes(group = SNR)) + geom_errorbar(aes(ymin = asymp.LCL, ymax = asymp.UCL, width = 0.1)) + ylab(&quot;Estimated marginal mean RT&quot;) + xlab(&quot;SNR condition&quot;) + ylim(0,1300) Figure 8.1: Estimated marginal mean RT in each condition, controlling for participant and stimulus. Error bars denote the 95% confidence interval. This figure reveals the nature of the significant interaction. The effect of modality is greater in the Easy vs. Hard SNR condition (98 ms vs. 69 ms, calculated from the EMMs in emms_interaction). 8.9 Summary Mixed models provide a way of estimating fixed effects, while accounting for random effects of participants and items. They are becoming increasingly common in the psychological literature. Models can be more complex by having fixed effects with more than two levels, multiple fixed effects, and additional random effects. Random effects structures often need to be simplified to allow the model to be fit successfully. See Brown (2021) for an example with a binary response variable (e.g., accuracy = correct vs. incorrect); this analysis is called a generalised linear mixed model (GLMM). Jaeger (2008) also provides a tutorial. For more guidance on reporting mixed models, see Meteyard and Davies (2020). Interested students should consult the articles below. 8.10 References Recommended introductory texts: Brown, V. A. (2021). An introduction to linear mixed-effects modeling in R. Advances in Methods and Practices in Psychological Science, 4(1), Article 2515245920960351. https://doi.org/10.1177/2515245920960351 Singmann, H., &amp; Kellen, D.(2019). An Introduction to Mixed Models for Experimental Psychology. In D.H. Spieler &amp; E. Schumacher(Eds.), New Methods in Cognitive Psychology (pp.4–31).Psychology Press. [PDF link] Going further: Bates, D., Mächler, M., Bolker, B., &amp; Walker, S. (2015). Fitting Linear Mixed-Effects Models Using lme4. Journal of Statistical Software, 67, 1-48. [PDF link] Bolker, B. M. (2015). Linear and generalized linear mixed models. Ecological statistics: contemporary theory and application, 309-333. [PDF link] Bono, R., Alarcón R., &amp; Blanca M.J. (2021). Report quality of generalized linear mixed models in psychology: a systematic review. Frontiers in Psychology. 12:666182 [Article] Meteyard, L., &amp; Davies, R. A. (2020). Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112, 104092. [Article] Jaeger, T. F. (2008). Categorical data analysis: Away from ANOVAs (transformation or not) and towards logit mixed models. Journal of Memory and Language, 59(4), 434–446. [Article] "],["assessment2025.html", "Session 9 Data Analysis Assessment 2025", " Session 9 Data Analysis Assessment 2025 Chris Berry 2025 The instructions for the Analysis Assessment will be released in Session 3. They will appear in the \"Assessment\" section of the PSYC761 2024-25 DLE page. Answers to FAQs are on the following page. Slides for Rmd support * R Studio online Access here using University log-in "],["faqs.html", "Session 10 FAQs 10.1 What does it mean if RStudio prints out \"e-\" next to a number? 10.2 How can I find out more about a particular function, so I can explore its settings etc.? 10.3 How do I import a data file to R Studio? 10.4 How should I write up the report? 10.5 After downloading my .Rmd file to my computer, how do I open it again in RStudio? 10.6 On the word count, are figure captions included? 10.7 Should I make code visible in my html report? 10.8 Do I need to include references? 10.9 Can there be overlap in the variables used in each question? 10.10 Do we have to include a certain number of variables to achieve a good grade? 10.11 Can I include more than one figure? 10.12 Can I include categorical predictor variables in a hierarchical regression? 10.13 How do I apply for extenuating circumstances? 10.14 Can a book a slot in your office hours?", " Session 10 FAQs Chris Berry 2025 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} 10.1 What does it mean if RStudio prints out \"e-\" next to a number? It is a way of expressing very small numbers (when it's \"e-\") or very large numbers (when it's just \"e\"). If R prints \"1.4e-4\", in the output this actually means \"\\(1.4 \\times 10^{-4}\\)\", or \"0.00014\". \"e-4\" means \"move the decimal point 4 places to the left\". Run the code below -- it should return \"0.00014\". 1.4e-4 So: \"e-5\" means move the decimal point 5 places to the left (e.g., 2.5e-5 is \"\\(2.5 \\times 10^{-5}\\)\", or 0.000025) \"e-10\" means move the decimal point 10 places to the left (e.g., 2.5e-10 is \"\\(2.5 \\times 10^{-10}\\)\", or 0.00000000025). \"e5\" means move the decimal point 5 places to the right (e.g., 2.5e5 is \"\\(2.5 \\times 10^{5}\\)\", 250000). \"e10\" means move the decimal point 10 places to the right (e.g., 2.5e10 is \"\\(2.5 \\times 10^{10}\\)\", 25000000000). 10.2 How can I find out more about a particular function, so I can explore its settings etc.? For any function in R, if you run ?function_name in your code or at the console, then R will load a help file with details of the function with all its options and examples. Try, for example, running ?geom_point or ?mean. 10.3 How do I import a data file to R Studio? Your data file should be in a suitable format (e.g., \".csv\"). In the \"Files\" panel (lower right window in R Studio online), click \"Upload\". Choose the file to upload. It will then appear in the Files panel of R Studio online. Tip: In some cases, zipping a file prior to uploading it can prevent errors. 10.4 How should I write up the report? My guidance is to write this up in a similar style to that of a results section in an article, but you will obviously want to provide a bit of context to your analysis so you can explain what you’ve done. In the worksheets I’ve provided examples of interpretation of analyses, and reporting of statistics and you can use this to give you an idea of the kinds of things that could be mentioned when reporting the results. In the Rmd support lecture I provided guidance on how the knitted html document should look. 10.5 After downloading my .Rmd file to my computer, how do I open it again in RStudio? To open an .Rmd file that you’ve downloaded from RStudio, you must first upload it back to RStudio using the following steps: In the Files panel of RStudio (lower-right of the screen), click the Upload button. Then click “Choose File” and locate your .Rmd file on your computer. Once uploaded to RStudio, you can open it as normal from within the Files panel. Note. The reason why .Rmd files will not open when you try to open them once downloaded to your PC is because RStudio runs from a web browser. You must take the steps above to open a .Rmd file in RStudio. Alternatively, it is possible to open a .Rmd file downloaded to your PC if you have RStudio installed locally on your machine. 10.6 On the word count, are figure captions included? If the text falls within the figure image or table, then it won’t be counted, otherwise it will be. 10.7 Should I make code visible in my html report? There's no need to include code in your report. (If you do choose to though, the code won't count towards the word limit.) Please also see the Rmd support lecture for guidance on how the report should look (e.g., slide 16). 10.8 Do I need to include references? If you do cite something in your report, then you should include the reference to the article in a References section. There’s no expectation for you to consult the psychological literature concerning the topic of the questionnaire. Remember, this is primarily an assignment concerning data analysis and, as such, you aren’t expected to delve into the psychological literature, either in the formulation of your question or in the interpretation of your findings. Your analyses can be guided by what's said in assessment instructions and the Tips section of it. You may, of course, consult the psychological literature if you want to. You may want to and are encouraged to look at statistical articles or textbooks to inform your analyses. Please also see the “How should I write up the report?” FAQ above. 10.9 Can there be overlap in the variables used in each question? Yes, there may end up being overlap in the variables. There is no requirement in the assessment instructions for the variables in each question to be non-overlapping. There is also no requirement for Questions 1 and 2 to be explicitly linked or to follow on from one another. 10.10 Do we have to include a certain number of variables to achieve a good grade? As stated in the instructions (p.2), for Question 1, you must include at least one continuous predictor variable, and for Question 2, you must include at least one categorical variable. The instructions say that you may include more variables. If including more, then you will of course have to ensure that you report and interpret the results appropriately. 10.11 Can I include more than one figure? You can include more than one figure for each answer if you feel it is necessary, but it is not a requirement (please see assignment instructions, p.2). 10.12 Can I include categorical predictor variables in a hierarchical regression? Yes, this is possible. Please see the examples in Session 5 (slide 14 onwards, and worksheet section 5.4 onwards). 10.13 How do I apply for extenuating circumstances? All requests for extensions to the deadline are dealt with by the Faculty office, and need to applied for via the extenuating circumstances procedure detailed at the link below: https://liveplymouthac.sharepoint.com/sites/x70/SitePages/Extenuating-circumstances.aspx 10.14 Can a book a slot in your office hours? If you're still stuck after going through the materials in the workbook, the assessment instructions, and these FAQs and want to make an appointment to speak to me, then you can book a slot during my office hours using the online booking system link below: https://dle.plymouth.ac.uk/course/view.php?id=66067 "],["reading.html", "Session 11 Reading 11.1 Recommended texts and resources 11.2 Data preprocessing", " Session 11 Reading Chris Berry 2025 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} 11.1 Recommended texts and resources 11.1.1 Library books Gravetter, F. J., &amp; Forzano, L. B. (2015). Research Methods for the Behavioral Sciences (5th ed.). Belmont, CA: Wadsworth. [Library Link] Gravetter, F. J., &amp; Walnau, J. B. (2017). Statistics for the Behavioural Science (10th ed.). Cengage/Wadsworth, Boston. [Library Link] Howell, D. C. (2002). Statistical Methods for Psychology (5th ed.). Duxbury/Thomson Learning. [Library Link] Field, A., Miles, J., &amp; Field, Z. (2012). Discovering Statistics Using R [Library link to online book] 11.1.2 Electronic resources and books Wills, A. J. Research Methods in R (RminR): https://www.andywills.info/rminr/ Navarro, D. Learning Statistics with R. https://learningstatisticswithr.com/ Poldrack, R. A. Statistical Thinking for the 21st Century: https://statsthinking21.github.io/statsthinking21-core-site/ 11.2 Data preprocessing Wickham, H., Çetinkaya-Rundel, &amp; Grolemund, G., M. (2023). R for Data Science. O’Reilly Media. https://r4ds.hadley.nz/ See also the References section of each worksheet. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
