[
["index.html", "PSYC753 Data Fluency", " PSYC753 Data Fluency This workbook contains details of the following PSYC753 sessions given by Chris Berry: Building models 1 - 21/1/20 Building models 2 - 29/1/20 Fitting curves - 4/2/20 Materials for sessions by Ben Whalley can be found at https://benwhalley.github.io/datafluency/ "],
["assessments.html", "Assessments ", " Assessments "],
["data-analysis-and-visualisation-task.html", "Data analysis and visualisation task", " Data analysis and visualisation task This assignment is due in week 31 and counts 50% towards your module grade. Your task is to answer the questions below. Submitting your answers You should submit exactly 3 files: An rmd file A PDF document, produced by knitting your rmd file. As a separate document, upload a copy of the standard CW coversheet (and complete the feedback section). Within the Rmd file you should: Label each question clearly Where necessary, include comments explaining what specific lines of code do. Intersperse explanatory text with code chunks (don’t put everything in the same code chunk). Your rmd file should “Just Work” when the marker opens and runs it, and produce the same output as the knitted PDF file you submitted (i.e. there won’t be any errors or missing datafiles). If you work on your own computer at home, you should check your rmd file ‘knits’ correctly on the online Rstudio server. See common problems when trying to knit Rmd files Responses to frequently asked questions will be posted at this FAQ link Questions The number of marks each question is worth [out of 50] is given below. 1. [10 marks] Use dplyr functions like group_by and summarise to recreate the two tables below, from the gapminder::gapminder dataset. Mean Life Expectancy in Europe by Year year mean(lifeExp) 1952 64.4 1957 66.7 1962 68.5 1967 69.7 1972 70.8 1977 71.9 1982 72.8 1987 73.6 1992 74.4 1997 75.5 2002 76.7 2007 77.6 Country With the Highest GDP in 2007, by Continent continent country gdpPercap Africa Gabon 13206 Oceania Australia 34435 Americas United States 42952 Asia Kuwait 47307 Europe Norway 49357 2. [10 marks] Recreate the plot below, using the datasets::PlantGrowth data: Figure 1: Plant yield by treatment condition 3. [20 marks] Use read_csv to load the climate dataset held at https://bit.ly/3a1eF7w. The data are responses to a survey concerning people’s attitudes to climate change. These are the variables: sex 0 = male, 1 = female age age in years change To what extent do you agree or disagree with the statement: I can personally help to reduce climate change by changing my behaviour. 1 = strongly disagree…5 = strongly agree concern How concerned are you, if at all, about climate change, sometimes referred to as global warming? 1 = not concerned…4 = very concerned nuclear On a purely emotional level, how do you feel about nuclear power? 1 = very negative…5 = very positive exagerate To what extent do you agree or disagree with the statement: The seriousness of climate change is exaggerated. 1 = strongly disagree…5 = strongly agree hedonism How important to you is the gratification of desires, enjoyment in life, and self-indulgence? 1 = not important…5 = very important Fit a linear model to determine the extent to which change is predicted by concern. Report the results in APA format, include an appropriate plot of the data, and explain the results. Add nuclear, exagerate, and hedonism to the model in part a. Report the results from this multiple regression in APA format and explain the findings. Does the addition of these variables to the model improve the prediction of change? Using the model with concern, nuclear, exagerate, and hedonism as predictors, derive a prediction for a person who has scores of concern = 2, nuclear = 2.5, exagerate = 3, and hedonism = 3.5. 4. [10 marks] Is there sufficient evidence for a linear, quadratic or cubic component of age in the prediction of change? Explain your answer. "],
["building-models-1.html", "Building models 1", " Building models 1 In brief Models need to be appropriately complex. That is, we want to make models that represent our theories for the underlying causes of our data. Often this means adding many variables to a regression model. But we won’t always be sure which variables to add. Adding multiple variables also brings challenges. Where predictors are highly correlated (termed multicollinearity) then model results can be confusing. "],
["session-1-multiple-regression.html", "Session 1: Multiple regression", " Session 1: Multiple regression Overview So far, you have used regression to predict an outcome variable from a predictor variable. For example, can we predict academic performance from hours of study? You’ve also used it to determine whether the relation between two variables differs according to a categorical variable. Does the relation between academic performance and hours of study, for example, differ for men and women? We often want to determine the extent to which an outcome variable is predicted by several continuous predictors. For example, in addition to hours of study, a person’s IQ or academic interest might also predict their academic performance. We may want to add these predictors to a model because it may serve to improve the prediction of academic performance. Today, we will: learn how to conduct a multiple regression with several continuous predictor variables evaluate the regression model with statistics (\\(R^2\\), F-statistic, t-values) use Venn diagrams to help conceptualise the contribution of predictors to a model Simple vs. multiple regression Simple regression is a linear model of the relationship between one outcome variable and one predictor variable. For example, can we predict exam performance on the basis of IQ scores? Multiple regression is a linear model of the relationship between one outcome variable and more than one predictor variable. For example, can we predict exam performancebased on IQ scores and attendance at lectures? "],
["analysis-of-regression.html", "Analysis of regression", " Analysis of regression Suppose we want to construct a model to predict final university exam scores. This is the task faced by some admissions tutors! We’ll start off with a simple regression model, then work up to multiple regression. Load the ExamData dataset from https://bit.ly/37GkvJg. This contains exam scores for students taking a university course. (Make sure tidyverse is loaded first!) Learning tip Try typing out the code today if you usually cut and paste it to R! ExamData &lt;- read_csv(&#39;https://bit.ly/37GkvJg&#39;) ExamData %&gt;% head() &gt; # A tibble: 6 x 7 &gt; finalex entrex age project iq proposal attendance &gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &gt; 1 38 44 21.9 50 110 44 0 &gt; 2 49 40 22.6 75 120 70 0 &gt; 3 61 43 21.8 54 119 54 0 &gt; 4 65 42 22.5 60 125 53 0 &gt; 5 69 44 21.9 82 121 73 0 &gt; 6 73 46 21.8 65 140 62 0 These are the variables in ExamData: finalex: final examination marks entrex: entrance examination marks age: age in years project: dissertation project marks iq: IQ score proposal: dissertation proposal grade attendance: 1 = high attendance; 0 = low attendance First, let’s ask whether finalex is predicted by entrex. Plot these variables: ExamData %&gt;% ggplot(aes(x = entrex, y = finalex)) + geom_point() + geom_smooth(se=F, method=lm) There looks to be a positive association - students with higher entrance exam scores tend to have higher final exam scores. A good start! To conduct the simple regression with finalex as the outcome variable, and entrex as the predictor variable, use lm: m1 &lt;- lm(finalex ~ entrex, data = ExamData) Explanation: finalex ~ entrex can be read as “finalex is predicted by entrex”. The model is stored in m1. View the intercept of the regression line and the coefficient for entrex: m1 &gt; &gt; Call: &gt; lm(formula = finalex ~ entrex, data = ExamData) &gt; &gt; Coefficients: &gt; (Intercept) entrex &gt; -46.305 3.155 We can therefore write the regression equation: \\(Predicted\\ final\\ exam\\ score = -46.305 + 3.155(entrance\\ exam)\\) Use summary(m1) to display statistical analysis of the model: summary(m1) &gt; &gt; Call: &gt; lm(formula = finalex ~ entrex, data = ExamData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -54.494 -21.185 3.733 18.124 30.969 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) -46.3045 25.4773 -1.817 0.0788 . &gt; entrex 3.1545 0.5324 5.925 1.52e-06 *** &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 22.7 on 31 degrees of freedom &gt; Multiple R-squared: 0.531, Adjusted R-squared: 0.5159 &gt; F-statistic: 35.1 on 1 and 31 DF, p-value: 1.52e-06 Explanation of the output: Residuals: provides an indication of the discrepancy between the values of finalex predicted by the model (i.e., the regression equation) and the actual values of finalex. If the model does a good job in predicting finalex, the residuals should be relatively small. The difference between Min and Max gives us some idea of the range of error in the prediction of finalex scores. The difference in 3Q and 1Q is the interquartile range. The median of the residuals is 3.73. Coefficients: contains tests of statistical significance for each of the coefficients. The values in the column headed Pr(&gt;|t|) are the p-values associated with the t-values for the coefficients for each predictor. The t-values test a null hypothesis that the coefficients are equal to zero. A p-value less than .05 indicates that a predictor is statistically significant. The row for the (intercept) reports a t-test for whether the value of the intercept differs from zero. We’re not usually interested in this test (so don’t report it). The row for entrex tests whether the value of its coefficient (3.15) differs from zero. A coefficient of zero would be expected if the predictor explained no variance in the outcome variable. The coefficient for entrex (3.15) is clearly greater than zero. We can report this by saying that extrex is a statistically significant predictor of finalex, b = 3.15, t(31) = 5.92, p &lt; .001. Multiple R-squared: This is \\(R^2\\) - the proportion of variance in finalex explained by entrex. Here, \\(R^2\\) = 0.531. So approximately half of the variance in finalex is explained by entrex. It’s usually referred to simply as “R-squared” or \\(R^2\\). \\(R^2\\) is often reported as a percentage. To get this, simply multiply the value by 100. i.e., 0.531 x 100 = 53.10%. Adjusted R-squared: is an estimate of \\(R^2\\), but adjusted for the population. Despite the usefulness of this statistic, most studies still tend to report only the (unadjusted) \\(R^2\\) value. If reporting the Adjusted R-squared value, be sure to label it clearly as such. Here, Adjusted R-squared = 0.52. F-statistic: This compares the variance in finalex explained by the model with the variance that it does not explain (i.e., explained variance divided by unexplained variance). Higher values of F indicate that the model explains greater variance in an outcome variable. If the p-value associated with the F-statistic is less than .05, we can say that the model significantly predicts the outcome variable. Hence, we can say that a model consisting of entrex alone is a significant predictor of finalex, F(1, 31) = 35.10, p &lt; .001. Higher entrex scores tend to be associated with higher finalex scores. If our model did not explain any variance in finalex, we wouldn’t expect this to be statistically significant. In simple regression, the null hypothesis being tested on the F-statistic is that the slope of the regression line in the population is equal to zero. This is actually equivalent to the t-test on the entrex coefficient. So in simple regression, report the F-statistic for the overall regression or the t-test on the coefficient (not both). This equivalence between F and t does not hold true for multiple regression, as we shall see later. Now you have a go Run another simple regression: set finalex as the outcome variable and project as the predictor variable store the output in a variable with a different name (m2) then display the output of m2 using summary(). Try yourself first before clicking to show the code m2 &lt;- lm(finalex ~ project, data= ExamData) summary(m2) &gt; &gt; Call: &gt; lm(formula = finalex ~ project, data = ExamData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -64.015 -21.686 -0.573 21.758 70.427 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) 4.6968 40.1677 0.117 0.9077 &gt; project 1.4442 0.5861 2.464 0.0195 * &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 30.32 on 31 degrees of freedom &gt; Multiple R-squared: 0.1638, Adjusted R-squared: 0.1368 &gt; F-statistic: 6.072 on 1 and 31 DF, p-value: 0.01948 Answer the following: (report statistics to 2 decimal places) What is the value of the coefficient for project? What proportion of the variance in finalex is explained by project?: \\(R^2\\) = (or %). Write down the regression equation (on a bit of paper). Show me \\(Predicted\\ final\\ exam\\ score = 4.70 + 1.44(project)\\) Is project alone a statistically significant predictor of finalex, as indicated by the F-statistic? no yes Report the F-ratio in APA style, that is, in the form F(df1, df2) = F-statistic, p = p-value: Show me F(1, 31) = 6.07, p = .02 Individuals with lower higher project scores tended to have higher final exam scores. "],
["conceptualising-the-variance-explained-by-predictors.html", "Conceptualising the variance explained by predictors", " Conceptualising the variance explained by predictors Venn diagrams are useful for understanding the variance that predictors explain in the outcome variable. They are especially useful for understanding what’s going on in multiple regression. Suppose the rectangle below represents all of the variance in finalex to be explained. The area of the circle below represents the variance in finalex explained by entrex in the first simple regression we did. If this diagram were drawn to scale (it’s not), the area of the circle would be equal to the value of \\(R^2\\) (i.e., 53.1% of the rectangle). The part of the rectangle not inside the circle represents the variance in finalex that is not explained by the model (i.e., the unexplained or residual variance). To improve the model, we can explore whether adding in other predictors to the model explains additional variance, thereby increasing the total \\(R^2\\) of the model. You might think that we can simply add in variables (circles, above) to the model as we wish, until all the residual variance has been explained. This seems fine to do until we learn that if we were to add as many predictors to the model as there are rows in our data (33 individuals in our ExamData), then we’d perfectly predict the outcome variable, and have an \\(R^2\\) of 100%! This would be true even if the predictors consisted of random values. Our model would clearly be meaningless though. We ideally want to explain the outcome variable with relatively few predictors. "],
["adding-predictor-variables-to-the-model.html", "Adding predictor variables to the model", " Adding predictor variables to the model An issue that can arise when adding variables to a model is that predictors are usually correlated to some extent. This can make interpretation of multiple regressions tricky. For example, a predictor that is statistically significant in a simple regression may become non-significant in a multiple regression. Let’s see a demonstration of this! We’ll now add project to the model with entrex. First, check the correlation between predictors: ExamData %&gt;% select(entrex,project) %&gt;% cor() &gt; entrex project &gt; entrex 1.0000000 0.2908253 &gt; project 0.2908253 1.0000000 The correlation between entrex and project is r = Our predictor variables are weakly correlated. We should keep this in mind going forward. Now run a multiple regression to predict finalex from both entrex and project. Again, use lm but use the + symbol to add predictors to the model: m3 &lt;- lm(finalex ~ entrex + project, data = ExamData) summary(m3) &gt; &gt; Call: &gt; lm(formula = finalex ~ entrex + project, data = ExamData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -41.880 -16.617 4.636 15.562 35.273 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) -84.8289 33.6846 -2.518 0.0174 * &gt; entrex 2.8894 0.5406 5.344 8.81e-06 *** &gt; project 0.7515 0.4457 1.686 0.1021 &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 22.06 on 30 degrees of freedom &gt; Multiple R-squared: 0.5716, Adjusted R-squared: 0.5431 &gt; F-statistic: 20.02 on 2 and 30 DF, p-value: 3e-06 In this model with entrex and projectas predictors: What is the value of \\(R^2\\) (as a percentage): % By how much has \\(R^2\\) increased in this model, relative to the model with entrex alone (where \\(R^2\\) was 53.10%)? (as a percentage) (you will need to calculate this) % Is the overall regression model predicting finalex on the basis of entrex and project statistically significant? yes no Is entrex a statistically significant predictor of finalex? yes no We can report this in the following way: the t-test on the coefficient for entrex is statistically significant, b = 2.89, t(30) = 5.34, p &lt; .001. Is project a statistically significant predictor of finalex in this model? yes no What is the value of the coefficient for project? b = Report the t-statistic in APA style: Show me Project mark was not a statistically significant predictor of final examination in this model, b = 0.75, t(30) = 1.69, p = .10 Looking across the analyses we’ve performed, we can see that project is a (weak) but statistically significant predictor of finalex in a simple regression. However, when it is included in a model that also includes entrex it is not a significant predictor! What’s going on? The model containing only project explains 16.38% of the variability in finalex. The model containing only entrex explains 53.10% of the variability in finalex. However, a model containing both project and entrex only explains 57.16% of the variability in finalex, not 16.38 + 53.10 = 69.48%, as we might expect. This is because the predictors are correlated (r = .29) and so the variance they explain in finalex is shared. We could represent this on a Venn diagram as follows: The correlation is represented as an overlap in the circles. Their total area (57.16%) is therefore less than the area they’d explain if there were no overlap (69.48%) (i.e., if there was no correlation). This demonstrates an important point: The t-tests on the coefficients in a multiple regression assess the unique contribution of each predictor in the model. That is, they test the variance a predictor explains in an outcome variable, after the variance explained by the other predictors has been taken into account. This is why project is not statistically significant in the multiple regression model – it only explains a small amount of variance once entrex has been taken into account. It is possible to think of the F-statistic and t-value in multiple regression in terms of the Venn diagram: The F-statistic compares the explained variance with the unexplained variance. The explained variance is represented by the outline of the two circles in the Venn diagram above. The unexplained variance is the remaining blue area of the rectangle. The t-value compares the unique variance a predictor explains with the remaining unexplained variance. For example, for project in the Venn diagram above, this would be the area in the orange crescent, relative to the remaining blue area in the rectangle. "],
["multicolinearity.html", "Multicolinearity", " Multicolinearity If the correlation between predictors is very high (greater than r = 0.9), this is known as multicolinearity. On a Venn diagram, the circles representing the predictors would almost completely overlap. Multicolinearity can be a problem in multiple regression. Predictors may explain a large amount of variance in the outcome variable, but their ‘unique’ contribution in a multiple regression may be small. A situation can arise where neither predictor may be statistically significant even though the overall regression is significant! An example of multicolinearity in the ExamData dataset can be seen with the variables project and proposal. Obtain the correlation between project and proposal: Show me ExamData %&gt;% select(project, proposal) %&gt;% cor() &gt; project proposal &gt; project 1.0000000 0.9371487 &gt; proposal 0.9371487 1.0000000 The correlation between project and proposal is r = . To see the effects of multicolinearity, conduct a regression with finalex as the outcome variable and project and proposal as the predictor variables. Show me multi1 &lt;- lm(finalex ~ project + proposal, data = ExamData) summary(multi1) &gt; &gt; Call: &gt; lm(formula = finalex ~ project + proposal, data = ExamData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -64.287 -22.590 -0.346 22.395 70.289 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) 4.8784 40.8601 0.119 0.906 &gt; project 1.2751 1.7072 0.747 0.461 &gt; proposal 0.1826 1.7263 0.106 0.916 &gt; &gt; Residual standard error: 30.81 on 30 degrees of freedom &gt; Multiple R-squared: 0.1641, Adjusted R-squared: 0.1084 &gt; F-statistic: 2.945 on 2 and 30 DF, p-value: 0.06797 How much variance in finalex is explained by the model: \\(R^2\\) = %. Is the overall regression statistically significant? yes no Is the coefficient for project statistically significant? yes no Is the coefficient for proposal statistically significant? yes no Now run two simple regressions to determine whether project and proposal explain variance in finalex and are statistically significant predictors when in models on their own. Show me multi2 &lt;- lm(finalex ~ project, data = ExamData) summary(multi2) &gt; &gt; Call: &gt; lm(formula = finalex ~ project, data = ExamData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -64.015 -21.686 -0.573 21.758 70.427 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) 4.6968 40.1677 0.117 0.9077 &gt; project 1.4442 0.5861 2.464 0.0195 * &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 30.32 on 31 degrees of freedom &gt; Multiple R-squared: 0.1638, Adjusted R-squared: 0.1368 &gt; F-statistic: 6.072 on 1 and 31 DF, p-value: 0.01948 multi3 &lt;- lm(finalex ~ proposal, data = ExamData) summary(multi3) &gt; &gt; Call: &gt; lm(formula = finalex ~ proposal, data = ExamData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -64.987 -22.987 -1.378 24.059 68.921 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) 16.628 37.441 0.444 0.6601 &gt; proposal 1.391 0.598 2.326 0.0267 * &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 30.59 on 31 degrees of freedom &gt; Multiple R-squared: 0.1486, Adjusted R-squared: 0.1211 &gt; F-statistic: 5.409 on 1 and 31 DF, p-value: 0.02675 In a simple regression with finalex as the outcome variable, and project as the predictor variable, \\(R^2\\) = %. Is the overall regression statistically significant? yes no In a simple regression with finalex as the outcome variable, and proposal as the predictor variable, \\(R^2\\) = %. Is the overall regression statistically significant? yes no Try to explain what’s going on here in your own words. Click below or ask if you get stuck. Explain Interpretation Because proposal and project are highly correlated (r = 0.94), this gives rise to the situation where the simple regressions indicate that they explain variance in finalex, but when both are included as predictors in a multiple regression, it appears as if neither are significant predictors of finalex ! If this were a real scenario, we’d consider dropping project or proposal from the model. Because the correlation is so high, having one predictor is as good as having the other (more or less). It seems intuitive that a person’s final project mark would be highly correlated with their proposal mark. The take-home message here is to check for high correlations between your predictor variables before including them in a multiple regression. "],
["final-exercise.html", "Final exercise", " Final exercise As a final exercise, run a multiple regression to predict finalex from three predictors: entrex, project, and iq. Show me how multi2 &lt;- lm(finalex ~ entrex + project + iq, data = ExamData) summary(multi2) &gt; &gt; Call: &gt; lm(formula = finalex ~ entrex + project + iq, data = ExamData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -40.444 -16.174 5.509 14.312 33.338 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) -130.3803 54.7288 -2.382 0.023981 * &gt; entrex 2.6180 0.5978 4.379 0.000142 *** &gt; project 0.6874 0.4490 1.531 0.136620 &gt; iq 0.4862 0.4610 1.055 0.300214 &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 22.02 on 29 degrees of freedom &gt; Multiple R-squared: 0.5875, Adjusted R-squared: 0.5448 &gt; F-statistic: 13.77 on 3 and 29 DF, p-value: 9.168e-06 Which variables are statistically significant predictors of finalex? entrex yes no project yes no iq yes no On the basis of all the models conducted so far (with entrex, project, and iq), which model would you choose to best predict finalex? Tell me which model seems best The model containing entrex alone, as this seems to provide the simplest and most effective model of the finalex. A general goal of regression is to identify the fewest predictor variables necessary to predict an outcome variable, where each predictor variable predicts a substantial and independent segment of the variability in the outcome variable. "],
["summary-of-key-points.html", "Summary of key points", " Summary of key points Predictors can be added to a model in lm using the + symbol e.g., lm(finalex ~ entrex + project + iq) Predictor variables are often correlated to some extent. This can affect the interpretation of individual predictor variables. Venn diagrams help to understand the results. The F-statistic tells us whether the model as a whole significantly predicts the outcome variable. The t-values tell us whether individual predictors in the model are statistically significant. In multiple regression, it’s important to understand that the statistical significance of individual predictors only holds after taking into account the other predictors in the model. Multicolinearity exists when predictors are very highly correlated (r above 0.9) and should be avoided. "],
["faq-data-analysis-and-visualisation-task.html", "FAQ Data Analysis and Visualisation Task", " FAQ Data Analysis and Visualisation Task Frequently asked questions will appear here. "]
]
