[["index.html", "PSYC753 Data Fluency: Analysis Overview", " PSYC753 Data Fluency: Analysis Chris Berry Overview This workbook contains worksheets for the seven sessions given by Chris Berry: 17/01/22. Simple regression 24/01/22. Multiple regression 1: multiple continuous predictors 31/01/22. ANOVA 1 07/02/22. Multiple regression 2: one continuous, one categorical 14/02/22. Multiple regression 3: evaluating and comparing models (21/02/22. No session) 28/02/22. ANOVA 2 07/03/22. Pre-post data, effect sizes, clinically significant change It also contains details of: the Analysis Assessment (50% of module grade) (released in 3rd session) the assessment FAQs Use the left and right arrow keys to navigate this workbook Ben Whalley's materials from part 1 of PSYC753: https://benwhalley.github.io/datafluency/ The PSYC753 DLE page 2021-22: https://dle.plymouth.ac.uk/course/view.php?id=56923 "],["simple1.html", "Session 1 Simple Regression 1.1 Overview 1.2 Worked Example 1.3 Predicting 1.4 Residuals 1.5 Evaluating the model 1.6 Exercise 1.7 Further Exercises 1.8 Summary 1.9 References", " Session 1 Simple Regression Chris Berry 2022 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} 1.1 Overview Slides from the lecture part of the session: Download In simple regression, we create a linear (straight line) model of the relationship between one outcome variable and one predictor variable The outcome variable is what we want to predict or explain (e.g., anxiety scores) The predictor variable is what we use to predict the outcome variable (e.g., average hours of screen time per week) Both the outcome and predictor variable are continuous variables Regression is actually a more general technique that underpins a wide variety of analyses. Simple regression is the most basic form of regression. The simple regression equation has the form Predicted outcome = a + b(Predictor) a is the intercept, it is the height of the line. More formally, it's the value of the outcome when the predictor is equal to zero. b is the slope (or coefficient for the predictor variable). It determines the steepness of the line, or, more formally, the amount of change in the outcome variable for a one unit increase in the predictor variable. The goal of simple regression is to obtain the values of the intercept (a) and slope (b) so that the line 'fits' or 'goes through' our data as closely as possible. The specific method of 'fitting' the line to the data is called the method of least squares (described in the lecture). R can do this all automatically for us (with lm()). 1.2 Worked Example Is screen time linked to mental health? Teychenne and Hinkley (2016) used regression to investigate the association between anxiety and daily hours of screen time (e.g., TV, computer, or device use) in 528 mothers with young children. Read in the data from their study to R and store in mentalh: # ensure tidyverse is loaded library(tidyverse) # read the data to R using read_csv() mentalh &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/1_mental_health_data.csv&#39;) There are numerous variables in mentalh (use mentalh %&gt;% glimpse() to take a look). We will focus only on two here: anxiety_score: a score representing the number of anxiety symptoms experienced in the past week, and screen_time: hours per day of screen time use on average. (Note: the data are publicly available, but I've changed some of the variable names for clarity.) First, create a scatterplot of the two variables. Put the predictor variable on the x-axis, and the outcome variable on the y-axis. # The code below takes mentalh and pipes it to ggplot() # aes() is used to tell ggplot() to put # screen_time on the x-axis, and anxiety_score on the y-axis # The settings in geom_smooth() tell ggplot() to # add the regression line (method = &#39;lm&#39;) and # not show confidence intervals (se = FALSE) mentalh %&gt;% ggplot(aes(x = screen_time, y = anxiety_score)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = FALSE) Figure 1.1: Scatterplot of anxiety score according to screen time (hours per day) Exercise 1.1 Describe the relationship between screen time and anxiety evident in the scatterplot (pick one): Individuals with lower levels of screen time tend to have higher anxiety scores No association between screen time and anxiety scores is apparent Individuals with higher levels of screen time tend to have higher anxiety scores Use lm() to run the simple regression and store the results in simple1: # conduct a simple regression to predict anxiety_score from screen_time # store the results in simple1 simple1 &lt;- lm(anxiety_score ~ screen_time, data = mentalh) Explanation: To specify the regression equation, we use outcome_variable ~ predictor_variable.The ~ symbol is a tilde. We use it to specify certain formulas in R. When you see ~, you can read it as \"as a function of\". So, outcome variable ~ predictor variable means \"outcome variable as a function of the predictor variable\". In our case, \"anxiety_score as a function of screen_time\". The intercept (a) and slope (b) are automatically calculated by R and stored in simple1: # look at the results simple1 ## ## Call: ## lm(formula = anxiety_score ~ screen_time, data = mentalh) ## ## Coefficients: ## (Intercept) screen_time ## 5.5923 0.1318 Exercise 1.2 The value of the intercept a is 5.590.13 The value of the slope b for the screen_time predictor is 5.590.13 The regression equation (Predicted Outcome = a + b(Predictor)) can therefore be written as what? predicted screen time = 5.59 + 0.13(anxiety score) predicted anxiety score = 5.59 + 0.13(screen_time) predicted anxiety score = 0.13 + 5.59(screen_time) 1.3 Predicting The regression equation can be used for prediction. Suppose someone asked us what the anxiety_score would be for a new person whose screen_time score is 10 hours per week. By reading off from the regression line on the scatterplot from earlier, the anxiety_score looks to be around 7: Figure 1.2: Predicted anxiety score for a person with 10 hours screen time Using the regression equation, we can substitute 10 for screen_time, then calculate predicted anxiety_score more precisely. The augment() function in the broom package can be used to work out the prediction for new data automatically: # load the broom package library(broom) # store new scores as a tibble new_scores &lt;- tibble(screen_time = 10) # give new_scores to &#39;newdata&#39; option in augment() augment(simple1, newdata = new_scores) screen_time .fitted 10 6.91012 The predicted anxiety_score is in the .fitted column and is 6.91 Predictions for multiple individuals can also be made at once. Here we obtain the predictions for two people with screen_time scores of 10 and 15. # store the scores we want predictions for in new_scores new_scores &lt;- tibble(screen_time = c(10, 15)) # use augment() to obtain the predicted anxiety_scores augment(simple1, newdata = new_scores) screen_time .fitted 10 6.910120 15 7.569031 Each row shows one individual. Their predicted anxiety_scores are 6.91 and 7.57. 1.4 Residuals The residual for a given datapoint is its vertical distance from the regression line. It is the error in prediction of the outcome variable for that datapoint. Residual = Observed - Predicted To view the residuals, again use the augment() function in the broom package (this time without including newdata). The residual for each observation is given in the column .resid # look at the residuals for simple1 (in .resid) # pipe to head() to only show the first 6 rows augment(simple1) %&gt;% head() anxiety_score screen_time .fitted .resid .hat .sigma .cooksd .std.resid 7 2.571429 5.931167 1.068833 0.0022003 3.514985 0.0001024 0.304677 10 1.428571 5.780559 4.219441 0.0029659 3.510454 0.0021534 1.203238 13 4.214286 6.147666 6.852334 0.0019133 3.502526 0.0036559 1.953016 13 7.285714 6.552426 6.447574 0.0039508 3.503969 0.0067111 1.839532 3 18.571430 8.039682 -5.039682 0.0402420 3.508118 0.0449817 -1.464784 2 1.500000 5.789972 -3.789972 0.0029045 3.511390 0.0017011 -1.080735 Exercise 1.3 What was the residual for a person with anxiety_score equal to 13, and screen_time score equal to 7.29? 6.556.856.45 For this person, was the anxiety score predicted by the model overpredicted (too high)fit exactlyunderpredicted (too low) Explain The person has an anxiety_score of 13 and screen_time score of 7.29. The predicted anxiety_score for this datapoint is 6.55 (in the .fitted column), so the model underpredicts the observed value of anxiety_score. An assumption underlying regression is that the residuals are like random noise. When we plot the residual against the predicted values, there should be no trend evident in the datapoints in the plot. We can use this plot for checking this assumption of regression. # Create a plot of the predicted values vs. residuals # Use the &quot;.fitted&quot; and &quot;.resid&quot; columns in augment() # Use geom_hline() to draw a horizontal line at y = 0 augment(simple1) %&gt;% ggplot(aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0) Figure 1.3: Predicted anxiety score vs. the residual Explanation: If there's no trend in the residuals, we'd expect the points to look like a random cloud above and below the horizontal line (at y = 0). There should be no patterns, and the points should be pretty symmetrically distributed around a single point in the middle of the plot. There's some slight indication that the residuals tend to have lower values as the predicted values (.fitted) increase. In other words, there's some tendency for the model to overestimate anxiety_score as screen_time becomes more extreme. The residuals also seem more spread out above the horizontal at lower predicted values, but this doesn't look too serious and the plot seems okay. Issues here can indicate that improvement in the model is possible. 1.5 Evaluating the model 1.5.1 R2 R2 is a statistic that describes how well our model explains the outcome variable. It ranges between 0 and 1 and can be interpreted as the proportion of variance in the outcome variable that is explained by the predictor variable. To obtain R2 for the model: # use glance() to obtain R-squared glance(simple1) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.0148346 0.0129617 3.511952 7.920493 0.0050709 1 -1411.456 2828.913 2841.72 6487.581 526 528 The column r.squared contains R-squared for the model, and is equal to 0.0148. To report as a percentage, multiply by 100. This means that screen_time explains 1.48% of the variance in anxiety_score. In psychological research, this is a relatively small amount of variance to explain with a model. It may still be meaningful in some contexts though (e.g., where it may be better to have a model with some predictive power rather than none at all). In simple regression, R2 is actually the squared value of the Pearson correlation between the outcome and predictor variable: # load corrr package library(corrr) # calculate the Pearson correlation between screen_time and anxiety_score mentalh %&gt;% select(screen_time, anxiety_score) %&gt;% correlate(method = &quot;pearson&quot;) term screen_time anxiety_score screen_time NA 0.1217973 anxiety_score 0.1217973 NA The correlation between screen_time and anxiety_score is r = 0.1217973. 0.1217973 * 0.1217973 = 0.0148, which is equal to the R2 value obtained with glance() 1.5.2 Bayes Factor To further evaluate the model, we can obtain a Bayes Factor. The Bayes Factor tells us how much more likely the model is than one comprising the mean of the outcome variable only. We call this baseline model the intercept-only model. It is a model in which the regression line is a flat line (i.e., has a slope equal to zero), and the predictor does not predict the outcome at all. To compute the Bayes Factor, we use lmBF() in the BayesFactor package: # load BayesFactor package library(BayesFactor) # Compute the Bayes Factor lmBF(anxiety_score ~ screen_time, data = data.frame(mentalh)) ## Bayes factor analysis ## -------------- ## [1] screen_time : 4.465124 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS The Bayes Factor is 4.47. We'd report this as BF = 4.47. This BF means that a model consisting of screen_time alone as a predictor of anxiety_score is over four times more likely than an intercept-only model (in which screen_time has a zero-slope and so does not predict anxiety_score). In other words, there's sufficient evidence to say that screen_time predicts anxiety_score. Reporting the simple regression: A simple regression was conducted to model the number of anxiety symptoms reported in the past week (anxiety score) from average hours of screen time usage per day (screen time). Screen time was found to have a positive association with anxiety scores, whereby individuals who reported greater levels of screen time also tended to have greater anxiety scores. The regression equation was \"Predicted anxiety score = 5.59 + 0.13(screen time)\", indicating that every hour of screen time use was associated with an increase in 0.13 in the anxiety score. Screen time explained 1.30% of the variance in anxiety score (adjusted R2 value). The Bayes Factor, comparing the model against an intercept-only model, was 4.47, indicating substantial evidence for the model, with it being over four times more likely than an intercept-only model. 1.6 Exercise Exercise 1.4 Is screen time predicted by age? In addition to screen time, Teychenne and Hinkley (2016) also asked participants their age in years, recorded in age in the mentalh dataset. Let's explore whether age predicts screen_time using simple regression. Adapt the code in this worksheet to do the following: 1. Produce a scatterplot of age vs. screen_time Hint Pipe mentalh to ggplot() and use geom_point() and geom_smooth(). Put the new predictor variable (age) on the x-axis and the outcome variable (screen_time) on the y-axis. Solution mentalh %&gt;% ggplot(aes(x = age, y = screen_time)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = FALSE) + xlab(&quot;Age&quot;) + ylab(&quot;Screen time (hours)&quot;) Describe the relationship between age and screen time in the scatterplot (pick one): Older individuals tend to have higher screen time scores Older individuals tend to have lower screen time scores No association between age and screen time appears to be present 2. Conduct a simple regression, with screen_time as the outcome variable, and age as the predictor variable Hint Use lm() to specify the simple regression Solution simple2 &lt;- lm(screen_time ~ age, data = mentalh) simple2 What is the value of the intercept a (to two decimal places)? What is the value of the slope b (to two decimal places)? What is the regression equation? Predicted screen time = 7.48 - 0.10(age) Predicted screen time = 0.10 - 7.48(age) Predicted screen time = 7.48 + 0.10(age) 3. Obtain R-squared Hint Make sure you have stored the regression results (e.g., in simple2), then use glance() with those results Solution glance(simple2) What proportion of variance in the screen time is explained by age? (Report the adjusted R-squared value, to two decimal places) Report the value of adjusted R-squared as a percentage, to two decimal places: The adjusted R2 value is equal to % 4. Obtain the Bayes Factor for the model Hint Use lmBF() to specify the model Solution simple2_BF &lt;- lmBF(screen_time ~ age, data = data.frame(mentalh)) simple2_BF How many times more likely is the model with age as a predictor of screen_time, compared to an intercept-only model? (to two decimal places) 5. Produce a plot of the fitted (predicted) values against the residuals Hint Use augment() with ggplot() and geom_point() Solution augment(simple2) %&gt;% ggplot(aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0) + geom_smooth() What type of trend is evident between the predicted values and the residuals? Further interpretation No association is apparent, but the points above the line appear to be more spread out than the points below the horizontal line. This indicates that the model tends to underestimate some of the screen time scores. This could be because the screen time scores are positively skewed, e.g., see mentalh %&gt;% ggplot(aes(screen_time)) + geom_density(), and therefore taking the log transform of the scores prior to analysis may improve this plot (though may not necessarily change the outcome of the analysis). 6. On balance, does age seem to be a good predictor of a person's daily screen time use? No Yes Cannot determine Explanation Yes, the older the individuals were, the lower the screen time score tended to be. A model with age as a predictor of screen time explained only 1.68% of the variance in screen time scores (adjusted R2), but the Bayes Factor (BF = 12.19) indicated strong evidence for this model compared to an intercept-only model. The regression equation was \"Predicted screen time = 7.48 - 0.10(age)\", indicating that an increase in age of one year was associated with a reduction of approximately 6 minutes (i.e., one tenth of 1 hour) of screen time per week. 1.7 Further Exercises For those feeling confident with everything so far. Exercise 1.5 Further Exercise The variable physical_activity in the mentalh dataset is a measure of moderate-to-vigorous physical activity, based on participant's self reported weekly activity. To what extent is participants' anxiety_score explained by their physical_activity? Investigate by producing the following: Scatterplot Correlation Simple regression model Adjusted R-squared value Bayes Factor On balance, does the anxiety_score seem to be predicted by physical_activity? Solution: code # scatterplot mentalh %&gt;% ggplot(aes(x = physical_activity, y = anxiety_score)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = F) + xlab(&quot;Physical activity&quot;) + ylab(&quot;Anxiety score&quot;) + theme_classic() # correlation mentalh %&gt;% select(anxiety_score, physical_activity) %&gt;% correlate() # simple regression model model_activity &lt;- lm(anxiety_score ~ physical_activity, data = mentalh) # look at intercept and slope model_activity # look at plot of fitted values and residuals augment(model_activity) %&gt;% ggplot(aes(x=.fitted, y=.resid)) + geom_point() + geom_hline(yintercept = 0) # look at R-squared glance(model_activity) # calculate Bayes Factor lmBF(anxiety_score ~ physical_activity, data = mentalh) Solution: interpretation No, there's no evidence that the anxiety scores are predicted by self reported measures of moderate-to-vigorous levels of physical activity. The two measures showed virtually no correlation, r = -0.01. The regression equation was Predicted Anxiety Score = 6.23 - 0.0001(physical activity), and the model explained no variance in anxiety score with the adjusted R2 = -0.0017. The Bayes Factor was equal to 0.10. Given that this value of the Bayes Factor is less than 0.33, this indicates substantial evidence for the intercept-only model, compared to the simple regression model where physical activity is the sole predictor of anxiety scores. In other words, if we only had these two variables, the best predictor of anxiety scores would be the mean value of the anxiety scores. Interestingly, although there appears to be no relationship between anxiety and physical activity in this sample of individuals (mothers), other populations do apparently show reductions in anxiety with greater levels of vigorous physical activity (e.g., in adolescents, see Hrafnkelsdottir et al., 2018). 1.8 Summary Simple regression can be used to model the relationship between an outcome and predictor variable, where both variables are continuous. Once obtained, the regression equation allows us to: precisely describe the relationship between the outcome and predictor variables (whether positive or negative). derive predictions for the outcome variable, given new values of the predictor variable. evaluate the model with R2 and use Bayes factor to compare how much more likely it is to an intercept-only model Key functions Visualise the data: ggplot() Simple regression: lm() R2: glance() Residuals: augment() Bayes Factor: lmBF() 1.9 References Hrafnkelsdottir S.M., Brychta R.J., Rognvaldsdottir V., Gestsdottir S., Chen K.Y., Johannsson E., et al. (2018) Less screen time and more frequent vigorous physical activity is associated with lower risk of reporting negative mental health symptoms among Icelandic adolescents. PLoS ONE 13(4): e0196286. https://doi.org/10.1371/journal.pone.0196286 Teychenne M, &amp; Hinkley T (2016) Associations between screen-based sedentary behaviour and anxiety symptoms in mothers with young children. PLoS ONE, 11(5): e0155696. https://doi.org/10.1371/journal.pone.0155696 "],["multiple1.html", "Session 2 Multiple regression: multiple continuous predictors 2.1 Overview 2.2 Worked example 2.3 Understanding the contribution of individual predictors 2.4 Multicollinearity 2.5 Exercise 2.6 Further exercises 2.7 Summary of key points 2.8 References", " Session 2 Multiple regression: multiple continuous predictors Chris Berry 2022 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} 2.1 Overview Slides from the lecture part of the session: Download This worksheet assumes you have gone through the previous one on simple regression. When we want to determine the extent to which an outcome variable (e.g., psychological wellbeing) is predicted by multiple continuous predictors (e.g., both worry and mindfulness scores), we can use multiple regression. Adding multiple predictors to a model may serve to improve the prediction of the outcome variable. It can also be a way to test specific theories or hypotheses. Simple vs. Multiple Regression Simple regression is a linear model of the relationship between one outcome variable and one predictor variable. For example, can we predict wellbeing on the basis of worry scores? Multiple regression is a linear model of the relationship between one outcome variable and more than one predictor variable. For example, can we predict wellbeing based on worry, mindfulness, and emotional intelligence scores? 2.2 Worked example Iani et al. (2019) looked at factors associated with psychological wellbeing and distress in 66 individuals with generalised anxiety disorder. For educational purposes, we'll focus on a subset of their data, namely whether wellbeing is predicted by worry and describing scores in a multiple regression. Describing is the mindfulness skill of being able to describe one's inner experiences and feelings with words. Read the data to R. The data are stored at: https://raw.githubusercontent.com/chrisjberry/Teaching/master/2_wellbeing_data.csv # First ensure tidyverse is loaded, i.e., &#39;library(tidyverse)&#39; # read in the data using read_csv(), store in wellbeing_data wellbeing_data &lt;- read_csv(&#39;https://raw.githubusercontent.com/chrisjberry/Teaching/master/2_wellbeing_data.csv&#39;) # preview the data with glimpse() wellbeing_data %&gt;% glimpse() We'll use these three variables in the dataset: wellbeing: Higher scores indicate higher wellbeing. worry: Higher scores indicate higher levels of worry. describing: Higher scores indicate higher self-reported ability to describe one's inner experiences. (Note. The data are publicly available, but I've changed the variable names for clarity. As in Iani et al. (2019), missing values were replaced with the mean of the relevant variable.) Visualise the data with a scatterplot. Place the outcome variable wellbeing on the y-axis, the predictor worry on the x-axis, and let the size of each point represent the second predictor, the describing score: # Scatterplot of all three variables # Use alpha to alter transparency of points # to make overlap easier to see wellbeing_data %&gt;% ggplot(aes(x = worry, y = wellbeing, size = describing)) + geom_point(alpha = 0.6, colour = &quot;cornflowerblue&quot;) Figure 2.1: Scatterplot of wellbeing scores vs. worry and describing Exercise 2.1 From inspection of the scatterplot: Greater worry scores tend to be associated with lowerconstanthigher wellbeing scores. Explain A negative trend between worry and wellbeing is evident in the scatterplot Greater describing scores tend to be associated with lowerconstanthigher wellbeing scores. Explain The size of the describing points tend to be larger when wellbeing scores are higher. The above trends are also apparent in the Pearson correlations between variables: # select the relevant columns from wellbeing_data and # use correlate() to obtain the correlations wellbeing_data %&gt;% select(wellbeing, worry, describing) %&gt;% correlate(method = &quot;pearson&quot;) term wellbeing worry describing wellbeing NA -0.5419352 0.5356548 worry -0.5419352 NA -0.2477970 describing 0.5356548 -0.2477970 NA The correlation between wellbeing and worry (to 2 decimal places) is r = The correlation between wellbeing and describing (to 2 decimal places) is r = The correlation between the two predictors (worry and describing) (to 2 decimal places) is r = Multiple regression using lm() To include more than one predictor in a regression model, use the + symbol when specifying the model with lm(): lm(outcome ~ predictor_1 + predictor_2 + predictor_3.... , data = mydata) This runs a model of the form: \\(Predicted \\ outcome = a + b_1(Predictor \\ 1) + b_2(Predictor \\ 2) + b_3(Predictor \\ 3) ...\\) Note that we don't need to specify the intercept a in lm() since it is included automatically by R (as is the case with simple regression). # conduct a multiple regression, store it in multiple1 multiple1 &lt;- lm(wellbeing ~ worry + describing, data = wellbeing_data) # look at the coefficients multiple1 ## ## Call: ## lm(formula = wellbeing ~ worry + describing, data = wellbeing_data) ## ## Coefficients: ## (Intercept) worry describing ## 70.7306 -0.7708 1.2484 (Intercept) is the value of the intercept a in the regression equation. Type to two decimal places: worry is the value of the coefficient \\(b_1\\) for the worry predictor. describing is the value of the coefficient \\(b_2\\) for the describing predictor. The regression equation is therefore written as: \\(Predicted\\ wellbeing = 70.73 - 0.77(worry) + 1.25(describing)\\) 2.2.1 Residual plot We can obtain a plot of the predicted values vs. the residuals in the same way as for simple regression by using augment() in the broom package. augment(multiple1) %&gt;% ggplot(aes(x = .fitted, y = .resid)) + geom_hline(yintercept = 0) + geom_point() Figure 2.2: Scatterplot of the predicted values vs. residuals The points seem randomly and evenly distributed around the horizontal, in line with assumptions of homoscedasticity (equal variance of residuals at each predicted value), and independence of residuals. 2.2.2 Evaluating the model: Bayes Factor The Bayes Factor tells us how many more times likely the multiple regression model is, relative to the intercept-only model. Use lmBF() to obtain the Bayes Factor for the multiple regression model: # store the BF for the model in multiple1_BF multiple1_BF &lt;- lmBF(wellbeing ~ worry + describing, data = data.frame(wellbeing_data)) # show the BF multiple1_BF ## Bayes factor analysis ## -------------- ## [1] worry + describing : 4190994 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS The Bayes Factor for the model is . This tells us that the model with worry and describing is over four million times more likely than an Intercept only model (one with no predictors). Thus, there's strong evidence that the multiple regression model explains wellbeing. 2.2.3 Evaluating the model: R2 R2 tells us how much variance in the outcome variable is explained by the multiple regression model. Use glance() in the broom package to obtain the R2 for the model: glance(multiple1) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.4653263 0.4483526 9.393314 27.41444 0 2 -239.9547 487.9093 496.6679 5558.763 63 66 r.squared is R2, the proportion of variance in wellbeing explained by the model. Thus, the model explains 0.4656, or 46.56% of the variance in wellbeing. adj.r.squared is the Adjusted R2 value, which is R2 adjusted for the sample size and the number of predictors in the model. It is an estimate of R2 for the population (not merely the scores we have in the sample), and is always less than R2. You'll see researchers reporting either R2 or the adjusted R2 in the literature. If you're not sure which one to use, report the adjusted R2, and say so (e.g., \"adjusted R2 = 44.83%\"). The adjusted R2 value is 0.4483, so in a report we could say that a model with worry and describing explains 44.83% of the variance in wellbeing. 2.3 Understanding the contribution of individual predictors Because predictor variables are often correlated to a degree, some of the variance they explain in the outcome will be shared. A predictor's contribution to a model must therefore only be interpreted after the other predictors in the model have been taken into account. This is explained in more detail below. 2.3.1 R2 in simple vs. multiple regression In a simple regression of wellbeing ~ worry, the variance in wellbeing explained by worry is R2 = 29.37%: s1 &lt;- lm(wellbeing ~ worry, data = wellbeing_data) glance(s1) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.2936938 0.2826578 10.71152 26.61226 2.6e-06 1 -249.1416 504.2832 510.8522 7343.15 64 66 In a simple regression of wellbeing ~ describing, the variance in wellbeing explained by describing is R2 = 28.69% s2 &lt;- lm(wellbeing ~ describing, data = wellbeing_data) glance(s2) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.286926 0.2757842 10.76272 25.75226 3.6e-06 1 -249.4563 504.9126 511.4816 7413.512 64 66 So, by looking at the (non-adjusted) R2 values of our simple and multiple regressions so far, an interesting pattern is evident: In the simple regression of wellbeing ~ worry, R2 = 29.37% In the simple regression of wellbeing ~ describing, R2 = 28.69% Yet, in a multiple regression of wellbeing ~ worry + describing, R2 = 46.56%, which is less than the sum of R2 from the simple regressions (29.37 + 28.69 = 58.06%). The reason why the R2 values from the simple regressions don't add up to the same value as the R2 for the multiple regression is because worry and describing are correlated (r = -0.25, as you obtained earlier). This means that some of the variance that they explain in wellbeing is shared. 2.3.2 Venn Diagrams Venn diagrams are useful for understanding the variance that predictors explain in the outcome variable. They are especially useful for understanding R2 in multiple regression. Here, I use them to explain why the R2 values from simple regressions don't necessarily add up to R2 when the same predictors are in a multiple regression. Suppose the rectangle below represents all of the variance in wellbeing to be explained. The area of the circle below represents the variance in wellbeing explained by worry in the first simple regression. If this diagram were drawn to scale (it's not!), the area of the circle would be equal to the value of \\(R^2\\) (i.e., 29.37%). The part of the rectangle not inside the circle represents the variance in wellbeing that is not explained by the model (i.e., the unexplained or residual variance). We'll now add describing to the model with worry. We could represent this on a Venn diagram as follows: The correlation is represented as an overlap in the circles. Their total area (R2 for the multiple regression model = 46.56%) is less than the area they'd explain if there were no overlap (58.06%). This highlights an important point: Predictors are often correlated to some degree, so in multiple regression it only really makes sense to talk about the contribution a predictor makes in the context of the other predictors in the model. That is, a given predictor explains variance in the outcome variable only after the other predictors in the model have been taken into account. Given that the unique contributions of worry and describing are lower in a multiple regression model, we must ask whether there's evidence that each predictor makes a unique contribution, over an above the other predictor. For example, is there sufficient evidence for a unique contribution of worry, once describing has been taken into account? If not, we probably wouldn't include it in the model. 2.3.3 Using Bayes factors to assess the unique contribution of predictors We can compare Bayes Factors to determine whether a given predictor in a multiple regression model makes a unique contribution to the prediction of the outcome variable. First, obtain the Bayes Factor for the model in which predictor_1 has been left out of the full model. Then obtain the Bayes Factor for model in which predictor_2 has been left out of the model. Repeat this for as many predictors you have in the model. In our example, this involves obtaining the BFs of the model of wellbeing ~ worry, and the BF of the model of wellbeing ~ describing: # BF for wellbeing ~ worry worry_BF &lt;- lmBF(wellbeing ~ worry, data = data.frame(wellbeing_data)) # BF for wellbeing ~ describing describing_BF &lt;- lmBF(wellbeing ~ describing, data = data.frame(wellbeing_data)) We also need the BF for the full model: # BF for wellbeing ~ worry + describing # (this is the same as multiple1_BF above, but # with a different name, which will help us see what&#39;s going on later) worry_describing_BF &lt;- lmBF(wellbeing ~ worry + describing, data = data.frame(wellbeing_data)) Comparing models We can use the following general formula to determine whether there's evidence for a more complex version of a model, relative to a simpler model: BF_more_complex_model / BF_simpler_model That is, we divide the BF for the more complex model by the BF for the simpler one. This then tells us how many more times more likely the more complex model is, relative to the simpler one. For example, if BF_more_complex_model = 10 and BF_simpler_model = 2, then the more complex model is five times more likely than the simpler one (because 10 / 2 = 5). There'd be strong evidence to prefer the more complex model. In our case, we can compare Bayes Factor of the full model (worry_describing_BF) with that of our simpler models (worry_BF) and (describing_BF) in order to determine whether each predictor makes a unique contribution to the full model or not. To determine if there's evidence for the unique contribution of describing to the model: # compare the BFs for the full model and one in which describing is left out worry_describing_BF / worry_BF ## Bayes factor analysis ## -------------- ## [1] worry + describing : 738.4095 ±0% ## ## Against denominator: ## wellbeing ~ worry ## --- ## Bayes factor type: BFlinearModel, JZS The BF comparing the full model with one containing worry alone is 738.41. This indicates that the model with both worry and describing is over seven hundred times more likely than the model with worry alone. The BF is greater than 3, so this indicates substantial evidence for the unique contribution of describing to the prediction of wellbeing. To determine if there's evidence for the unique contribution of worry to the model: # compare BFs for the full model and one in which worry is left out worry_describing_BF / describing_BF ## Bayes factor analysis ## -------------- ## [1] worry + describing : 981.4912 ±0% ## ## Against denominator: ## wellbeing ~ describing ## --- ## Bayes factor type: BFlinearModel, JZS The BF for the comparison is 981.49, indicating that there's substantial evidence that the addition of worry to a model containing describing improves the model. In other words, there's evidence for a unique contribution of worry to the prediction of wellbeing. Thus, in a multiple regression model, there's substantial evidence that both worry and describing predict wellbeing overall (BF = 4,190,994). There's also substantial evidence that describing (BF = 738.41) and worry (BF = 981.49) make unique contributions to this model. The model with both predictors included therefore seems justified. 2.4 Multicollinearity If the correlation between predictors is very high (greater than r = 0.8 or less than -0.8), this is known as multicollinearity. Multicollinearity can be a problem in multiple regression. Predictors may explain a large amount of variance in the outcome variable, but their 'unique' contribution in a multiple regression may be small. In an extreme scenario, neither predictor makes a unique contribution, even though the overall regression explains a large amount of variance in the outcome variable! On a Venn diagram, the circles representing the predictors would almost completely overlap: Multicollinearity should be avoided. Therefore, check for extreme correlations between your predictor variables before including them in a multiple regression. The correlation between the predictor variables in the model of wellbeing ~ worry + describing was , and multicollinearity was therefore of concernof no concernnot possible to determine. How many predictor variables should be included in a model? If adding predictors to the regression improves the prediction of the outcome variable, you may think that we can simply keep adding in variables to the model, until all the residual variance has been explained. This seems fine until we learn that if we were to add as many predictors to the model as there are rows in our data (e.g., 66 participants inwellbeing_data), then we'd perfectly predict the outcome variable, and have a (non-adjusted) \\(R^2\\) of 100%! This would be true even if the predictors consisted of random values. Our model would clearly be meaningless though. We ideally want to explain the outcome variable with relatively few predictors, or for our selection of predictors to be guided by specific hypotheses or theory. 2.5 Exercise Exercise 2.2 Main Exercise The wellbeing_data also contain these columns: gad : Severity of symptoms of GAD (Generalised Anxiety Disorder). Higher scores indicate greater severity of symptoms. brooding : Higher scores indicate higher levels of brooding (i.e., being preoccupied with depressing, morbid, or painful memories or thoughts). observing : Higher scores indicate greater tendency to notice things in one's environment (e.g., smells, sounds, visual elements). It's another measure of mindfulness. Use multiple regression to investigate the extent to which brooding and observing predict gad. Adapt the code in this worksheet to do the following things (and try not to look at the solution until you've attempted the question): 1. Visualise the relationship between gad, brooding and observing in a scatterplot. Hint Pipe wellbeing_data to ggplot() and use geom_point(). Use the size option to specify the second predictor variable in aes() Solution Figure 2.3: GAD vs. brooding and observing scores In the scatterplot, observing could have also been swapped around with brooding. Try both ways. Greater brooding scores tend to be associated with lowerno appreciable change inhigher gad scores. Greater observing scores tend to be associated with lowerno appreciable change inhigher gad scores. 2. Obtain the correlations between gad, brooding, and observing Hint Pipe the wellbeing_data to select() and use correlate() Solution wellbeing_data %&gt;% select(gad, brooding, observing) %&gt;% correlate(method = &quot;pearson&quot;) State the correlations to two decimal places: The correlation between the GAD score and brooding is r = The correlation between the GAD score and observing is r = The correlation between brooding and observing is r = Is multicollinearity a concern between the two predictor variables? yesnocannot determine Explanation The correlation between the predictor variables is r = -.18. Although they are weakly correlated, this does not exceed r = -.80, and therefore multicolinearity is not a concern. 3. Conduct a multiple regression, with gad as the outcome variable, and brooding and observing as the predictor variables Hint Use lm() to specify the simple regression. Store it in multiple2. Solution multiple2 &lt;- lm(gad ~ brooding + observing, data = wellbeing_data) multiple2 What is the value of the intercept a (to two decimal places)? What is the value of the coefficient for brooding (to two decimal places)? What is the value of the coefficient for observing (to two decimal places)? What is the regression equation? Predicted GAD score = 0.07 - 0.89(brooding) + 0.02(observing) Predicted GAD score = 0.07 + 0.02(brooding) - 0.89(observing) Predicted GAD score = 0.07 + 0.89(brooding) - 0.02(observing) 4. Obtain R2 for the model Hint Make sure you have stored the regression results (e.g., in mutliple2), then use glance() with that variable. Solution glance(multiple2) What proportion of variance in the GAD score is explained by the model containing brooding and observing? (Report the adjusted R-squared value as a proportion, to two decimal places) Report the value of adjusted R-squared as a percentage, to two decimal places: The adjusted R2 value is equal to % 5. Obtain the Bayes Factor for the model Hint Use lmBF() to specify the multiple regression model Solution multiple2_BF &lt;- lmBF(gad ~ brooding + observing, data = data.frame(wellbeing_data)) How many times more likely is the model with brooding and observing as a predictor of gad, compared to an intercept-only model? (to two decimal places) 6. Plot the predicted values against the residuals. Hint Use augment() with ggplot() and geom_point() Solution augment(multiple2) %&gt;% ggplot(aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0) What type of trend is evident between the predicted values and the residuals positive trendthere's no associationnegative association Further interpretation Although there's a slight negative association for lower predicted values, the assumptions of homoscedasticity and independence of residuals appear to be met. you could also try adding the code + geom_smooth(method = \"lm\", se = FALSE) to your plot to see what the (linear) trend line would actually be. 7. Using lmBF(), determine whether there's evidence for a unique contribution of brooding and observing to prediction of gad The BF associated with the unique contribution of observing is The BF associated with the unique contribution of brooding is Hint 1 Use lmBF() to obtain the BF for both simple regressions and the multiple regression. Then use the general formula BF_more_complex_model / BF_simpler_model to obtain the BF for the unique contribution of each predictor. Hint 2 To get the BF for the two simple regressions: BF_brooding &lt;- lmBF(gad ~ brooding, data = data.frame(wellbeing_data)) BF_observing &lt;- lmBF(gad ~ observing, data = data.frame(wellbeing_data)) The multiple regression BF (same as multiple2_BF): BF_brooding_observing &lt;- lmBF(gad ~ brooding + observing, data = data.frame(wellbeing_data)) Then the BFs for relevant models need to be compared, using: BF_more_complex_model / BF_simpler_model Solution # BF for gad ~ brooding BF_brooding &lt;- lmBF(gad ~ brooding, data = data.frame(wellbeing_data)) # BF for gad ~ observing BF_observing &lt;- lmBF(gad ~ observing, data = data.frame(wellbeing_data)) # BF for full model BF_brooding_observing &lt;- lmBF(gad ~ brooding + observing, data = data.frame(wellbeing_data)) # BF for the unique contribution of observing BF_brooding_observing / BF_brooding # BF for the unique contribution of brooding BF_brooding_observing / BF_observing 8. On balance, is there evidence for a model containing both brooding and observing as predictors of GAD scores? Hint Look at BF for the unique contributions. If the BF is greater than 3, this indicates substantial evidence for the inclusion of that predictor in the model. If the BF is less than 0.33, then there's substantial evidence against its inclusion (the simpler model is preferable). Intermediate BFs are inconclusive, and the predictor should therefore be left out. Solution Together, brooding and observing explain 34.50% of the variance in GAD scores (adjusted R2), and there was strong evidence for this model, compared to an intercept only model, BF = 26889.69. There was, however, no evidence for a unique contribution of observing to the model (BF = 0.19), but there was substantial evidence for a unique contribution of brooding to the model, BF = 741841.28, whereby greater GAD scores were associated with higher brooding scores. In the interest of parsimony (not making models more complex than they need to be), the results suggest that only brooding predicts GAD and observing should not be included in the model that also contains brooding. 2.6 Further exercises Not essential. For those confident with everything. 2.6.1 regressionBF() When determining the BF for the unique contribution of a predictor, as a shortcut, we could have used regressionBF(), rather than using lmBF() multiple times. regressionBF() automatically calculates the BFs for all permutations of a set of predictors in a model. For our first model of wellbeing on the basis of worry and describing: # obtain BFs for all permutations of the model predictors all_BFs &lt;- regressionBF(wellbeing ~ worry + describing, data = data.frame(wellbeing_data)) # look at the BFs all_BFs ## Bayes factor analysis ## -------------- ## [1] worry : 5675.703 ±0% ## [2] describing : 4270.027 ±0% ## [3] worry + describing : 4190994 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS \"[1]\" is the BF for the model with worry as the sole predictor of wellbeing \"[2]\" is the BF for the model with describing as the sole predictor of wellbeing \"[3]\" is the BF for the model with worry and describing as predictors Comparing the BF for [3] and [1] will therefore tell us whether there's evidence that describing makes a unique contribution to the model: # compare the BF for [3] and [1] all_BFs[3] / all_BFs[1] ## Bayes factor analysis ## -------------- ## [1] worry + describing : 738.4095 ±0% ## ## Against denominator: ## wellbeing ~ worry ## --- ## Bayes factor type: BFlinearModel, JZS The BF for this comparison is 738.41, which matches the BF for the unique contribution of describing calculated earlier. Comparing the BF for [3] and [2] will tell us whether there's evidence that worry makes a unique contribution. # compare multiple regression with simple regression 2 all_BFs[3] / all_BFs[2] ## Bayes factor analysis ## -------------- ## [1] worry + describing : 981.4912 ±0% ## ## Against denominator: ## wellbeing ~ describing ## --- ## Bayes factor type: BFlinearModel, JZS The BF for this comparison is 981.49, which matches the BF for the unique contribution of describing calculated earlier. Thus, the BFs derived by this method are the same as when we used lmBF() in the main section of the worksheet. Exercise 2.3 Further exercise 1 Use regressionBF() to obtain the BFs for the unique contribution of brooding and observation in the model in the Main Exercise (i.e., gad ~ brooding + observation), and check that they are the same as the ones produced when you used lmBF() 2.6.2 Predicting new data As with simple regression, we can use augment() to predict what the outcome variable would be, given new data. For example, for a new individual with a worry score of 20 and describing score of 15: # specify the new data (for each predictor in the model) new_scores &lt;- tibble(worry = 20, describing = 15) # use augment() in the broom package to obtain the prediction library(broom) augment(multiple1, newdata = new_scores) worry describing .fitted 20 15 74.03992 .fitted contains the predicted wellbeing score, and is equal to 74.04. To derive predictions for several new participants, use c(score1, score2...) when specifying the new_scores. To add another individual with a worry score of 25 and describing score of 10 # specify the new data (for both predictors) new_scores &lt;- tibble(worry = c(20, 25), describing = c(15, 10)) # use augment() in the broom package for the predictions augment(multiple1, newdata = new_scores) worry describing .fitted 20 15 74.03992 25 10 63.94410 Each row contains the details for a new person. The .fitted column contains the predicted wellbeing scores: 74.04 and 63.94. Exercise 2.4 Predicting data For the model in the main exercise (i.e., gad ~ brooding + observing), what are the predicted values of gad for three new individuals with brooding scores of 10, 15, and 20, and observing scores of 3, 6 and 9, respectively. Predicted GAD of participant 1 (to two decimal places) = Predicted GAD of participant 2 (to two decimal places) = Predicted GAD of participant 3 (to two decimal places) = Solution new_scores &lt;- tibble(brooding = c(10, 15, 20), observing = c(3, 6, 9)) multiple2 &lt;- lm(gad ~ brooding + observing, data = wellbeing_data) augment(multiple2, newdata = new_scores) Exercise 2.5 Further exercise Not essential. Only for if you are really confident with everything. The variables attention, clarity and repair in the wellbeing_data are measures of emotional intelligence. To what extent do these three predictors uniquely explain wellbeing in a multiple regression? The BF for the unique contribution of attention = The BF for the unique contribution of clarity = The BF for the unique contribution of repair = Hint Obtain the BF for the full model and the BFs for the models in which each of the predictors have been left out. Compare the BF for the full model with that of the model with the predictor left out in order to obtain the BF for the unique contribution of that predictor. Solution Check correlations: wellbeing_data %&gt;% select(wellbeing, attention, clarity, repair) %&gt;% correlate(method = &quot;pearson&quot;) ## ## Correlation method: &#39;pearson&#39; ## Missing treated using: &#39;pairwise.complete.obs&#39; term wellbeing attention clarity repair wellbeing NA -0.3162625 0.3755731 0.5002038 attention -0.3162625 NA 0.1560582 0.0216476 clarity 0.3755731 0.1560582 NA 0.4078774 repair 0.5002038 0.0216476 0.4078774 NA   Run multiple regression: multiple3 &lt;- lm(wellbeing ~ attention + clarity + repair, data = wellbeing_data) Adjusted R2 for the overall model: glance( multiple3 ) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.4167062 0.3884823 9.889914 14.76431 2e-07 3 -242.8268 495.6536 506.6019 6064.245 62 66 Using lmBF(): BF_attention_clarity_repair &lt;- lmBF(wellbeing ~ attention + clarity + repair, data = data.frame(wellbeing_data)) BF_attention_clarity &lt;- lmBF(wellbeing ~ attention + clarity, data = data.frame(wellbeing_data)) BF_attention_repair &lt;- lmBF(wellbeing ~ attention + repair, data = data.frame(wellbeing_data)) BF_clarity_repair &lt;- lmBF(wellbeing ~ clarity + repair, data = data.frame(wellbeing_data)) # unique contribution of attention BF_attention_clarity_repair / BF_clarity_repair # unique contribution of clarity BF_attention_clarity_repair / BF_attention_repair # unique contribution of repair BF_attention_clarity_repair / BF_attention_clarity ## Bayes factor analysis ## -------------- ## [1] attention + clarity + repair : 81.00941 ±0% ## ## Against denominator: ## wellbeing ~ clarity + repair ## --- ## Bayes factor type: BFlinearModel, JZS ## ## Bayes factor analysis ## -------------- ## [1] attention + clarity + repair : 3.836891 ±0% ## ## Against denominator: ## wellbeing ~ attention + repair ## --- ## Bayes factor type: BFlinearModel, JZS ## ## Bayes factor analysis ## -------------- ## [1] attention + clarity + repair : 82.35924 ±0% ## ## Against denominator: ## wellbeing ~ attention + clarity ## --- ## Bayes factor type: BFlinearModel, JZS Using regressionBF(): all_BFs &lt;- regressionBF(wellbeing ~ attention + clarity + repair, data = data.frame(wellbeing_data)) # look at output all_BFs # unique contribution of attention all_BFs[7] / all_BFs[6] # unique contribution of clarity all_BFs[7] / all_BFs[5] # unique contribution of repair all_BFs[7] / all_BFs[4] ## Bayes factor analysis ## -------------- ## [1] attention : 4.759631 ±0% ## [2] clarity : 18.08091 ±0.01% ## [3] repair : 960.1706 ±0% ## [4] attention + clarity : 874.9426 ±0% ## [5] attention + repair : 18780.73 ±0% ## [6] clarity + repair : 889.5214 ±0% ## [7] attention + clarity + repair : 72059.61 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS ## ## Bayes factor analysis ## -------------- ## [1] attention + clarity + repair : 81.00941 ±0% ## ## Against denominator: ## wellbeing ~ clarity + repair ## --- ## Bayes factor type: BFlinearModel, JZS ## ## Bayes factor analysis ## -------------- ## [1] attention + clarity + repair : 3.836891 ±0% ## ## Against denominator: ## wellbeing ~ attention + repair ## --- ## Bayes factor type: BFlinearModel, JZS ## ## Bayes factor analysis ## -------------- ## [1] attention + clarity + repair : 82.35924 ±0% ## ## Against denominator: ## wellbeing ~ attention + clarity ## --- ## Bayes factor type: BFlinearModel, JZS 2.7 Summary of key points Predictors can be added to a model in lm using the + symbol e.g., lm(wellbeing ~ worry + describing + predictor_3 + ....) Predictor variables are often correlated to some extent. This can affect the interpretation of individual predictor variables. Venn diagrams can help to understand the unique contributions of predictors In multiple regression, it's important to understand that each predictor makes a contribution to explaining the outcome variable only after taking into account the other predictors in the model. The Bayes Factor for a multiple regression model tells us how much more likely it is, compared to an intercept-only model. To know whether there's evidence for the unique contribution of a predictor, we have to compare Bayes Factors for the more complex model (which includes it), versus the simpler model (in which the predictor is left out). i.e., BF_more_complex_model / BF_simpler_model. Multicollinearity exists when predictors are highly correlated (r above 0.8 or below -0.8) and should be avoided by dropping one of the predictors. 2.8 References Iani, L., Quinto, R. M., Lauriola, M., Crosta, M. L., &amp; Pozzi, G. (2019). Psychological well-being and distress in patients with generalized anxiety disorder: The roles of positive and negative functioning. PloS one, 14(11), e0225646. https://doi.org/10.1371/journal.pone.0225646 "],["anova1.html", "Session 3 ANOVA: between-subjects designs", " Session 3 ANOVA: between-subjects designs Chris Berry 2022 "],["mutliple2.html", "Session 4 Multiple regression: one continuous, one categorical", " Session 4 Multiple regression: one continuous, one categorical Chris Berry 2022 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} "],["multiple3.html", "Session 5 Multiple regression: evaluating and comparing models", " Session 5 Multiple regression: evaluating and comparing models Chris Berry 2022 "],["anova2.html", "Session 6 ANOVA: Repeated measures", " Session 6 ANOVA: Repeated measures Chris Berry 2022 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} "],["prepost.html", "Session 7 Pre-post data, effect sizes, clinically significant change", " Session 7 Pre-post data, effect sizes, clinically significant change Chris Berry 2022 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} "],["assessment2022.html", "Session 8 Data Analysis Assessment 2022", " Session 8 Data Analysis Assessment 2022 Chris Berry 2022 "],["faqs.html", "Session 9 FAQs 9.1 What does it mean if RStudio prints out \"e-\" next to a number?", " Session 9 FAQs Chris Berry 2022 div.exercise { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} div.tip { background-color:#D5F5E3; border-radius: 5px; padding: 20px;} 9.1 What does it mean if RStudio prints out \"e-\" next to a number? If R prints \"1.4e-4\", in the output this actually means \"\\(1.4 \\times 10^{-4}\\)\", or \"0.00014\". It is a way of printing out very small (or very large) numbers. \"e-4\" means \"move the decimal point 4 places to the left\". Run the code below -- it should return \"0.00014\". 1.4e-4 So: \"e-5\" means move the decimal point 5 places to the left (e.g., 2.5e-5 is \"\\(2.5 \\times 10^{-5}\\)\", or 0.000025) \"e-10\" means move the decimal point 10 places to the left (e.g., 2.5e-10 is \"\\(2.5 \\times 10^{-10}\\)\", or 0.00000000025). \"e5\" means move the decimal point 5 places to the right (e.g., 2.5e5 is \"\\(2.5 \\times 10^{5}\\)\", 250000). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
