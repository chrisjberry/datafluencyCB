[
["index.html", "Data Fluency", " Data Fluency Ben Whalley and Chris Berry "],
["overview.html", "Overview", " Overview From the module aims: The module aims to foster fluency and confidence in the handling, visualisation and communication of quantitative data, alongside skills and techniques for working with larger corpuses of textual and other data. Data visualisation is taught as a foundational technique for exploring and communicating insights from quantitative data. Existing knowledge of linear models is extended with the introduction of the generalised linear model, and a contemporary approach, emphasising prediction and model evaluation is introduced. In a nutshell: we want to give you the skills to analyse your data as independent researchers, and to give you confidence in working with data which will stand you in good stead in your future careers. "],
["approach-of-this-course.html", "Approach of this course", " Approach of this course Sometimes, psychology students learn statistics through a “bag of tricks” approach: Workshops might teach how to “do an Anova”, or “how run a multiple regression”. Or you might be given a checklist of things to do when analysing data of a particular type, but all without any bigger picture of what we are trying to achieve when we collect and analyse data. To provide a common thread to our teaching, research methods modules at Plymouth adopt the model for the work of data scientists proposed by Wickham, 2017 (see figure): Wickham’s model of a data science workflow In this module we do cover specific skills, but we hope you will also learn about this general approach to working with data, and integrate it into your own research. "],
["format-of-the-sessions.html", "Format of the sessions", " Format of the sessions We have 16 sessions, which work as follows: We avoid extended lectures. This doesn’t work well with this subject matter. The focus is on learning by doing (this is more like cooking than chemisty). In the first hour of each session we will (often) work together. In the second hour your work will be self-paced, or in pairs or small groups. Activities in the workshops are variable in length, sometimes you will finish early, other times you may be expected to complete the activities outside of class. "],
["the-most-important-thing-of-all.html", "The most important thing of all", " The most important thing of all The most important thing of all is to practice. These materials provide lots of practice tasks. You NEED to work through them all to be able to pass the course effectively. Lab diary/R project archive It’s recommended that you keep a running note of all the work you do in class. This can take for form of a notebook in Word, a blog, or an R script (see first session). Without a running record of what you have/haven’t done it’s much harder for teaching staff to help you. The record also allows us to review your progress and make suggestions/improvements. "],
["list-of-sessions.html", "List of sessions", " List of sessions Don’t worry if all the terms in this list don’t make sense yet … they will soon! BeginneRs 1: Getting started with RStudio BeginneRs 2: Basic skills in R Visualisation and plotting: Relationships, group differences, color. Real world plotting Data handling: Working with multiple files; Rmarkdown. Regression 1: Core concepts. Regression 2: Causes/Multiple regression. Regression 3: Tests and variable/effect coding. Uncertainty and intervals: Confidence and prediction intervals. (CHRISTMAS BREAK) Fitting curves: Using polynomials. Building models 1: Choosing variables in multiple regression Building models 2: Anova and Bayes Factors Contrasts and tests: Using emmeans Communicating regression and Anova Reproducing an analysis 1: groups work on example, start to finish. Reproducing an analysis 2: finding a suitable paper, data handling. Reproducing an analysis 3: reproducing the analysis. A catchup/revision session then follows, but no new material is introduced. "],
["access-to-r.html", "Access to R", " Access to R Throughout the module we use R for data processing and analysis. If you are taking this course at Plymouth University, the easiest way to run the code examples here is to the school’s RStudio Server. Login to your account on the server here To get an account on the server, or reset a password, contact the Psychology technical office Installing at home If you want to install R on your own machine, instructions are available here: https://github.com/plymouthPsychology/installR/ Be sure to install the recommended packages or the examples given here won’t work. "],
["reasons-to-use-r.html", "Why do we use R?", " Why do we use R? This material copied from Andy Wills’ RminR. This document covers some of the reasons we use R in this course. It’s not “required reading”, but take a look if you’re interested. Introduction R is a piece of software for handling data. It’s the one used on this course, but it’s not the only option available, others include: Excel, Jamovi, JASP, MATLAB,Stata and, perhaps the most talked-about alternative, SPSS. Student experience Students prefer R. In a recent study, undergraduate psychology students at Glasgow University were given a choice between R and SPSS, having experienced both. Two-thirds of the students chose R. Those who chose R did better in the final assessments and showed lower stats anxiety. R is being used to teach Plymouth University undergraduates (and visiting Year 10 students) across a range of different courses. Read more. Employability Data science is a graduate skill in high demand, and using R is a key skill in that market. In contrast, demand for SPSS skills has been declining dramatically for a decade. At SPSS’s current rate of decline, it’ll be gone by the time you graduate. Read more. Free R is free. You don’t need to pay anything to download or use it, and never will. In contrast, once you leave university, SPSS would cost you or your employer around £6000 per person per year. Never out of date Every analysis you can think of is already available in R, thanks to over 12,000 free packages. As new analyses are developed, they become available in R first. In 2013, SPSS realised it couldn’t keep up with R, and admitted defeat. Real Real data analysis is mainly preprocessing – scientists spend around 80% of their analysis time getting the data into a format where they can apply statistical tests. R is fantastically good at preprocessing. Our course focusses on realistic data analysis, making R the perfect tool for the job. Accurate The alternatives to R for real data analysis are either kludgy, error prone and have poor reproducibility (e.g. preprocessing in Excel, followed by statistics in SPSS), or are more niche in the graduate jobs market (e.g. MATLAB). In particular, Excel is famously error prone with, for example, 1 in 5 experiments in genetics having been screwed up by Excel and the case for the UK government’s policy of financial austerity being based on an Excel screwup. Reproducible R’s use of scripts means that, if you have done the analysis completely in R, you already have a full, reproducible record of your analysis path. Anyone with an internet connection can download R, and reproduce your analysis using your script. Making your analyses reproducible is an essential skill in many areas of research. Free as in freedom R is “free as in freedom” because all the source code is available to everyone (it’s “open source”). Some reasons this is important: All software has bugs; making the source code available means it’s more likely that these bugs are found and fixed. In contrast, no one outside of IBM can look at the source code for SPSS, and it’s entirely up to IBM whether they fix, or tell you about, the bugs it has. All software is eventually abandoned by the people who wrote it (if for no other reason than their death). Open source software only dies if no one in the world cares enough about it to maintain it. In contrast, closed-source software (e.g. SPSS) dies as soon as the current owners decide to kill it. Supported by Plymouth University R is already installed on many public machines at the University of Plymouth. Runs inside a browser You can use R without having to install it, e.g. RStudio Plymouth. "],
["assessments.html", "Assessments", " Assessments The assessment for PSYC753 includes 3 components: A pass fail assessment which will ensure you have acquired the core skills in handling and visualising data. A structured data analysis and visualisation task, requiring short answers (50%) An authentic analysis assignment, where you will replicate the analysis of a published study (50%). For submission dates please check the DLE. Don’t rely on this website. "],
["core-skills-test.html", "Core skills test", " Core skills test Details of the core skills test will be released in week 13 via the DLE, and will be due around week 20 (early December). "],
["data-analysis-and-visualisation-task.html", "Data analysis and visualisation task", " Data analysis and visualisation task This assignment is due in week 31 and counts 50% towards your module grade. Your task is to answer the questions below. Submitting your answers You should submit exactly 3 files: An rmd file A PDF document, produced by knitting your rmd file. As a separate document, upload a copy of the standard CW coversheet (and complete the feedback section). Within the Rmd file you should: Label each question clearly Where necessary, include comments explaining what specific lines of code do. Intersperse explanatory text with code chunks (don’t put everything in the same code chunk). Your rmd file should “Just Work” when the marker opens and runs it, and produce the same output as the knitted PDF file you submitted (i.e. there won’t be any errors or missing datafiles). If you work on your own computer at home, you should check your rmd file ‘knits’ correctly on the online Rstudio server. See common problems when trying to knit Rmd files Responses to frequently asked questions will be posted at this FAQ link Questions The number of marks each question is worth [out of 50] is given below. 1. [10 marks] Use dplyr functions like group_by and summarise to recreate the two tables below, from the gapminder::gapminder dataset. Mean Life Expectancy in Europe by Year year mean(lifeExp) 1952 64.4 1957 66.7 1962 68.5 1967 69.7 1972 70.8 1977 71.9 1982 72.8 1987 73.6 1992 74.4 1997 75.5 2002 76.7 2007 77.6 Country With the Highest GDP in 2007, by Continent continent country gdpPercap Africa Gabon 13206 Oceania Australia 34435 Americas United States 42952 Asia Kuwait 47307 Europe Norway 49357 2. [10 marks] Recreate the plot below, using the datasets::PlantGrowth data: Figure 1: Plant yield by treatment condition 3. [20 marks] Use read_csv to load the climate dataset held at https://bit.ly/3a1eF7w. The data are responses to a survey concerning people’s attitudes to climate change. These are the variables: sex 0 = male, 1 = female age age in years change To what extent do you agree or disagree with the statement: I can personally help to reduce climate change by changing my behaviour. 1 = strongly disagree…5 = strongly agree concern How concerned are you, if at all, about climate change, sometimes referred to as global warming? 1 = not concerned…4 = very concerned nuclear On a purely emotional level, how do you feel about nuclear power? 1 = very negative…5 = very positive exagerate To what extent do you agree or disagree with the statement: The seriousness of climate change is exaggerated. 1 = strongly disagree…5 = strongly agree hedonism How important to you is the gratification of desires, enjoyment in life, and self-indulgence? 1 = not important…5 = very important Fit a linear model to determine the extent to which change is predicted by concern. Report the results in APA format, include an appropriate plot of the data, and explain the results. Add nuclear, exagerate, and hedonism to the model in part a. Report the results from this multiple regression in APA format and explain the findings. Does the addition of these variables to the model improve the prediction of change? Using the model with concern, nuclear, exagerate, and hedonism as predictors, derive a prediction for a person who has scores of concern = 2, nuclear = 2.5, exagerate = 3, and hedonism = 3.5. 4. [10 marks] Is there sufficient evidence for a linear, quadratic or cubic component of age in the prediction of change? Explain your answer. "],
["assessment-authentic-analysis.html", "Authentic analysis assessment", " Authentic analysis assessment This task will be due around week 40. Find an empirical paper for which the authors have freely-shared the dataset but not their analysis scripts. Ideally this will link to your research interests, or be similar to your project analysis, but this is not required. A good way of finding a suitable paper would be to look through a journal known for publishing papers with open-source datasets, for example PlosOne. Another would be to search Zenodo or a similar repository for recently-published datasets, and look for where the articles reporting thse data were published. To keep the assignment simple, choose a paper which uses statistical methods taught as part of the module, or which can be approximated by them. For example: a multiple regression, between-subjects Anova, or repeated measures Anova or mixed model. Some papers report many studies, or many different analyses, and may have technical details with which you are not familiar. If this is the case then you do NOT need to deal with the whole paper: Simply focus on one of the primary analyses. For example, if one of the main study hypotheses can be answered by a 2x2 Anova or mixed model, but there are lots of other analyses included in the paper, then focus on just this one analysis. If you are unsure about the scope of this assignment (e.g. don’t know how much of a paper to try and replicate) then please check with the module leader early in the process. Your task is to: Imagine you were the author of the paper, just prior to collecting data for this study. Complete the form on ‘aspredicted.org’. You don’t actually need to use the aspredicted website for this — simply copy the heading structure from the form, and include your answers in your submitted manuscript. Comment on whether it is possible and/or straightforward to reconstruct what would have been in the pre-registration from what is written in the journal article. Using ggplot, make at least one plot of the data which illustrate the main findings. Explain your design decisions for these plots: what features of the data were you trying to highlight, and how? Replicate the statistical analysis using methods which are appropriate to the data. You do not need to use the exact method used in the original paper: For example, it’s fine to use a mixed model in place of a repeated-measures Anova. Report your findings in APA format. Compare/contrast your results with the published manuscript. Document your responses and version of the analysis in an Rmd file with accompanying dataset, suitable for upload to Zenodo or OSF.io. Reflect on the extent to which the concept of ‘reseacher degrees of freedom’ should influence our understanding of your chosen study. Important notes: Don’t assume that a published dataset and manuscript can be easily replicated simply because they have been shared: It may well be the case that it is hard to replicate the findings and this is OK (and interesting!) You will not lose marks simply because you cannot replicate the published findings. You do not need to use the exact method used in the original paper; the point of the exercise is to apply methods we have learned in an appropriate way. Your mark will be based on how appropriate the analysis you propose is, and the range of skill and understanding you demonstrate in answering the questions. Submitting your answers You should submit exactly 5 files: An Rmd file and datafile which when ‘knitted’ produce An HTML or PDF document. As a separate document, upload a copy of the standard CW coversheet (and complete the feedback section). Also include a pdf copy of the study you chose to replicate. Within the Rmd file you should: Label each question clearly Where necessary, include comments explaining what specific lines of code do. Intersperse explanatory text with code chunks (don’t put everything in the same code chunk). Your Rmd file should “Just Work” when the marker opens and runs it, and produce the same output as the knitted HTML or PDF file you submitted (i.e. there won’t be any errors or missing datafiles). If you work on your own computer at home, you should check your Rmd file ‘knits’ correctly on the online Rstudio server. See common problems when trying to knit Rmd files "],
["building-models.html", "Building models", " Building models In brief Models need to be appropriately complex. That is, we want to make models that represent our theories for the underlying causes of our data. Often this means adding many variables to a regression model. But we won’t always be sure which variables to add. Adding multiple variables also brings challenges. Where predictors are highly correlated (termed multicollinearity) then model results can be confusing. "],
["session-1-multiple-regression.html", "Session 1: Multiple regression", " Session 1: Multiple regression Overview So far, you have used regression to predict an outcome variable from a predictor variable. For example, can we predict academic performance from hours of study? You’ve also used it to determine whether the relation between two variables differs according to a categorical variable. Does the relation between academic performance and hours of study, for example, differ for men and women? We often want to determine the extent to which an outcome variable is predicted by several continuous predictors. For example, in addition to hours of study, a person’s IQ or academic interest might also predict their academic performance. We may want to add these predictors to a model because it may serve to improve the prediction of academic performance. Today, we will: learn how to conduct a multiple regression with several continuous predictor variables evaluate the regression model with statistics (\\(R^2\\), F-statistic, t-values) use Venn diagrams to help conceptualise the contribution of predictors to a model Simple vs. multiple regression Simple regression is a linear model of the relationship between one outcome variable and one predictor variable. For example, can we predict exam performance on the basis of IQ scores? Multiple regression is a linear model of the relationship between one outcome variable and more than one predictor variable. For example, can we predict exam performancebased on IQ scores and attendance at lectures? "],
["analysis-of-regression.html", "Analysis of regression", " Analysis of regression Suppose we want to construct a model to predict final university exam scores. This is the task faced by some admissions tutors! We’ll start off with a simple regression model, then work up to multiple regression. Load the ExamData dataset from https://bit.ly/37GkvJg. This contains exam scores for students taking a university course. (Make sure tidyverse is loaded first!) Learning tip Try typing out the code today if you usually cut and paste it to R! ExamData &lt;- read_csv(&#39;https://bit.ly/37GkvJg&#39;) ExamData %&gt;% head() &gt; # A tibble: 6 x 7 &gt; finalex entrex age project iq proposal attendance &gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &gt; 1 38 44 21.9 50 110 44 0 &gt; 2 49 40 22.6 75 120 70 0 &gt; 3 61 43 21.8 54 119 54 0 &gt; 4 65 42 22.5 60 125 53 0 &gt; 5 69 44 21.9 82 121 73 0 &gt; 6 73 46 21.8 65 140 62 0 These are the variables in ExamData: finalex: final examination marks entrex: entrance examination marks age: age in years project: dissertation project marks iq: IQ score proposal: dissertation proposal grade attendance: 1 = high attendance; 0 = low attendance First, let’s ask whether finalex is predicted by entrex. Plot these variables: ExamData %&gt;% ggplot(aes(x = entrex, y = finalex)) + geom_point() + geom_smooth(se=F, method=lm) There looks to be a positive association - students with higher entrance exam scores tend to have higher final exam scores. A good start! To conduct the simple regression with finalex as the outcome variable, and entrex as the predictor variable, use lm: m1 &lt;- lm(finalex ~ entrex, data = ExamData) Explanation: finalex ~ entrex can be read as “finalex is predicted by entrex”. The model is stored in m1. View the intercept of the regression line and the coefficient for entrex: m1 &gt; &gt; Call: &gt; lm(formula = finalex ~ entrex, data = ExamData) &gt; &gt; Coefficients: &gt; (Intercept) entrex &gt; -46.305 3.155 We can therefore write the regression equation: \\(Predicted\\ final\\ exam\\ score = -46.305 + 3.155(entrance\\ exam)\\) Use summary(m1) to display statistical analysis of the model: summary(m1) &gt; &gt; Call: &gt; lm(formula = finalex ~ entrex, data = ExamData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -54.494 -21.185 3.733 18.124 30.969 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) -46.3045 25.4773 -1.817 0.0788 . &gt; entrex 3.1545 0.5324 5.925 1.52e-06 *** &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 22.7 on 31 degrees of freedom &gt; Multiple R-squared: 0.531, Adjusted R-squared: 0.5159 &gt; F-statistic: 35.1 on 1 and 31 DF, p-value: 1.52e-06 Explanation of the output: Residuals: provides an indication of the discrepancy between the values of finalex predicted by the model (i.e., the regression equation) and the actual values of finalex. If the model does a good job in predicting finalex, the residuals should be relatively small. The difference between Min and Max gives us some idea of the range of error in the prediction of finalex scores. The difference in 3Q and 1Q is the interquartile range. The median of the residuals is 3.73. Coefficients: contains tests of statistical significance for each of the coefficients. The values in the column headed Pr(&gt;|t|) are the p-values associated with the t-values for the coefficients for each predictor. The t-values test a null hypothesis that the coefficients are equal to zero. A p-value less than .05 indicates that a predictor is statistically significant. The row for the (intercept) reports a t-test for whether the value of the intercept differs from zero. We’re not usually interested in this test (so don’t report it). The row for entrex tests whether the value of its coefficient (3.15) differs from zero. A coefficient of zero would be expected if the predictor explained no variance in the outcome variable. The coefficient for entrex (3.15) is clearly greater than zero. We can report this by saying that extrex is a statistically significant predictor of finalex, b = 3.15, t(31) = 5.92, p &lt; .001. Multiple R-squared: This is \\(R^2\\) - the proportion of variance in finalex explained by entrex. Here, \\(R^2\\) = 0.531. So approximately half of the variance in finalex is explained by entrex. It’s usually referred to simply as “R-squared” or \\(R^2\\). \\(R^2\\) is often reported as a percentage. To get this, simply multiply the value by 100. i.e., 0.531 x 100 = 53.10%. Adjusted R-squared: is an estimate of \\(R^2\\), but adjusted for the population. Despite the usefulness of this statistic, most studies still tend to report only the (unadjusted) \\(R^2\\) value. If reporting the Adjusted R-squared value, be sure to label it clearly as such. Here, Adjusted R-squared = 0.52. F-statistic: This compares the variance in finalex explained by the model with the variance that it does not explain (i.e., explained variance divided by unexplained variance). Higher values of F indicate that the model explains greater variance in an outcome variable. If the p-value associated with the F-statistic is less than .05, we can say that the model significantly predicts the outcome variable. Hence, we can say that a model consisting of entrex alone is a significant predictor of finalex, F(1, 31) = 35.10, p &lt; .001. Higher entrex scores tend to be associated with higher finalex scores. If our model did not explain any variance in finalex, we wouldn’t expect this to be statistically significant. In simple regression, the null hypothesis being tested on the F-statistic is that the slope of the regression line in the population is equal to zero. This is actually equivalent to the t-test on the entrex coefficient. So in simple regression, report the F-statistic for the overall regression or the t-test on the coefficient (not both). This equivalence between F and t does not hold true for multiple regression, as we shall see later. Now you have a go Run another simple regression: set finalex as the outcome variable and project as the predictor variable store the output in a variable with a different name (m2) then display the output of m2 using summary(). Try yourself first before clicking to show the code m2 &lt;- lm(finalex ~ project, data= ExamData) summary(m2) &gt; &gt; Call: &gt; lm(formula = finalex ~ project, data = ExamData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -64.015 -21.686 -0.573 21.758 70.427 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) 4.6968 40.1677 0.117 0.9077 &gt; project 1.4442 0.5861 2.464 0.0195 * &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 30.32 on 31 degrees of freedom &gt; Multiple R-squared: 0.1638, Adjusted R-squared: 0.1368 &gt; F-statistic: 6.072 on 1 and 31 DF, p-value: 0.01948 Answer the following: (report statistics to 2 decimal places) What is the value of the coefficient for project? What proportion of the variance in finalex is explained by project?: \\(R^2\\) = (or %). Write down the regression equation (on a bit of paper). Show me \\(Predicted\\ final\\ exam\\ score = 4.70 + 1.44(project)\\) Is project alone a statistically significant predictor of finalex, as indicated by the F-statistic? no yes Report the F-ratio in APA style, that is, in the form F(df1, df2) = F-statistic, p = p-value: Show me F(1, 31) = 6.07, p = .02 Individuals with lower higher project scores tended to have higher final exam scores. "],
["conceptualising-the-variance-explained-by-predictors.html", "Conceptualising the variance explained by predictors", " Conceptualising the variance explained by predictors Venn diagrams are useful for understanding the variance that predictors explain in the outcome variable. They are especially useful for understanding what’s going on in multiple regression. Suppose the rectangle below represents all of the variance in finalex to be explained. The area of the circle below represents the variance in finalex explained by entrex in the first simple regression we did. If this diagram were drawn to scale (it’s not), the area of the circle would be equal to the value of \\(R^2\\) (i.e., 53.1% of the rectangle). The part of the rectangle not inside the circle represents the variance in finalex that is not explained by the model (i.e., the unexplained or residual variance). To improve the model, we can explore whether adding in other predictors to the model explains additional variance, thereby increasing the total \\(R^2\\) of the model. You might think that we can simply add in variables (circles, above) to the model as we wish, until all the residual variance has been explained. This seems fine to do until we learn that if we were to add as many predictors to the model as there are rows in our data (33 individuals in our ExamData), then we’d perfectly predict the outcome variable, and have an \\(R^2\\) of 100%! This would be true even if the predictors consisted of random values. Our model would clearly be meaningless though. We ideally want to explain the outcome variable with relatively few predictors. "],
["adding-predictor-variables-to-the-model.html", "Adding predictor variables to the model", " Adding predictor variables to the model An issue that can arise when adding variables to a model is that predictors are usually correlated to some extent. This can make interpretation of multiple regressions tricky. For example, a predictor that is statistically significant in a simple regression may become non-significant in a multiple regression. Let’s see a demonstration of this! We’ll now add project to the model with entrex. First, check the correlation between predictors: ExamData %&gt;% select(entrex,project) %&gt;% cor() &gt; entrex project &gt; entrex 1.0000000 0.2908253 &gt; project 0.2908253 1.0000000 The correlation between entrex and project is r = Our predictor variables are weakly correlated. We should keep this in mind going forward. Now run a multiple regression to predict finalex from both entrex and project. Again, use lm but use the + symbol to add predictors to the model: m3 &lt;- lm(finalex ~ entrex + project, data = ExamData) summary(m3) &gt; &gt; Call: &gt; lm(formula = finalex ~ entrex + project, data = ExamData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -41.880 -16.617 4.636 15.562 35.273 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) -84.8289 33.6846 -2.518 0.0174 * &gt; entrex 2.8894 0.5406 5.344 8.81e-06 *** &gt; project 0.7515 0.4457 1.686 0.1021 &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 22.06 on 30 degrees of freedom &gt; Multiple R-squared: 0.5716, Adjusted R-squared: 0.5431 &gt; F-statistic: 20.02 on 2 and 30 DF, p-value: 3e-06 In this model with entrex and projectas predictors: What is the value of \\(R^2\\) (as a percentage): % By how much has \\(R^2\\) increased in this model, relative to the model with entrex alone (where \\(R^2\\) was 53.10%)? (as a percentage) (you will need to calculate this) % Is the overall regression model predicting finalex on the basis of entrex and project statistically significant? yes no Is entrex a statistically significant predictor of finalex? yes no We can report this in the following way: the t-test on the coefficient for entrex is statistically significant, b = 2.89, t(30) = 5.34, p &lt; .001. Is project a statistically significant predictor of finalex in this model? yes no What is the value of the coefficient for project? b = Report the t-statistic in APA style: Show me Project mark was not a statistically significant predictor of final examination in this model, b = 0.75, t(30) = 1.69, p = .10 Looking across the analyses we’ve performed, we can see that project is a (weak) but statistically significant predictor of finalex in a simple regression. However, when it is included in a model that also includes entrex it is not a significant predictor! What’s going on? The model containing only project explains 16.38% of the variability in finalex. The model containing only entrex explains 53.10% of the variability in finalex. However, a model containing both project and entrex only explains 57.16% of the variability in finalex, not 16.38 + 53.10 = 69.48%, as we might expect. This is because the predictors are correlated (r = .29) and so the variance they explain in finalex is shared. We could represent this on a Venn diagram as follows: The correlation is represented as an overlap in the circles. Their total area (57.16%) is therefore less than the area they’d explain if there were no overlap (69.48%) (i.e., if there was no correlation). This demonstrates an important point: The t-tests on the coefficients in a multiple regression assess the unique contribution of each predictor in the model. That is, they test the variance a predictor explains in an outcome variable, after the variance explained by the other predictors has been taken into account. This is why project is not statistically significant in the multiple regression model – it only explains a small amount of variance once entrex has been taken into account. It is possible to think of the F-statistic and t-value in multiple regression in terms of the Venn diagram: The F-statistic compares the explained variance with the unexplained variance. The explained variance is represented by the outline of the two circles in the Venn diagram above. The unexplained variance is the remaining blue area of the rectangle. The t-value compares the unique variance a predictor explains with the remaining unexplained variance. For example, for project in the Venn diagram above, this would be the area in the orange crescent, relative to the remaining blue area in the rectangle. "],
["multicolinearity.html", "Multicolinearity", " Multicolinearity If the correlation between predictors is very high (greater than r = 0.9), this is known as multicolinearity. On a Venn diagram, the circles representing the predictors would almost completely overlap. Multicolinearity can be a problem in multiple regression. Predictors may explain a large amount of variance in the outcome variable, but their ‘unique’ contribution in a multiple regression may be small. A situation can arise where neither predictor may be statistically significant even though the overall regression is significant! An example of multicolinearity in the ExamData dataset can be seen with the variables project and proposal. Obtain the correlation between project and proposal: Show me ExamData %&gt;% select(project, proposal) %&gt;% cor() &gt; project proposal &gt; project 1.0000000 0.9371487 &gt; proposal 0.9371487 1.0000000 The correlation between project and proposal is r = . To see the effects of multicolinearity, conduct a regression with finalex as the outcome variable and project and proposal as the predictor variables. Show me multi1 &lt;- lm(finalex ~ project + proposal, data = ExamData) summary(multi1) &gt; &gt; Call: &gt; lm(formula = finalex ~ project + proposal, data = ExamData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -64.287 -22.590 -0.346 22.395 70.289 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) 4.8784 40.8601 0.119 0.906 &gt; project 1.2751 1.7072 0.747 0.461 &gt; proposal 0.1826 1.7263 0.106 0.916 &gt; &gt; Residual standard error: 30.81 on 30 degrees of freedom &gt; Multiple R-squared: 0.1641, Adjusted R-squared: 0.1084 &gt; F-statistic: 2.945 on 2 and 30 DF, p-value: 0.06797 How much variance in finalex is explained by the model: \\(R^2\\) = %. Is the overall regression statistically significant? yes no Is the coefficient for project statistically significant? yes no Is the coefficient for proposal statistically significant? yes no Now run two simple regressions to determine whether project and proposal explain variance in finalex and are statistically significant predictors when in models on their own. Show me multi2 &lt;- lm(finalex ~ project, data = ExamData) summary(multi2) &gt; &gt; Call: &gt; lm(formula = finalex ~ project, data = ExamData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -64.015 -21.686 -0.573 21.758 70.427 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) 4.6968 40.1677 0.117 0.9077 &gt; project 1.4442 0.5861 2.464 0.0195 * &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 30.32 on 31 degrees of freedom &gt; Multiple R-squared: 0.1638, Adjusted R-squared: 0.1368 &gt; F-statistic: 6.072 on 1 and 31 DF, p-value: 0.01948 multi3 &lt;- lm(finalex ~ proposal, data = ExamData) summary(multi3) &gt; &gt; Call: &gt; lm(formula = finalex ~ proposal, data = ExamData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -64.987 -22.987 -1.378 24.059 68.921 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) 16.628 37.441 0.444 0.6601 &gt; proposal 1.391 0.598 2.326 0.0267 * &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 30.59 on 31 degrees of freedom &gt; Multiple R-squared: 0.1486, Adjusted R-squared: 0.1211 &gt; F-statistic: 5.409 on 1 and 31 DF, p-value: 0.02675 In a simple regression with finalex as the outcome variable, and project as the predictor variable, \\(R^2\\) = %. Is the overall regression statistically significant? yes no In a simple regression with finalex as the outcome variable, and proposal as the predictor variable, \\(R^2\\) = %. Is the overall regression statistically significant? yes no Try to explain what’s going on here in your own words. Click below or ask if you get stuck. Explain Interpretation Because proposal and project are highly correlated (r = 0.94), this gives rise to the situation where the simple regressions indicate that they explain variance in finalex, but when both are included as predictors in a multiple regression, it appears as if neither are significant predictors of finalex ! If this were a real scenario, we’d consider dropping project or proposal from the model. Because the correlation is so high, having one predictor is as good as having the other (more or less). It seems intuitive that a person’s final project mark would be highly correlated with their proposal mark. The take-home message here is to check for high correlations between your predictor variables before including them in a multiple regression. "],
["final-exercise.html", "Final exercise", " Final exercise As a final exercise, run a multiple regression to predict finalex from three predictors: entrex, project, and iq. Show me how multi2 &lt;- lm(finalex ~ entrex + project + iq, data = ExamData) summary(multi2) &gt; &gt; Call: &gt; lm(formula = finalex ~ entrex + project + iq, data = ExamData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -40.444 -16.174 5.509 14.312 33.338 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) -130.3803 54.7288 -2.382 0.023981 * &gt; entrex 2.6180 0.5978 4.379 0.000142 *** &gt; project 0.6874 0.4490 1.531 0.136620 &gt; iq 0.4862 0.4610 1.055 0.300214 &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 22.02 on 29 degrees of freedom &gt; Multiple R-squared: 0.5875, Adjusted R-squared: 0.5448 &gt; F-statistic: 13.77 on 3 and 29 DF, p-value: 9.168e-06 Which variables are statistically significant predictors of finalex? entrex yes no project yes no iq yes no On the basis of all the models conducted so far (with entrex, project, and iq), which model would you choose to best predict finalex? Tell me which model seems best The model containing entrex alone, as this seems to provide the simplest and most effective model of the finalex. A general goal of regression is to identify the fewest predictor variables necessary to predict an outcome variable, where each predictor variable predicts a substantial and independent segment of the variability in the outcome variable. "],
["summary-of-key-points.html", "Summary of key points", " Summary of key points Predictors can be added to a model in lm using the + symbol e.g., lm(finalex ~ entrex + project + iq) Predictor variables are often correlated to some extent. This can affect the interpretation of individual predictor variables. Venn diagrams help to understand the results. The F-statistic tells us whether the model as a whole significantly predicts the outcome variable. The t-values tell us whether individual predictors in the model are statistically significant. In multiple regression, it’s important to understand that the statistical significance of individual predictors only holds after taking into account the other predictors in the model. Multicolinearity exists when predictors are very highly correlated (r above 0.9) and should be avoided. "],
["session-2-anova-and-bayes-factor.html", "Session 2: ANOVA and Bayes Factor", " Session 2: ANOVA and Bayes Factor In this session we discuss model selection in the context of ANOVA and the use of Bayes Factors to choose between theoretically interesting models. "],
["fitting-curves.html", "Fitting curves", " Fitting curves In brief So far all our regression models have assumed that our variables have linear relationships. That isn’t always the case, and sometimes we need to fit curved lines to describe the relationship of predictors and outcomes. As we saw before, fitting curved lines has costs as well as benefits: A curved line is more likely to overfit the data, and may be less good at predicting new data. But for some models curved lines are essential to describe the world as it really is. &lt;!– "],
["session-1.html", "Session 1", " Session 1 We will cover use of polynomials and other techniques to fit curved lines with multiple regression. "],
["learning-outcomes.html", "Learning outcomes", " Learning outcomes Understand that polynomial terms can be added to a regression model to fit curves to the data (rather than just a straight line). Understand how to evaluate the fit of a model using R-squared. Understand how to select between different models using ANOVA. "],
["structure.html", "Structure", " Structure In the first part of the workshop, we will play around with plotting various curves using ggplot. In the second part, we will see how we can fit a curve to some experimental data using polynomials. In the exercise at the end, you will try to figure out the curve that best fits a set of data. Accordingly, it is recommended to store your commands in a .r file as you go along, to make it easy to modify later. "],
["polynomials.html", "Polynomials", " Polynomials The regression models we have been fitting assume a linear (i.e., straight line) relationship between variables. However, variables may not always be related in a linear fashion. When we plot our data, the line of best fit may appear curved indicating that a non-linear relationship may be present. Suppose variables x and y showed the following trend: It is clear that this relationship would not be explained well by fitting a standard regression model, which assumes a linear relationship between x and y. We’d lose important information about the relationship if we only fit a straight line. To improve our model, we can fit a curve by using polynomial terms in our regression. Polynomial means that a variable is raised to a particular power. For example: \\(x^2\\) means x-squared, which is x-multiplied-by-x. \\(x^3\\) means x-cubed, which is x-multiplied-by-x-multiplied-by-x The power that we raise x to could be any number. To understand how polynomials allow us to do this, let’s review how lines can be represented as equations. It may also be worth revisiting the section on Linear Equations in the Regression lecture. "],
["constant.html", "Constant", " Constant The equation \\(y = 1\\) would be represented like this: Because y is the same value (i.e., 1) at each value of x, we say that y is equal to a “constant”. The value of y determines the height of the line. "],
["linear.html", "Linear", " Linear The equation \\(y = 0.5(x)\\) would be represented as follows: Each value of y on the line is half that of x. When x is 2, y is 1 When x is 5, y is 2.5 We say that the relationship between y and x is linear, because it is a described by a straight line, and y is simply equal to a value of x multiplied by a constant (0.5). Multiplying x by larger numbers would make this line steeper; smaller numbers make this line less steep. If the number were negative, the line would slope from the upper left of the plot to the lower right. "],
["constant-linear-component.html", "Constant + linear component", " Constant + linear component The equation \\(y = 1 + 0.5(x)\\) would be represented as follows: We can think of this line as being made up of a constant and a linear component. The constant in this equation is indicated by the dashed horizontal line. The linear component to this equation 0.5(x) is indicated by the dashed slope line. The solid orange line is a combination of these two components. "],
["quadratic.html", "Quadratic", " Quadratic The equation \\(y = x^2\\) would be represented as follows: To get each value of y, we square the value of x. So, when x = -5, y is 25. And if x = -4, y = 16, and so on… \\(y = -x^2\\), would look as follows: "],
["linear-plus-quadratic-components.html", "Linear plus quadratic components", " Linear plus quadratic components The equation \\(y = 50 + 5(x) - x^2\\) has a constant 50 a linear component 5(x) a quadratic component \\(-x^2\\): The dashed lines on the plot indicate the intercept, linear component, and quadratic components of the equation. The solid line represents the equation. When we see any curve, it is possible to think of it as being composed of components like this. Let’s plot some curves. First, load the tidyverse package: library(tidyverse) We can use ggplot() to plot curves: tibble(x=1:10) %&gt;% ggplot(aes(x)) + stat_function(fun=function(x) 2*x + x^2) stat_function(fun=function(x) .... allows us to superimpose any function we specify on a plot, in this case \\(2x + x^2\\). ‘x^2’ means ‘x-to-the-power-of-2’, or \\(x^2\\). Modify the equation in stat_function(fun=function(x) EQUATION) to plot the following: \\(y = 3\\) \\(y = x\\) \\(y = 3 + x\\) \\(y = x^2\\) Hint: the first one would be stat_function(fun=function(x) 3) Try to reproduce the following plot: (Hint: what do you have to do to the values of x to get the values of y?) Answer tibble(x=1:10) %&gt;% ggplot(aes(x)) + stat_function(fun=function(x) - x^2) "],
["identifying-components.html", "Identifying components", " Identifying components It is worth noting some general characteristics of polynomial equations to give you a better idea of what these various components in the equation look like. "],
["linear-first-order-components.html", "Linear (first-order) components", " Linear (first-order) components In a linear equation of the form \\(y = b_0 + b_1(x)\\), the coefficient \\(b_0\\) represents the intercept. \\(b_1\\) is the coefficient for x. The line for a linear equation keeps on increasing and decreasing for all values of x, with no bends: "],
["quadratic-second-order-components.html", "Quadratic (second-order) components", " Quadratic (second-order) components Equations with a quadratic component have form \\(y = b_0 + b_1(x) + b_2(x^2)\\). Again, \\(b_0\\) represents the intercept. \\(b_1\\) and \\(b_2\\) are the coefficients for \\(x\\) and \\(x^2\\), respectively. The maximum/minimum of a curve is the point of inflection at which the curve switches from decreasing to increasing (or increasing to decreasing). Equations with a quadratic component have one minimum (if the \\(x^2\\) component is positive) or one maximum (if the \\(x^2\\) component is negative). For curves with positive \\(x^2\\) components: The dot indicates the minimum. Either side of this point, the values of y increase. For curves with \\(-x^2\\) components: The dot indicates the maximum. Either side of this point, the values of y decrease. Use ggplot() and stat_function() to plot a 2nd degree (quadratic) polynomial with the following characteristics: a negative intercept a positive linear component a negative quadratic component Show me an example tibble(x=1:10) %&gt;% ggplot(aes(x)) + stat_function(fun=function(x) -5 + 0.5*x - 6.0*x^2) + xlim(-5,5) + ylim(-100,100) &gt; Warning: Removed 21 rows containing missing values (geom_path). xlim() is used to specify the range of values on the x-axis (-5 to 5). ylim() is used to specify the range of values on the y-axis (-100 to 100). The curve specified in the line stat_function(fun=function(x) -5 + 0.5*x - 6.0*x^2) is \\(y = -5 + 0.5x - 6.0x^2\\) "],
["cubic-third-order-components.html", "Cubic (third) order components", " Cubic (third) order components Curves with a cubic component have two minima/maxima (i.e., two points of inflection), and are of the form \\(y = b_0 + b_1(x) + b_2(x^2) + b_3(x^3)\\), where \\(b_0\\) is the intercept, and \\(b_1\\), \\(b_2\\), and \\(b_3\\) are the coefficients for the \\(x\\), \\(x^2\\), and \\(x^3\\) components. Use ggplot() and stat_function() to plot a 3rd degree (cubic) polynomial with the following characteristics: a positive intercept a negative linear component a positive quadratic component a negative cubic component Show me an example tibble(x=1:10) %&gt;% ggplot(aes(x)) + stat_function(fun=function(x) 5 - 0.5*x + 6.0*x^2 - 0.5*x^3) + xlim(-10,10) + ylim(0,1000) &gt; Warning: Removed 3 rows containing missing values (geom_path). The curve specified in the line stat_function(fun=function(x) 5 - 0.5*x + 6.0*x^2 - 0.5*x^3) is \\(y = 5 - 0.5x + 6.0x^2 - 0.5x^3\\) "],
["quartic-fourth-order-components.html", "Quartic (fourth) order components", " Quartic (fourth) order components A curve with a quartic component would have three maxima/minima (i.e., three points of inflection). The equation would be of the form \\(y = b_0 + b_1(x) + b_2(x^2) + b_3(x^3) + b_4(x^4)\\). Use ggplot and stat_function to plot a 4th degree polynomial. Note: The points of inflection may not always be visible if the scale on either the y- or x-axis is very large. You can use ylim() and xlim() to adjust the scale of the axes, e.g., for the quartic function above: Show me tibble(x=1:10) %&gt;% ggplot(aes(x)) + stat_function(fun=function(x) 0 + 0.5*x - 6.0*x^2 + 1.0*x^3 + 0.5*x^4) + xlim(-5,5) + ylim(-100,100) &gt; Warning: Removed 10 rows containing missing values (geom_path). The code specified in stat_function(fun=function(x) 0 + 0.5*x - 6.0*x^2 + 1.0*x^3 + 0.5*x^4) is \\(y = 0 + 0.5x - 6.0x^2 + 1.0x^3 + 0.5x^4)\\) "],
["fitting-curves-to-data-rather-than-straight-lines.html", "Fitting curves to data rather than straight lines", " Fitting curves to data rather than straight lines When fitting a regression model to the data, how can we identify whether our regression model should have polynomial components? A 2016 survey of the National Office for Statistics investigated happiness across the life span (https://bit.ly/2jaZKBx). Approximately 300,000 individuals of all ages answered questions related to well-being. A BBC news report of the study can be found here. Each participant answered the following question regarding happiness: “Overall, how happy did you feel yesterday? Where 0 is ‘not at all happy’ and 10 is ‘completely happy’.” A fictitious sample of this data is in the file located at (“https://bit.ly/2QlLFAZ”). (Although I made the data up for teaching purposes, the relationships present are the same.) Load the data into R: SurveyData &lt;- read_csv(&quot;https://bit.ly/30HuHPE&quot;) Preview it: SurveyData %&gt;% glimpse() &gt; Observations: 150 &gt; Variables: 3 &gt; $ age &lt;dbl&gt; 65.97, 35.03, 58.61, 35.04, 60.16, 67.51, 44.39, 66.... &gt; $ happiness &lt;dbl&gt; 7.85, 7.56, 7.59, 7.06, 7.96, 8.11, 6.99, 7.95, 8.35... &gt; $ anxiety &lt;dbl&gt; 2.33, 2.58, 3.43, 1.67, 2.13, 1.09, 3.43, 3.82, 3.37... Obtain some descriptive statistics: SurveyData %&gt;% summary() &gt; age happiness anxiety &gt; Min. :30.16 Min. :6.750 Min. :0.800 &gt; 1st Qu.:39.46 1st Qu.:7.263 1st Qu.:2.420 &gt; Median :51.38 Median :7.495 Median :2.940 &gt; Mean :50.31 Mean :7.544 Mean :2.937 &gt; 3rd Qu.:61.18 3rd Qu.:7.768 3rd Qu.:3.417 &gt; Max. :69.58 Max. :8.530 Max. :4.700 What was the age range of the sample? Use a scatterplot to look at the relationship between age and happiness: SurveyData %&gt;% ggplot(aes(x=age, y=happiness)) + geom_point() Does age appear to be related to happiness? How would you describe the nature of the relationship? Does there appear to be a linear, quadratic, or cubic component? How would you describe this relationship in psychological terms? "],
["linear-regression.html", "Linear regression", " Linear regression Run a linear regression to determine whether happiness can be predicted from age: model1 &lt;- lm(happiness ~ age, data = SurveyData) summary(model1) &gt; &gt; Call: &gt; lm(formula = happiness ~ age, data = SurveyData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -0.78019 -0.16858 -0.04762 0.19811 0.84368 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) 6.484101 0.102340 63.36 &lt;2e-16 *** &gt; age 0.021076 0.001979 10.65 &lt;2e-16 *** &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 0.2912 on 148 degrees of freedom &gt; Multiple R-squared: 0.4339, Adjusted R-squared: 0.4301 &gt; F-statistic: 113.5 on 1 and 148 DF, p-value: &lt; 2.2e-16 Write the regression equation for predicting happiness from age, using the values from the Estimate column. Show me The regression equation is: Predicted happiness = 6.48 + 0.02*age The following parts of the output tell us how well the model explains the data: Multiple R-squared (\\(R^2\\)) is the proportion of variance in the outcome variable (happiness) that can be accounted for by the regression model. It gives us some idea of how well our model explains the data. If we multiply the value by 100, we get a percentage. Therefore, in this model, age explains 43.4% of the variance in happiness scores. The F-statistic gives us an idea of how much variance in happiness the model explains, relative to how much variance in happiness it does not explain (i.e., the error in prediction or residuals). There are two separate degrees of freedom associated with the F-statistic (1, 148). A p-value associated with F is also given. This is the probability of obtaining an F-statistic as extreme as this, given that the null hypothesis is true. Given that our p-value is less than .05, we can declare age to be a statistically significant predictor of happiness, and could report F as follows: Age is a statistically significant predictor of happiness, F(1,148) = 113.00, p &lt; .001. Now let’s see how well the happiness values predicted by the regression equation match the trends in our data. Plot the regression line from model1 on the same plot as the data: SurveyData %&gt;% mutate(fit1 = fitted(model1)) %&gt;% ggplot(aes(x=age, y=happiness)) + geom_point() + geom_line(aes(y=fit1)) The above code uses the mutate function from the dplyr package to add an additional column named fit1 to the SurveyData. fit1 contains the values of happiness that are predicted by the model, given each individual’s age. An alternative way to plot this graph, consistent with the session on regression, would be to use the augment() function in the broom package. broom::augment(model1) %&gt;% ggplot(aes(x=age, y=happiness)) + geom_point() + geom_line(aes(y=.fitted)) .fitted are the predicted values computed by augment() By visual inspection, does the model capture the trend in the data well? According to the model, happiness increases with age in a linear fashion. However, it is clear from the plot that this straight line is not capturing the non-linear trend that is evident in our data. "],
["adding-a-quadratic-component.html", "Adding a quadratic component", " Adding a quadratic component We can add a quadratic component to the regression model using the poly() function. model2 &lt;- lm(happiness~poly(age,2), data=SurveyData) summary(model2) &gt; &gt; Call: &gt; lm(formula = happiness ~ poly(age, 2), data = SurveyData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -0.58896 -0.12752 -0.02333 0.13274 0.59724 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) 7.54433 0.01779 424.06 &lt;2e-16 *** &gt; poly(age, 2)1 3.10223 0.21789 14.24 &lt;2e-16 *** &gt; poly(age, 2)2 2.36118 0.21789 10.84 &lt;2e-16 *** &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 0.2179 on 147 degrees of freedom &gt; Multiple R-squared: 0.6853, Adjusted R-squared: 0.681 &gt; F-statistic: 160.1 on 2 and 147 DF, p-value: &lt; 2.2e-16 The ‘2’ in the poly() function tells R that we want to fit a model with a quadratic component. R will then fit a model of the following form: \\(happiness = a + b_1(age) + b_2(age^2)\\) where \\(a\\) is the intercept, and \\(b_1\\) and \\(b_2\\) are the coefficients for the linear and quadratic components, respectively. R will fit this model, and provide estimates of \\(a\\), \\(b_1\\) and \\(b_2\\). \\(b_1\\) and \\(b_2\\) are the coefficients in the regression equation. Visually inspect the fit of the second model: Use mutate and fitted to derive the predicted values, as before. The regression line should now appear curved: SurveyData %&gt;% mutate(fit2=fitted(model2)) %&gt;% ggplot(aes(x=age,y=happiness)) + geom_point() + geom_line(aes(y=fit2)) Compare the value of Multiple R-Squared in model1 and model2. Does the addition of a quadratic component result in an increase in R-Squared in model 2? By how much? Answer R-squared change from model1 to model2 = 0.685 - 0.434 = 0.25. Therefore, the model with the quadratic component accounts for 25% more variance in happiness. We can test whether the increase in R-squared in model2 represents a statistically significant increase or not by comparing the models using an ANOVA: anova(model1, model2) &gt; Analysis of Variance Table &gt; &gt; Model 1: happiness ~ age &gt; Model 2: happiness ~ poly(age, 2) &gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) &gt; 1 148 12.5542 &gt; 2 147 6.9791 1 5.5752 117.43 &lt; 2.2e-16 *** &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova computes a new F-statistic, testing whether the increase in variance explained by model2 is statistically significant or not. In the previous lecture you were recommended against using anova() to specify models. The use here is different: we are comparing two existing models, rather than specifying and running a model in order to run an ANOVA in the way you did in the previous lecture. You are still advised not to use anova() to specify models as this is cumbersome. By examining the F-statistic, is there sufficient evidence for an improvement in fit in the model as a result of adding in the quadratic component? Answer We can report the improvement in fit as follows: A model with a quadratic component accounted for a statistically significantly greater proportion of variance in happiness than a model with only a linear component, F(1,148) = 117.00, p &lt; .001. "],
["adding-a-cubic-component.html", "Adding a cubic component", " Adding a cubic component Now run a model with a cubic component: model3 &lt;- lm(happiness~poly(age,3),data=SurveyData) summary(model3) &gt; &gt; Call: &gt; lm(formula = happiness ~ poly(age, 3), data = SurveyData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -0.60468 -0.14165 -0.01844 0.13839 0.58176 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) 7.54433 0.01777 424.447 &lt;2e-16 *** &gt; poly(age, 3)1 3.10223 0.21769 14.251 &lt;2e-16 *** &gt; poly(age, 3)2 2.36118 0.21769 10.846 &lt;2e-16 *** &gt; poly(age, 3)3 -0.24530 0.21769 -1.127 0.262 &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 0.2177 on 146 degrees of freedom &gt; Multiple R-squared: 0.688, Adjusted R-squared: 0.6816 &gt; F-statistic: 107.3 on 3 and 146 DF, p-value: &lt; 2.2e-16 The ‘3’ in poly(age,3) tells R that we want to specify a model with a cubic component, of the form: \\(happiness = a + b_1(age) + b_2(age^2) + b_3(age^3)\\) As before, plot the predicted values from this model in a scatterplot: SurveyData %&gt;% mutate(fit3= fitted(model3)) %&gt;% ggplot(aes(x=age,y=happiness)) + geom_point() + geom_line(aes(y=fit3)) Does the visual fit of the model with a cubic component seem better than the fit of previous models? What is the increase in R-squared as a result of adding in the cubic component? (Compare R-squared between model3 and model2). The increase in R-squared is To determine if this change in R-squared is statistically significant or not, we can again use anova(): anova(model2,model3) &gt; Analysis of Variance Table &gt; &gt; Model 1: happiness ~ poly(age, 2) &gt; Model 2: happiness ~ poly(age, 3) &gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) &gt; 1 147 6.9791 &gt; 2 146 6.9189 1 0.060171 1.2697 0.2617 The F-statistic comparing model3 and model2 is not statistically significant, F(1, 146) = 1.27, p = .26, indicating that the addition of the cubic component into the regression model does not result in a statistically significant increase in the variability explained by the model. This suggests that a model with a quadratic component (model2) is sufficient to explain the data, and that would be the model that we would choose to report. There was insufficient evidence for a model with a cubic trend. In conclusion, over the range of age considered, there is evidence for a quadratic relationship between age and happiness. There was insufficient evidence for a model with a cubic trend. "],
["bayesian-approach.html", "Bayesian approach", " Bayesian approach Load the BayesFactor package: If we wanted to compare model1, model2, and model3 using a Bayesian approach, then the process is a bit more fiddly. First, because poly() does not work seamlessly with BayesFactor::lmBF(), we need to compute \\(age^2\\) and \\(age^3\\) ourselves: SurveyData &lt;- SurveyData %&gt;% mutate(age2= age^2, age3 = age^3) This creates a new column in SurveyData with \\(age^2\\) calculated for each individual. Likewise for \\(age^3\\). Now derive model1 and model2, but using BayesFactor::lmBF(): model1BF &lt;- BayesFactor::lmBF(happiness~age, data=SurveyData) model2BF &lt;- BayesFactor::lmBF(happiness~age+age2, data=SurveyData) R produces a warning here (don’t worry about this). Note that when adding polynomial terms to the regression equation in lmBF(), we use ‘+’, rather than ’*’. Compute the Bayes Factor for the comparison of model2BF and model1BF as follows: model2BF / model1BF &gt; Bayes factor analysis &gt; -------------- &gt; [1] age + age2 : 2.618277e+17 ±0.01% &gt; &gt; Against denominator: &gt; happiness ~ age &gt; --- &gt; Bayes factor type: BFlinearModel, JZS How many more times likely is model2BF than model1BF? Does this constitute strong or anecdotal evidence for model2BF? Refer to the guidelines on interpretation of Bayes Factors in the previous lecture Answer model2BF is 2.62 times more likely than model1BF. This constitutes only weak or anecdotal evidence for model2BF. What would the dartboard representation for this Bayes Factor look like? To compare model3BF and model2BF using a Bayesian approach, we can take similar steps. Determine the Bayes Factor for model3: model3BF &lt;- BayesFactor::lmBF(happiness~age+age2+age3, data=SurveyData) &gt; Warning: data coerced from tibble to data frame Compare model2BF and model3BF: model3BF / model2BF &gt; Bayes factor analysis &gt; -------------- &gt; [1] age + age2 + age3 : 0.1631281 ±0% &gt; &gt; Against denominator: &gt; happiness ~ age + age2 &gt; --- &gt; Bayes factor type: BFlinearModel, JZS Should model3BF be preferred over model2BF? Answer No. There is scarcely any evidence for model3 according to the Bayes Factor. The model without the cubic component (model2BF) should be preferred. The results of this analysis largely agree with the traditional (NHST) analyses: the Bayes Factors indicate that there is more evidence for model2 than model1 (although the evidence is weak), but there is scarcely any evidence for model3 over model2. Tip: An advanced way to add polynomial terms to SurveyData would be: SurveyData &lt;- bind_cols(SurveyData, poly(SurveyData$age,3) %&gt;% as.tibble() %&gt;% setNames(paste0(&quot;age&quot;,1:3))) &gt; Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics). &gt; This warning is displayed once per session. This can be useful if testing a large number of polynomials. The code uses bind_cols() to add new columns to SurveyData: poly(SurveyData$age,3) creates three new columns containing the \\(x\\), \\(x^2\\), and \\(x^3\\) components. as.tibble() converts these columns to tibble format setNames(paste0(\"age\"),1:3) sets the columns names for the three new columns as age1, age2, and age3. "],
["exercise.html", "Exercise ", " Exercise "],
["does-age-predict-anxiety.html", "Does age predict anxiety?", " Does age predict anxiety? The column anxiety in SurveyData contains responses to the question: “Overall, how anxious did you feel yesterday? Where 0 is ‘not at all anxious’ and 10 is ‘completely anxious’.” Create a scatterplot of age vs. anxiety. Does there appear to be a non-linear relationship? Show me SurveyData %&gt;% ggplot(aes(x=age, y=anxiety)) + geom_point() A slight bow is evident is evident in the plot such that age and anxiety seem to follow an inverted U-shaped relationship. By testing for polynomial components, derive the regression model for predicting anxiety from age. To do this, use the same sequence of steps that we used above, but replace happiness with anxiety: Answer the following questions: Is the relationship between age and anxiety linear or non-linear? Show me There is sufficient evidence to suggest that the relationship between age and anxiety is non-linear If the relationship is non-linear, then what is the form of the relationship (i.e., quadratic, cubic, quartic etc.)? Show me A model with a quadratic component seems to provide a best account of the data: # fit a linear model model1 &lt;- lm(anxiety ~ age, data = SurveyData) summary(model1) # fit a quadratic component model2 &lt;- lm(anxiety~ poly(age,2), data=SurveyData) summary(model2) # determine if the model with a quadratic component results in an improvement in fit vs. linear anova(model1, model2) # check for a cubic component model3 &lt;- lm(anxiety ~ poly(age,3), data=SurveyData) summary(model3) # determine if the model with a cubic component results in an improvement in fit vs. quadratic anova(model2, model3) Do the results of a Bayesian analysis agree? Show me The results of a Bayesian analysis agreed: library(BayesFactor) # create polynomials by hand SurveyData &lt;- SurveyData %&gt;% mutate(age2= age^2, age3 = age^3) # BF for model 1 model1BF &lt;- BayesFactor::lmBF(anxiety~ age, data=SurveyData) # BF for model 2 model2BF &lt;- BayesFactor::lmBF(anxiety~ age + age2, data=SurveyData) # compare BFs for models 1 and 2 model2BF / model1BF # BF for model 3 model3BF &lt;- BayesFactor::lmBF(anxiety~ age + age2 + age3, data=SurveyData) # compare BFs for models 2 and 3 model3BF / model2BF Once you have decided on the final model: What proportion of variance in anxiety is explained by the model? Write the regression equation. Produce a scatterplot showing the data with the final model superimposed on it. Describe the relationship between age and anxiety in psychological terms. Does this align with the relationship between age and happiness? Show me summary(model2) SurveyData %&gt;% mutate(fit2=fitted(model2)) %&gt;% ggplot(aes(x=age,y=anxiety)) + geom_point() + geom_line(aes(y=fit2)) The proportion of variance explained by the model R-squared is 5.63% The regression equation is: Predicted Anxiety = 2.94 - 1.02Age - 1.98Age^2 Reported anxiety levels increase from 30 years to middle age (approx. 50 years) and then declines from 50 to 70 years. This mirrors the relationship with age and happiness: anxiety is greatest when happiness seems lowest. "],
["summary.html", "Summary", " Summary Polynomial terms can be added to regression models to fit curves in our data. poly(predictor,X) can be used when conducting the regression to test for the significance of polynomial terms of the Xth order. The improvement in fit (R-squared) as a result of adding in a polynomial term can be tested using anova(model1,model2). A note: Although curves of any complexity can be fit, it may not always be meaningful or parsimonious to do so. Complex models may ‘overfit’ the data and may not necessarily generalise to new datasets well. It is also important not to extrapolate beyond the range of data used to generate the model when making predictions from the model, as the same relationship may not be present. –&gt; "]
]
