[
["fitting-curves.html", "Fitting curves", " Fitting curves In brief So far all our regression models have assumed that our variables have linear relationships. That isn’t always the case, and sometimes we need to fit curved lines to describe the relationship of predictors and outcomes. As we saw before, fitting curved lines has costs as well as benefits: A curved line is more likely to overfit the data, and may be less good at predicting new data. But for some models curved lines are essential to describe the world as it really is. "],
["using-polynomials-to-fit-curves.html", "Using polynomials to fit curves", " Using polynomials to fit curves Slides from the session (available 02-02-2020) Overview In this session we will: See how we can add polynomial terms such as \\(x^2\\), \\(x^3\\) to a regression model to capture non-linear relationships. Use ANOVA and Bayes Factors to determine whether these terms improve the model. You should be comfortable with what we did in the previous Building Models 1 and Building Models 2 sessions before attempting this one. "],
["polynomials.html", "Polynomials", " Polynomials The regression models we have been fitting assume a linear (i.e., straight line) relationship between variables. However, variables may not always be related in a linear fashion. Suppose variables x and y showed the following trend: It is clear that this relationship would not be explained well by a straight line. We’d lose important information about the relationship if we only fit a straight line. A curve would be better. We can fit a curve to the data by adding polynomial terms to the regression equation. Polynomial means that a variable is raised to a particular power. For example: \\(x^2\\) means x-squared, which is x-multiplied-by-x, or “x to the power of two” \\(x^3\\) means x-cubed, which is x-multiplied-by-x-multiplied-by-x, or “x to the power of three” If a model has a quadratic component it means it has an \\(x^2\\) term in the equation. If a model has a cubic component it means it has an \\(x^3\\) term in the equation. To see why this approach works, recall that lines can be represented by equations. Components of a regression line The equation \\(y = 1 + 0.5x\\) would be represented as follows: We can think of this line as being made up of the constant and a linear component. The constant in this equation is indicated by the dashed horizontal line. The linear component to this equation 0.5x is indicated by the dashed slope line. The solid blue line is a combination of these two components. Quadratic The equation \\(y = x^2\\) would be represented as follows: To get each value of y, we square the value of x. So, when x = -5, y is 25. And if x = -4, y = 16, and so on… \\(y = -x^2\\), would look as follows: Curves with quadratic components have one bend. Cubic The equation \\(y = x^3\\) would be represented as follows: To get each value of y, we cube the value of x. So, when x = -5, y is -125. And if x = 10, y = 1000, and so on… \\(y = -x^3\\), would look as follows: Curves with cubic components have two bends. Linear plus quadratic components The equation \\(y = 50 + 5x - x^2\\) has a constant equal to 50 a linear component 5x a quadratic component \\(-x^2\\): The dashed lines on the plot indicate the intercept, linear component, and quadratic component of the equation. The solid line represents the equation. Linear + quadratic + cubic components The equation \\(y = 5000 + 750x - 50x^2 - 20x^3\\) has a constant equal to 5000 a linear component 750x a negative quadratic component \\(-50x^2\\) a negative cubic component \\(-20x^3\\) The dashed lines in the plot indicate the different components of the curve indicated by the solid blue line. When we see any curve, it is possible to think of it as being composed of components like this. In theory, we can keep adding components, but it’s rare to see even higher-order components (e.g., \\(x^4\\)) being added. Issues regarding overfitting and generalisability can also arise (mentioned in the slides). "],
["identifying-polynomial-components.html", "Identifying polynomial components", " Identifying polynomial components To determine whether a model should have quadratic, cubic, or higher order components, we can use the sequential regression approach covered in the previous session. We take the following steps, and look at the change in \\(R^2\\) associated with each step. First fit the linear model then test for the addition of the quadratic (\\(x^2\\)) component then test for the addition of the cubic (\\(x^3\\)) component Let’s see this in action! Learning tip Try storing all your code in an R Markdown file today if you are not doing so already! You can use code chunks and write text to describe each chunk as was described in the slides. We’ll use a dataset inspired by a 2016 survey of the National Office for Statistics. They investigated happiness across the life span. Approximately 300,000 individuals of all ages answered questions related to well-being. Each participant answered the following question regarding their happiness: \"Overall, how happy did you feel yesterday? Where 0 is ‘not at all happy’ and 10 is ‘completely happy’.\" This happy dataset are located at “https://bit.ly/2uIxM5K”. Let’s load the data into R, and preview the data using head: # load the data SurveyData &lt;- read_csv(&quot;https://bit.ly/2uIxM5K&quot;) # preview it SurveyData %&gt;% head() &gt; # A tibble: 6 x 3 &gt; age happiness anxiety &gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &gt; 1 66.0 7.85 2.33 &gt; 2 35.0 7.56 2.58 &gt; 3 58.6 7.59 3.43 &gt; 4 35.0 7.06 1.67 &gt; 5 60.2 7.96 2.13 &gt; 6 67.5 8.11 1.09 Plot the relationship between age and happiness: SurveyData %&gt;% ggplot(aes(x=age, y=happiness)) + geom_point() If you had to guess from the plot, which components seem to be present in the relationship between happiness and age? Linear: no possibly Quadratic: no yes Cubic: no yes Try to describe the relationship between happiness and age. Try first, click here for a description Happiness of individuals appears to decline from 30 years to the late forties. Happiness then increases beyond the late forties, reaching its peak at 70 years, at which age people reported the highest levels of happiness - higher even than levels shown in early thirties. Linear component To determine whether there is a linear component, run a simple regression with happiness as the outcome variable and age as the predictor: polynomial1 &lt;- lm(happiness ~ age, data = SurveyData) summary(polynomial1) &gt; &gt; Call: &gt; lm(formula = happiness ~ age, data = SurveyData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -0.78019 -0.16858 -0.04762 0.19811 0.84368 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) 6.484101 0.102340 63.36 &lt;2e-16 *** &gt; age 0.021076 0.001979 10.65 &lt;2e-16 *** &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 0.2912 on 148 degrees of freedom &gt; Multiple R-squared: 0.4339, Adjusted R-squared: 0.4301 &gt; F-statistic: 113.5 on 1 and 148 DF, p-value: &lt; 2.2e-16 Explanation: The linear model is stored in polynomial1. summary displays the results. What percentage of the variance in happiness scores is explained by age? 0.44 43.39 29.12 % Is age a statistically significant predictor of happiness no yes The linear model does okay, but remember it is only fitting a straight line through our data, which appear to show a curved relationship! Adding a quadratic component We can add a quadratic component to the regression model using poly(). If we type poly(age, 2) when specifying the model, the ‘2’ in the poly() function tells R that we want to fit a model with both linear and quadratic components of age. This is the model it’ll fit: \\(predicted \\: happiness = a + b_1(age) + b_2(age^2)\\) where \\(a\\) is the intercept, and \\(b_1\\) and \\(b_2\\) are the coefficients for the linear and quadratic components, respectively. polynomial2 &lt;- lm(happiness ~ poly(age,2), data = SurveyData) summary(polynomial2) &gt; &gt; Call: &gt; lm(formula = happiness ~ poly(age, 2), data = SurveyData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -0.58896 -0.12752 -0.02333 0.13274 0.59724 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) 7.54433 0.01779 424.06 &lt;2e-16 *** &gt; poly(age, 2)1 3.10223 0.21789 14.24 &lt;2e-16 *** &gt; poly(age, 2)2 2.36118 0.21789 10.84 &lt;2e-16 *** &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 0.2179 on 147 degrees of freedom &gt; Multiple R-squared: 0.6853, Adjusted R-squared: 0.681 &gt; F-statistic: 160.1 on 2 and 147 DF, p-value: &lt; 2.2e-16 Explanation of the code: We’ve told R we want to add a quadratic component to the model by using happiness ~ poly(age, 2). Explanation of the output: You will see in the output separate coefficient estimates for poly(age, 2)1 and poly(age, 2). These are the estimates of the coefficients for the linear and quadratic components of age (i.e., \\(b_1\\) and \\(b_2\\) in the equation above). What percentage of the variance in happiness does a model with a quadratic component of age explain? % Compare the value of \\(R^2\\) in polynomial1 and polynomial2. Does the addition of a quadratic component result in a numerical increase in \\(R^2\\) in polynomial2? yes no What is the change in \\(R^2\\)? % (to 2 decimal places) Click to see how the answer is calculated \\(R^2\\) change from polynomial1 to polynomial2 = 68.53 - 43.39 = 25.14% Therefore, the model with the quadratic component of age accounts for 25.14% more variance in happiness than the model with only a linear component. We can test whether the increase in R2 in polynomial2 represents a statistically significant increase by comparing polynomial1 and polynomial2 using anova: anova(polynomial1, polynomial2) &gt; Analysis of Variance Table &gt; &gt; Model 1: happiness ~ age &gt; Model 2: happiness ~ poly(age, 2) &gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) &gt; 1 148 12.5542 &gt; 2 147 6.9791 1 5.5752 117.43 &lt; 2.2e-16 *** &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Is the increase in \\(R^2\\) associated with the addition of the quadratic component statistically significant? yes no Answer Yes. We can report the improvement in fit as follows: A model with a quadratic component of age accounted for a statistically significantly greater proportion of variance in happiness than a model with only a linear component, F(1, 147) = 117.43, p &lt; .001. Adding a cubic component Now we’ll test for a cubic component. polynomial3 &lt;- lm(happiness ~ poly(age,3), data = SurveyData) summary(polynomial3) &gt; &gt; Call: &gt; lm(formula = happiness ~ poly(age, 3), data = SurveyData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -0.60468 -0.14165 -0.01844 0.13839 0.58176 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) 7.54433 0.01777 424.447 &lt;2e-16 *** &gt; poly(age, 3)1 3.10223 0.21769 14.251 &lt;2e-16 *** &gt; poly(age, 3)2 2.36118 0.21769 10.846 &lt;2e-16 *** &gt; poly(age, 3)3 -0.24530 0.21769 -1.127 0.262 &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 0.2177 on 146 degrees of freedom &gt; Multiple R-squared: 0.688, Adjusted R-squared: 0.6816 &gt; F-statistic: 107.3 on 3 and 146 DF, p-value: &lt; 2.2e-16 The ‘3’ in poly(age,3) tells R that we want to specify a model with linear, quadratic and cubic components, of the form: \\(happiness = a + b_1(age) + b_2(age^2) + b_3(age^3)\\) What percentage of the variance in happiness does a model with a cubic component of age explain? % Compare the value of \\(R^2\\) in polynomial3 and polynomial2. Does the addition of a cubic component result in a numerical increase in \\(R^2\\) in polynomial3? yes no What is the increase in \\(R^2\\) as a result of adding in the cubic component? (Compare \\(R^2\\) between polynomial3 and polynomial2). The increase in \\(R^2\\) is % To determine if the increase in \\(R^2\\) is statistically significant, we can again use anova: anova(polynomial2, polynomial3) &gt; Analysis of Variance Table &gt; &gt; Model 1: happiness ~ poly(age, 2) &gt; Model 2: happiness ~ poly(age, 3) &gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) &gt; 1 147 6.9791 &gt; 2 146 6.9189 1 0.060171 1.2697 0.2617 Is the increase in \\(R^2\\) associated with the addition of a cubic component statistically significant? no yes Description of the answer The anova comparing polynomial3 and polynomial2 is not statistically significant, F(1, 146) = 1.27, p = .26, indicating that the addition of the cubic component of age into the regression model does not increase the variance in happiness explained. On the basis of the tests conducted so far, which model should be preferred? One with: a linear component of age only linear and quadratic components of age linear, quadratic, and cubic components of age Explain Our analyses suggest that a model with a quadratic component of age (i.e., the model in polynomial2) is sufficient to explain the data. A note about poly() poly automatically creates polynomial terms for us. The polynomials it creates are actually a special type, called orthogonal polynomials. This means that the polynomials are not correlated with one another. For example, the correlation between the \\(age\\) and \\(age^2\\) components created by poly is zero . Likewise, the correlation betweem \\(age^2\\) and \\(age^3\\) components created by poly is also zero. This is desirable because if the components were not orthogonalised, they’d be highly correlated with each other. That is, the raw scores for \\(age\\) and \\(age \\times age\\) are likely to be highly correlated. As we covered in the first Building Models 1 session, high correlations between our predictors is undesirable as it can lead to multicolinearity. "],
["bayesian-approach.html", "Bayesian approach", " Bayesian approach As we did in the previous session, we can use Bayes Factors to compare the models with different polynomial components. Preparations Unfortunately, poly() does not work seamlessly with lmBF, as it did with lm. Instead, we need to create separate variables in SurveyData for the quadratic and cubic components before we work out the Bayes Factors with lmBF. To add the quadratic component to SurveyData: SurveyData &lt;- SurveyData %&gt;% mutate( age2 = poly(age,2)[,&quot;2&quot;] ) Explanation of the code: The code takes SurveyData, then uses mutate to add a new variable age2 to the dataset. age2 contains the quadratic component of age, created by poly(age,2)[,\"2\"]. We can see the new variable age2 when we look at SurveyData again: SurveyData %&gt;% head() &gt; # A tibble: 6 x 4 &gt; age happiness anxiety age2 &gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &gt; 1 66.0 7.85 2.33 0.0761 &gt; 2 35.0 7.56 2.58 0.0483 &gt; 3 58.6 7.59 3.43 -0.0440 &gt; 4 35.0 7.06 1.67 0.0481 &gt; 5 60.2 7.96 2.13 -0.0246 &gt; 6 67.5 8.11 1.09 0.110 Now create the variable for the cubic component: SurveyData &lt;- SurveyData %&gt;% mutate( age3 = poly(age,3)[,&quot;3&quot;] ) Explanation of the code: As before, the code takes SurveyData, then uses mutate to add a new variable age3 to the dataset. age3 contains the quadratic component of age, created by poly(age,3)[,\"3\"]. Again, we can see the new variable age3 when we look at SurveyData: SurveyData %&gt;% head() &gt; # A tibble: 6 x 5 &gt; age happiness anxiety age2 age3 &gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &gt; 1 66.0 7.85 2.33 0.0761 0.00888 &gt; 2 35.0 7.56 2.58 0.0483 0.0353 &gt; 3 58.6 7.59 3.43 -0.0440 -0.0988 &gt; 4 35.0 7.06 1.67 0.0481 0.0356 &gt; 5 60.2 7.96 2.13 -0.0246 -0.0974 &gt; 6 67.5 8.11 1.09 0.110 0.0707 Derive the Bayes Factors First, make sure the BayesFactor package is loaded (library(BayesFactor)). We can use lmBF to get the Bayes Factor for each model, as we did in the previous session. To derive the Bayes Factor for polynomial1: polynomial1BF &lt;- lmBF(happiness ~ age, data = as.data.frame(SurveyData) ) To derive the Bayes Factor for polynomial2: # store the Bayes Factor for polynomial2 polynomial2BF &lt;- lmBF(happiness ~ age + age2, data = as.data.frame(SurveyData) ) Explanation: With lmBF we need to specify the polynomial equation with both linear and quadratic components separately, hence happiness ~ age + age2. The Bayes Factor comparing polynomial2 and polynomial1 is then: polynomial2BF / polynomial1BF &gt; Bayes factor analysis &gt; -------------- &gt; [1] age + age2 : 2.618277e+17 ±0.01% &gt; &gt; Against denominator: &gt; happiness ~ age &gt; --- &gt; Bayes factor type: BFlinearModel, JZS How many more times likely is a model with a quadratic component of age than one with only a linear component? 2.62 2.62e-17 2.62e+17 Does this constitute strong evidence for the addition of a quadratic component? yes no Explain why The Bayes Factor tells us that a model with a quadratic component of age is 2.62e+17 or \\(2.62 \\times 10^{17}\\) times more likely than one that simply contains a linear component. This is very strong evidence for the inclusion of a quadratic component of age in the model. Next, determine the Bayes Factor for polynomial3: polynomial3BF &lt;- BayesFactor::lmBF(happiness ~ age + age2 + age3, data = as.data.frame(SurveyData) ) Explanation: Again, we need to specify the polynomial equation with linear, quadratic, and cubic components separately, hence happiness ~ age + age2 + age3. Compare polynomial3BF and polynomial2BF: polynomial3BF / polynomial2BF &gt; Bayes factor analysis &gt; -------------- &gt; [1] age + age2 + age3 : 0.1631281 ±0% &gt; &gt; Against denominator: &gt; happiness ~ age + age2 &gt; --- &gt; Bayes factor type: BFlinearModel, JZS How many more times likely is a model with a cubic component than one with only linear and quadratic components? Does this constitute strong evidence for the inclusion of a cubic component in the model? yes no Explain why The Bayes Factor tells us that a model with a cubic component of age is only 0.16 times more likely than one that contains both linear and quadratic components. Because the Bayes Factor is less than 0.33, this constitutes strong evidence for the simpler model with only linear and quadratic components. On the basis of the model comparison with Bayes Factors, which model should be preferred? One with: a linear component of age only linear and quadratic components of age linear, quadratic, cubic components of age A comparison of Bayes Factors agrees with the comparison of the models with anova. There’s strong evidence that the relationship between age and happiness contains both linear and quadratic components of age. There was no evidence for a cubic component. "],
["exercise-1.html", "Exercise", " Exercise Now you try incorporating polynomials to a regression, and do so by investigating the relationship between age and anxiety in SurveyData. The column anxiety in SurveyData contains responses to the question: “Overall, how anxious did you feel yesterday? Where 0 is ‘not at all anxious’ and 10 is ‘completely anxious’.” Create a scatterplot of age vs. anxiety. Does there appear to be a linear or non-linear relationship? Try to create the plot yourself first. Click to show the code What kind of relationship between age and anxiety do you think is present? Try yourself first, then click to see answer A slight bow is evident in the plot such that age and anxiety seem to follow an inverted U-shaped relationship. Reported anxiety levels increase from 30 years to middle age (approx. 50 years) and then declines from 50 to 70 years. This mirrors the relationship with age and happiness. anxiety is greatest when happiness seems lowest. Answer the following questions: Linear component What percentage of the variance in anxiety is explained by a model with age as predictor? % Quadratic component What percentage of the variance in anxiety is explained by a model containing both linear and quadratic components of age as predictors? % What is the increase in \\(R^2\\) if a quadratic component of age is added to the model? % Does this increase represent a statistically significant increase? yes no What is the F-statistic comparing the model with a linear component vs. one with linear and quadratic components? F(1, 147) = , p = .009 Cubic component What percentage of the variance in anxiety is explained by a model containing both linear, quadratic and cubic components of age as predictors? % What is the increase in \\(R^2\\) if a cubic component of age is added to the model? % Does this increase represent a statistically significant increase? yes no What is the F-statistic for the test of the model with a linear + quadratic component vs. linear + quadratic + cubic components? F(1, 146) = , p = Decision - On the basis of the model comparison with ANOVA, which model should be preferred? linear component of age only linear and quadratic components of age linear, quadratic, cubic components of age Show me the code to do all of this # fit a linear model, show results anx1 &lt;- lm(anxiety ~ age, data = SurveyData) summary(anx1) # fit a quadratic component, show results anx2 &lt;- lm(anxiety ~ poly(age,2), data=SurveyData) summary(anx2) # compare linear and linear+quadratic models anova(anx1, anx2) # fit a cubic component anx3 &lt;- lm(anxiety ~ poly(age,3), data=SurveyData) summary(anx3) # compare (linear + quadratic) and (linear + quadratic + cubic) models anova(anx2, anx3) Now use Bayes Factors: (Note: you do not need to re-add the quadratic and cubic components of age to SurveyData, as we did this before. These should still be in SurveyData as age2 and age3.) The Bayes Factor comparing a model with linear and quadratic components vs. one with a linear component only is This indicates that there is more evidence for which model? linear component only linear plus quadratic components The Bayes Factor comparing a model with linear, quadratic and cubic components vs. one with linear and quadratic components only is This indicates that there is more evidence for which model? linear plus quadratic component linear, quadratic, and cubic components Do the comparisons of models with Bayes Factors agree with the conclusions made with anova? no yes Show me the code to determine the Bayes Factors library(BayesFactor) # BF for model anx1 anx1BF &lt;- lmBF(anxiety ~ age, data = as.data.frame(SurveyData) ) # BF for model anx2 anx2BF &lt;- lmBF(anxiety ~ age + age2, data = as.data.frame(SurveyData) ) # BF for model anx3 anx3BF &lt;- lmBF(anxiety~ age + age2 + age3, data = as.data.frame(SurveyData) ) # compare BFs for anx2 and anx1 anx2BF / anx1BF &gt; Bayes factor analysis &gt; -------------- &gt; [1] age + age2 : 5.476443 ±0% &gt; &gt; Against denominator: &gt; anxiety ~ age &gt; --- &gt; Bayes factor type: BFlinearModel, JZS # compare BFs for anx3 and anx2 anx3BF / anx2BF &gt; Bayes factor analysis &gt; -------------- &gt; [1] age + age2 + age3 : 0.7556007 ±0% &gt; &gt; Against denominator: &gt; anxiety ~ age + age2 &gt; --- &gt; Bayes factor type: BFlinearModel, JZS "],
["summary-of-key-points-2.html", "Summary of key points", " Summary of key points Polynomial terms (e.g., \\(x^2\\), \\(x^3\\)) can be added to regression models to fit curves in our data. poly(predictor name, X) can be used with lm to specify models with polynomial terms of the Xth order. The improvement in fit (\\(R^2\\)) as a result of adding in a polynomial term can be tested using anova(polynomial1, polynomial2). Bayes Factors can also be used to compare models with polynomial terms using lmBF. You must store the polynomial components in the dataset first before using lmBF. Use poly(predictor name, X)[,\"X\"], where X is the order of the polynomial you will test (e.g., poly(age, 3)[,\"3\"]). A note of caution: Although curves of any complexity can be fit, it may not always be meaningful or parsimonious to do so. Complex models may overfit the data and may not necessarily generalise to new datasets well. It is also important not to extrapolate beyond the range of data used to generate the model when making predictions from the model, as the same relationship may not be present. "]
]
