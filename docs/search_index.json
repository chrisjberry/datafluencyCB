[
["building-models-2.html", "Building models 2", " Building models 2 In brief In this session we discuss model selection in the context of ANOVA and the use of Bayes Factors to choose between theoretically interesting models. "],
["using-anova-and-bayes-factors-to-compare-models.html", "Using ANOVA and Bayes Factors to compare models", " Using ANOVA and Bayes Factors to compare models Slides from the session (available 25-01-2020) Overview In the previous session, we saw that we can construct a linear model to predict an outcome variable (e.g., final exam score from entrance exam score). We also investigated how we can improve a model by adding several continuous predictors to it. How do we know if one model is better or should be preferred over another model? We touched on a common sense approach in the last session - we ideally want models that explain the variance in an outcome variable but each predictor in the model should make a sizable and relatively independent contribution to the model. Today we will cover a more formal approach to model comparison using: ANOVA (Analysis of Variance) and Bayes Factors It’s important that you are comfortable with the material from the first Building Models 1 session before proceeding today. "],
["comparing-models-using-anova.html", "Comparing models using ANOVA", " Comparing models using ANOVA We can use ANOVA to determine whether the addition of variables into a model leads to a statistically significant improvement in the variance it explains overall. We may want to do this, for example, when building on existing theories or models. We’ll start by comparing a model with one predictor vs. a model with three predictors. Using the ExamData from the previous session, we’ll run: a linear model with finalex as the outcome variable, and entrex as the predictor. a linear model with finalex as the outcome variable, and entrex,age, and project as the predictors. ExamData &lt;- read_csv(&#39;https://bit.ly/37GkvJg&#39;) model1 &lt;- lm(finalex ~ entrex, data = ExamData) model2 &lt;- lm(finalex ~ entrex + age + project, data = ExamData) Explanation of the code: first the data is loaded into ExamData. The results of the simple regression are stored in model1. Those of the multiple regression are stored in model2. Use summary() to display the results of each regression: Model 1: summary(model1) &gt; &gt; Call: &gt; lm(formula = finalex ~ entrex, data = ExamData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -54.494 -21.185 3.733 18.124 30.969 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) -46.3045 25.4773 -1.817 0.0788 . &gt; entrex 3.1545 0.5324 5.925 1.52e-06 *** &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 22.7 on 31 degrees of freedom &gt; Multiple R-squared: 0.531, Adjusted R-squared: 0.5159 &gt; F-statistic: 35.1 on 1 and 31 DF, p-value: 1.52e-06 Model 2: summary(model2) &gt; &gt; Call: &gt; lm(formula = finalex ~ entrex + age + project, data = ExamData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -42.563 -16.519 4.901 16.991 36.424 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) -117.9159 46.4211 -2.540 0.0167 * &gt; entrex 3.0889 0.5734 5.387 8.66e-06 *** &gt; age 1.4231 1.3756 1.035 0.3094 &gt; project 0.6280 0.4609 1.363 0.1835 &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 22.03 on 29 degrees of freedom &gt; Multiple R-squared: 0.5869, Adjusted R-squared: 0.5442 &gt; F-statistic: 13.73 on 3 and 29 DF, p-value: 9.353e-06 (If you are not sure what it means by “e-06” in the output above then see the FAQs here) Make note of the variance explained by each model (\\(R^2\\)), i.e., Multiple R-squared: (report as a percentage, to 2 decimal places) Model 1: \\(R^2\\) = % Model 2: \\(R^2\\) = % Which model explains a greater proportion of variance in finalex? extrex alone entrex, age, project Calculate the difference in \\(R^2\\) between the models. model2 improves the prediction of finalex by % To compare the variance explained by each model, use anova(): anova(model1, model2) &gt; Analysis of Variance Table &gt; &gt; Model 1: finalex ~ entrex &gt; Model 2: finalex ~ entrex + age + project &gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) &gt; 1 31 15981 &gt; 2 29 14078 2 1903 1.9601 0.1591 Explanation of the output: anova() compares the variance that model1 and model2 explain with an F-statistic. Pr(&gt;F) gives the p-value for this statistic. If the p-value is less than .05, then we can reject the null hypothesis that there is no difference in the variance explained by each model, and we can say that the variance that model2 explains in finalex is significantly greater than that of model1. We can report the F-statistic in APA style as F(2, 29) = 1.96, p = .16. We can say that the additional 5.59% variance that model2 explains relative to model1 does not represent a statistically significant increase in \\(R^2\\), and so model2 should not be preferred over model1. Comparing models in steps as we’ve done is sometimes called hierarchical regression or sequential regression. This type of regression is usually used for logical or theoretical reasons, when we want to know the contribution of a predictor (or a set of predictors) over and above an existing one. Now, you try using anova to compare models. The variable attendance in ExamData scores individuals according to whether their class attendance was low (0) or high (1). A researcher suspects that attendance may explain additional variance in finalex over and above entrex. As an exercise, compare the following two models using the anova() approach above: a model with extrex as a sole predictor of finalex (i.e., model1), and a model where finalex is predicted by extrex and attendance (call this model3). Is there sufficient evidence that a model with entrex and attendance explains more variance than a model with entrex alone? Try yourself first, then click to see the code # model1 was created earlier summary(model1) &gt; &gt; Call: &gt; lm(formula = finalex ~ entrex, data = ExamData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -54.494 -21.185 3.733 18.124 30.969 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) -46.3045 25.4773 -1.817 0.0788 . &gt; entrex 3.1545 0.5324 5.925 1.52e-06 *** &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 22.7 on 31 degrees of freedom &gt; Multiple R-squared: 0.531, Adjusted R-squared: 0.5159 &gt; F-statistic: 35.1 on 1 and 31 DF, p-value: 1.52e-06 # specify model3 model3 &lt;- lm(finalex ~ entrex + attendance, data = ExamData) # show model3 summary(model3) &gt; &gt; Call: &gt; lm(formula = finalex ~ entrex + attendance, data = ExamData) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -42.750 -11.750 1.801 9.689 30.347 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) -63.3108 20.2768 -3.122 0.00395 ** &gt; entrex 3.2741 0.4173 7.846 9.35e-09 *** &gt; attendance 28.8202 6.3398 4.546 8.37e-05 *** &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 17.76 on 30 degrees of freedom &gt; Multiple R-squared: 0.7223, Adjusted R-squared: 0.7038 &gt; F-statistic: 39.02 on 2 and 30 DF, p-value: 4.499e-09 #compare model1 and model3 anova(model1, model3) &gt; Analysis of Variance Table &gt; &gt; Model 1: finalex ~ entrex &gt; Model 2: finalex ~ entrex + attendance &gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) &gt; 1 31 15980.6 &gt; 2 30 9462.4 1 6518.1 20.665 8.37e-05 *** &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The variance explained by a model with entrex alone is \\(R^2\\) = % The \\(R^2\\) for the model that also included attendance was \\(R^2\\) = % The increase in \\(R^2\\) was % The ANOVA comparing models can be reported as: F(, ) = , p &lt; .001. The increase in \\(R^2\\) was statistically significant not significant. As indicated by the estimates of the coefficients for entrex and attendance, both negatively positively predict finalex. A higher entrex score and greater attendance is associated with a higher lower finalex score. "],
["comparing-models-using-bayes-factors.html", "Comparing models using Bayes Factors", " Comparing models using Bayes Factors An alternative approach to using ANOVA to compare models is to use Bayes Factors. A Bayes Factor is the probability of obtaining the data under one model compared to another. For example, a Bayes Factor equal to 2 would tell us that the data are twice as likely under one model than another. A Bayes Factor equal to 0.5 would tell us that the data are half as likely under one model than another. Unlike classical tests of statistical significance (with p-values), Bayes Factors also allow us to quantify evidence for the null hypothesis. Very handy! To compute a Bayes Factor for a specific linear model, we use lmBF in the BayesFactor package (where lm stands for linear model and BF stands for Bayes Factor). First, we need to load the BayesFactor package: library(&#39;BayesFactor&#39;) We can use the lmBF function in the same way we use lm. The function will return a Bayes Factor for the model we specify. Let’s determine the Bayes Factor for model1 model1.BF &lt;- lmBF(finalex ~ entrex, data = as.data.frame(ExamData) ) Explanation of the code: The model is specified in exactly the same way as with lm. Due to a limitation of the package, however, we must convert ExamData from a tibble to a data frame using as.data.frame. Otherwise, the command works in the same way. The results are stored in model1.BF. To look at what’s stored in model1.BF: model1.BF &gt; Bayes factor analysis &gt; -------------- &gt; [1] entrex : 8310.846 ±0.01% &gt; &gt; Against denominator: &gt; Intercept only &gt; --- &gt; Bayes factor type: BFlinearModel, JZS Explanation of the output: The Bayes Factor provided for the model with entrex is equal to 8310.85. The Against denominator: Intercept only means that the model with entrex is being compared with a model that contains an intercept only. In an intercept-only model, the coefficient for entrex is equal to zero; that is, the regression line is a flat line (equal to the mean of entrex). The value of our Bayes Factor indicates that the model with entrex in is much more likely than a model that contains only an intercept (8310.85 times more likely, to be precise). We can therefore be confident that a model with entrex is preferable to the intercept only model (just as with our classical analysis). Happy days! Now let’s do the same for model2: # specify the model model2.BF &lt;- lmBF(finalex ~ entrex + age + project, data = as.data.frame(ExamData) ) # show the Bayes Factor model2.BF &gt; Bayes factor analysis &gt; -------------- &gt; [1] entrex + age + project : 2427.676 ±0% &gt; &gt; Against denominator: &gt; Intercept only &gt; --- &gt; Bayes factor type: BFlinearModel, JZS Explanation: The Bayes Factor is equal to 2427.68. Again, this indicates that the model with entrex and age is much more likely than a model with only the intercept in (this is not that surprising given the result for model1.BF above). But, what we want to know is whether model2 (containing entrex and age) is more likely than model1 (containing only entrex). We can determine this by dividing the Bayes Factor for model2 by the Bayes Factor for model1: model2.BF / model1.BF &gt; Bayes factor analysis &gt; -------------- &gt; [1] entrex + age + project : 0.2921093 ±0.01% &gt; &gt; Against denominator: &gt; finalex ~ entrex &gt; --- &gt; Bayes factor type: BFlinearModel, JZS Explanation: The Bayes Factor for this comparison is 0.29. This means that model2 is less than a third as likely than model1. So, model2 is much less likely than model1. Not good news for model2! Interpreting the Bayes Factor A Bayes Factor equal to 1 tells us that probability of each model is the same. A Bayes Factor greater than 1 means that model2 is more likely than model1. A Bayes Factor less than 1 means that model1 is more likely than model2. Thus, our Bayes Factor of 0.29 indicates that model1 is more likely than model2. Reporting Bayes Factors Notation We usually write the Bayes Factor in reports as \\(BF_{10}\\) where: the subscript 1 in \\(BF_{10}\\) denotes the less-constrained model (the alternative hypothesis). This is the model with more predictors (our model2). the subscript 0 in \\(BF_{10}\\) denotes the more constrained or simpler model (i.e., the null hypothesis). This is the model with fewer predictors (our model1). (You can just write BF10 if you prefer.) The Size of the Bayes Factor If the Bayes Factor is greater than 3 (i.e., \\(BF_{10}\\) &gt; 3), we say that there is substantial evidence for model2 (the less constrained model). If the Bayes Factor is less than 0.33 (i.e., \\(BF_{10}\\) &lt; 0.33), we usually say that there is substantial evidence for model1 (the more constrained model). We say that intermediate values for the Bayes Factor (between 0.33 and 3) don’t offer strong evidence for either model. Thus, because our Bayes Factor of 0.29 is less than 1, this indicates greater evidence for model1 than model2. Furthermore, because the Bayes Factor is less than 0.33, we have substantial evidence for model1 over model2. It’s becoming increasingly common to report the Bayes Factor alongside the results of a classical analysis. Thus, we could report our results as follows: “There was insufficient evidence that the addition of age and project to the model containing entrance exam resulted in an increase in \\(R^2\\), F(2, 29) = 1.96, p = .16; BF10 = 0.29.” Now you try using Bayes Factors to compare models To supplement the comparison of model3 and model1 that you did with anova, now compute the Bayes Factor for model3 vs. model1. You’ll need the following steps: Model 1: Obtain the Bayes Factor for a model with extrex as a sole predictor of finalex (we did this already above; it’s stored in model1.BF) Model 2: Obtain the Bayes Factor for a model where finalex is predicted by extrex and attendance and store this in model3.BF. Compare the Bayes Factors in model3.BF and model1.BF. Try yourself first, then click here for the code # 1. show the BF for model1 vs. intercept only model1.BF &gt; Bayes factor analysis &gt; -------------- &gt; [1] entrex : 8310.846 ±0.01% &gt; &gt; Against denominator: &gt; Intercept only &gt; --- &gt; Bayes factor type: BFlinearModel, JZS # 2. Obtain the BF for model3 vs. intercept only, then show it model3.BF &lt;- lmBF(finalex ~ entrex + attendance, data = as.data.frame(ExamData) ) model3.BF &gt; Bayes factor analysis &gt; -------------- &gt; [1] entrex + attendance : 2351114 ±0% &gt; &gt; Against denominator: &gt; Intercept only &gt; --- &gt; Bayes factor type: BFlinearModel, JZS # 3. Compare the BFs for model3 vs model1 model3.BF / model1.BF &gt; Bayes factor analysis &gt; -------------- &gt; [1] entrex + attendance : 282.897 ±0.01% &gt; &gt; Against denominator: &gt; finalex ~ entrex &gt; --- &gt; Bayes factor type: BFlinearModel, JZS Answer the following questions from the output: How much more likely is a model withentrex than an intercept only model? times more likely. How much more likely is a model with entrex and attendance than an intercept only model? times more likely. How much more likely is a model with entrex and attendance as predictors than a model with entrex alone? times more likely. There is insufficient strong evidence that a model with entrex and attendance should be preferred over a model with entrex alone, given the data. "],
["exercise.html", "Exercise", " Exercise Now you will practise using ANOVA and Bayes Factors to compare models with a new dataset. Scenario: A researcher would like to construct a model to predict scores in a memory task from several different variables. The data from 234 individuals are stored in the memory_data dataset, which are located at https://bit.ly/37pOTrC. Use read_csv to load in the data at the link above to the variable memory_data and preview it with head(). Try this yourself first. Click to show code memory_data &lt;- read_csv(&#39;https://bit.ly/37pOTrC&#39;) memory_data %&gt;% head() &gt; # A tibble: 6 x 7 &gt; attention sex blueberries iq age sleep memory_score &gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &gt; 1 95.8 1 308 99.9 44.9 9.94 128. &gt; 2 66.7 1 270 137. 29.4 8.04 127. &gt; 3 102. 1 442 110. 31.9 11.0 118. &gt; 4 36.9 1 219 110. 27.9 5.28 95.5 &gt; 5 91.7 0 450 119. 36.7 9.30 122. &gt; 6 146. 1 255 85.6 23.9 7.05 102. About the data: attention: sustained attention score (higher = better attention) sex: 0 = female, 1 = male blueberries: average number of blueberries consumed per year iq: the individual’s IQ age: age of person in years sleep: average hours of sleep per night memory_score: memory test score The researcher wants to test whether attention and sleep predict memory_score. She is already aware of a well-established finding in the literature, which is that iq and age predict memory_score. She therefore wants to use a hierarchical regression approach to determine whether attention and sleep explain additional variance in memory_score over and above iq and age. 1. First, fit a linear model to determine the extent to which memory_score is predicted by iq and age. Store the results in memory1. Try first, then click to see the code # specify the baseline model memory1 &lt;- lm(memory_score ~ iq + age, data = memory_data) # see the model results summary(memory1) &gt; &gt; Call: &gt; lm(formula = memory_score ~ iq + age, data = memory_data) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -44.154 -11.754 0.732 11.608 40.790 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) 71.1669 9.0796 7.838 1.67e-13 *** &gt; iq 0.1073 0.0699 1.534 0.126 &gt; age 0.8220 0.1461 5.627 5.27e-08 *** &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 16.1 on 231 degrees of freedom &gt; Multiple R-squared: 0.1303, Adjusted R-squared: 0.1228 &gt; F-statistic: 17.31 on 2 and 231 DF, p-value: 9.875e-08 2. Next, add attention and sleep to the model, storing your results in memory2. Try first, then click to see the code # specify the next model memory2 &lt;- lm(memory_score ~ iq + age + attention + sleep, data = memory_data) # show the results summary(memory2) &gt; &gt; Call: &gt; lm(formula = memory_score ~ iq + age + attention + sleep, data = memory_data) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -28.935 -8.555 1.713 8.450 31.384 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) 9.60112 8.57889 1.119 0.264246 &gt; iq 0.18673 0.05451 3.426 0.000726 *** &gt; age 0.86579 0.11308 7.656 5.32e-13 *** &gt; attention 0.22894 0.02757 8.302 8.88e-15 *** &gt; sleep 3.68609 0.39328 9.373 &lt; 2e-16 *** &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 12.46 on 229 degrees of freedom &gt; Multiple R-squared: 0.4839, Adjusted R-squared: 0.4749 &gt; F-statistic: 53.68 on 4 and 229 DF, p-value: &lt; 2.2e-16 3. Now, compare the memory1 and memory2 models using anova() Try first, then click to see the code anova(memory1, memory2) &gt; Analysis of Variance Table &gt; &gt; Model 1: memory_score ~ iq + age &gt; Model 2: memory_score ~ iq + age + attention + sleep &gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) &gt; 1 231 59912 &gt; 2 229 35554 2 24359 78.447 &lt; 2.2e-16 *** &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Answer the following questions: A model with iq and age as predictors explains % of the variance in memory_scores A model with iq, age, attention and sleep as predictors explains % of the variance in memory_scores Calculate the additional variance explained by the second model: Change in \\(R^2\\) = % The ANOVA comparing models can be reported as: F(, ) = , p &lt; .001. Is there a statistically significant improvement in the prediction of memory_scores as a result of adding attention and sleep to the model? no yes Now use Bayes Factors to determine how much more likely the memory2 model is than the memory1 model . Try first, click here for a reminder of the steps Determine the Bayes Factor for memory1 Determine the Bayes Factor for memory2 Compare the Bayes Factors for memory2 and memory1 Try first, click here to see the code # Store the Bayes Factor for the first model in memory1.BF memory1.BF &lt;- lmBF(memory_score ~ iq + age, data = as.data.frame(memory_data) ) # Store the Bayes Factor for the second model in memory2.BF memory2.BF &lt;- lmBF(memory_score ~ iq + age + attention + sleep, data = as.data.frame(memory_data) ) # Compute the Bayes Factors for memory2.BF vs memory1.BF memory2.BF / memory1.BF &gt; Bayes factor analysis &gt; -------------- &gt; [1] iq + age + attention + sleep : 4.168455e+23 ±0% &gt; &gt; Against denominator: &gt; memory_score ~ iq + age &gt; --- &gt; Bayes factor type: BFlinearModel, JZS Answer the following questions: The Bayes Factor comparing memory2 and memory1 to (2 decimal places) is e+ . Does the Bayes Factor support the conclusions from the ANOVA? no yes Click for answer Yes! The Bayes Factor is equal to \\(4.17 \\times 10^{23}\\), and this therefore strongly supports the inclusion of attention and sleep in the model already containing iq and age. Extra exercises, if there’s time 1. The researcher wishes to predict the memory_score for a new individual with iq = 105, age = 27, attention = 90, sleep = 8. Determine the prediction. Hint: in a previous session, you have previously used the predict() function to do this. The predicted memory_score is Try first, then click to show the code for the answer # create tibble for the new data new_data &lt;- tibble(iq = 105, age = 27, attention = 90, sleep = 8) # use predict to derive prediction from new data predict(memory2, new_data) &gt; 1 &gt; 102.6768 2. The researcher is interested to know whether annual consumption of blueberries has any bearing on memory_scores, and so wants to add blueberries to the model in memory2. Determine the Bayes Factor comparing memory2 with a model that additionally contains blueberries. The Bayes Factor for the model comparison is (to 2 decimal places) The Bayes Factor indicates that the model with blueberries is more likely less likely than the model without it. Should the researcher add blueberries to the model? no yes if it tastes good Try yourself first, then click for the code # add blueberries to memory2; store in memory3.BF memory3.BF &lt;- lmBF(memory_score ~ iq + age + attention + sleep + blueberries, data = as.data.frame(memory_data) ) # calculate the BF for memory3 vs memory2 memory3.BF / memory2.BF &gt; Bayes factor analysis &gt; -------------- &gt; [1] iq + age + attention + sleep + blueberries : 0.1663574 ±0% &gt; &gt; Against denominator: &gt; memory_score ~ iq + age + attention + sleep &gt; --- &gt; Bayes factor type: BFlinearModel, JZS "],
["summary-of-key-points-1.html", "Summary of key points", " Summary of key points We can compare a model with one that has more predictors by using anova(model1, model2). We can compare models using Bayes Factors with lmBF in the BayesFactor package. A Bayes Factor is probability of one model relative to another, given the data. To compare Bayes Factors of models: First obtain the Bayes Factors for model1 and model2. Then use model2 / model1 to get the Bayes Factor, indicating how much more likely model2 is. Bayes Factors less than 1 indicate evidence for model1 Bayes Factors greater than 1 indicate evidence for model2 We can report Bayes Factors as \\(BF_{10}\\) = 2.23 (or BF10 = 2.23) Next week’s session will build on what was done in this session, so make sure you understand what was covered and ask the instructor to clarify anything you’re unsure of. "]
]
