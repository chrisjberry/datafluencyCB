# Building models 2


### In brief

> In this session we discuss model selection in the context of ANOVA and the use
> of Bayes Factors to choose between theoretically interesting models.


```{r, echo=F, include=F}
knitr::opts_chunk$set(echo = TRUE, collapse=TRUE, cache=TRUE, comment=">", message=FALSE)
library(tidyverse)
library(webex)
library(pander)
theme_set(theme_minimal())
```




## Using ANOVA and Bayes Factors to compare models

-   [**Slides from the session**](slides/regression2-chris.ppt) (available 25-01-2020)

\

### Overview

In the previous session, we saw that we can construct a linear model to predict an outcome variable (e.g., *final exam score* from *entrance exam score*). We also investigated how we can _improve_ a model by adding predictors to it.

\
How do we know if one model is _better_ or should be _preferred_ over another model? We touched on a common sense approach in the last session - we ideally want models that explain the variance in an outcome variable but each predictor in the model should make a sizable and relatively independent contribution to the model.

\

Today we will cover a more formal approach to model comparison using:

  - **ANOVA (Analysis of Variance)** and

  - **Bayes Factors**
  
\

It's important that you are comfortable with the material from the first [Building Models 1 session](building-models-1.html) before proceeding today.


## Comparing models using ANOVA

We can use ANOVA to know whether the addition of variables into a model leads to a statistically significant improvement in the variance explained by the model _as a whole_. 


We may want to do this, for example, when building on existing theories or models.

\
We'll start by comparing a model with _one_ predictor vs. a model with _three_ predictors.

\
First, let's load the `ExamData` from the previous session, and then conduct:

- a linear model with `finalex` as the outcome variable, and `entrex` as the predictor.

- a linear model with `finalex` as the outcome variable, and `entrex`,`age`, and `project` as the predictors.

```{r}

ExamData <- read_csv('https://bit.ly/37GkvJg')              
model1   <- lm(finalex ~ entrex, data = ExamData)           
model2   <- lm(finalex ~ entrex + age + project, data = ExamData) 

```

\
**Explanation of the code**: first the data is loaded into `ExamData`. The results of the simple regression are stored in `model1`. Those of the multiple regression are stored in `model2`.

\
Use `summary()` to display the results of each regression:

**Model 1:**

```{r}
summary(model1)
```

\
**Model 2:**
```{r}
summary(model2)
```

\
(If you are not sure what it means by "e-07" in the output above then see the FAQs [here](#e-meaning))

:::{.exercise}

Make note of the variance explained by each model ($R^2$), i.e., `Multiple R-squared`: (report as a percentage, to 2 decimal places)

- Model 1: $R^2$ = `r fitb(answer="53.10",  num=TRUE)` %

- Model 2: $R^2$ = `r fitb(answer="58.69",  num=TRUE)` %

Which model explains a greater proportion of variance in `finalex`? `r mcq(c("extrex alone", answer="entrex, age, project"))`

- Calculate the difference in $R^2$ between the models. `model2` improves the prediction of `finalex` by  `r fitb(answer="5.59",  num=TRUE)` %

:::

\
To compare the variance explained by each model, use `anova()`:

```{r}
anova(model1, model2)
```

\
**Explanation:** 

- **`anova()`** compares the variance that `model1` and `model2` explain with an _F_-statistic. 

- **`Pr(>F)`** gives the _p_-value for this statistic. If the _p_-value is less than .05, then we can reject the null hypothesis that there is no difference in the variance explained by each model, and we can say that the variance that `model2` explains in `finalex` is significantly greater than that of `model1`.

We can report the _F_-statistic in APA style as _F_(2, 29) = 1.96, p = .16. We can say that the additional 5.59% variance that `model2` explains relative to `model1` does not represent a statistically significant increase, and so `model2` should **not** be preferred over `model1`.


\

:::{.tip}
Comparing models sequentially in this way is sometimes referred to as **hierarchical regression** or **sequential regression**. This type of regression is usually used for logical or theoretical reasons, when we want to know contribution of predictor (or a set of predictors) _over and above_ an existing one. 
:::


:::{.exercise}

Now you try:

The variable `attendance` in `ExamData` scores individuals according to whether their class attendance was low (0) or high (1). A researcher suspects that `attendance` may explain additional variance in `finalex` over and above `entrex`.

As an exercise, compare the following two models using the `anova()` approach above: 

1. a model with `extrex` as a sole predictor of `finalex` (i.e., `model1`), and 

2. a model where `finalex` is predicted by `extrex` and `attendance` (call this `model3`). 

Is there sufficient evidence that a model with `entrex` _and_ `attendance` explains more variance than a model with `entrex` alone?

`r hide("Show me the code")`
```{r}
model3 <- lm(finalex ~ entrex + attendance, data = ExamData)

summary(model3)

anova(model1, model3)
```
`r unhide()`

- The variance explained by a model with `entrex` alone is $R^2$ = `r fitb(answer="53.10",  num=TRUE)` %

- The $R^2$ for the model that also included `attendance` was $R^2$ = `r fitb(answer="72.23",  num=TRUE)` %  

- The increase in $R^2$ was `r fitb(answer="19.13",  num=TRUE)`% 

- The increase in $R^2$ was `r mcq(c(answer = "statistically significant", "not significant"))`, _F_(`r fitb(answer=2,  num=TRUE)`, `r fitb(answer=30,  num=TRUE)`) = `r fitb(answer="39.02",  num=TRUE)`, p < .001. 

- As indicated by the estimates of the coefficients for `entrex` and `attendance`, both  `r mcq(c("negatively", answer = "positively"))` predict `finalex`: a higher `entrex` score and greater `attendance` is associated with a `r mcq(c(answer = "higher", "lower"))` `finalex` score.

`r unhide()`

:::


## Comparing models using Bayes Factors

An alternative approach to using ANOVA to compare models is to use **Bayes Factors**. 


A **Bayes Factor** is the probability of obtaining the data under one model compared to another.  For example, a Bayes Factor equal to 2 would tell us that the data are twice as likely under one model than another. 


Unlike classical tests of statistical significance (with _p_-values), Bayes Factors also allow us to quantify evidence for the null hypothesis. 


To compute a Bayes Factor for a specific linear model, we can use `lmBF` in the `BayesFactor` package. First, we need to load the `BayesFactor` package: 

```{r}
library('BayesFactor')
```

The `BayesFactor` package contains the `lmBF` function (where `lm` stands for "linear model" and `BF` stands for "Bayes Factor"). We can use the `lmBF` function in the same way we use the `lm` function, except that it will return a Bayes Factor for the model we specify.

Let's determine the Bayes Factor for `model1`

```{r}
model1.BF <- lmBF(finalex ~ entrex, data = as.data.frame(ExamData) )  
```

**Explanation of the code**: The linear model is specified in exactly the same way as we did with `lm`. Due to a limitation of the package, however, we must convert `ExamData` from a tibble to a data frame using `as.data.frame`. Otherwise the command works in the same way. The results are stored in `model1.BF`.

To look at what's stored in `model1.BF`:

```{r}
model1.BF
```

**Explanation of the output**: 

- The Bayes Factor provided for the model with `entrex` is equal to 8310.846. 

- The `Against denominator: Intercept only` means that the model with `entrex` is being compared with a model that contains an intercept only. An intercept-only model is a model in which the coefficient for `entrex` is equal to zero; that is, the regression line is a flat line (equal to the _mean_ of `entrex`). 

- The value of the Bayes Factor indicates that the model with `entrex` in is much more likely than a model that contains only an intercept (8310.846 times more likely, to be precise). We can be confident that a model with `entrex` is preferable to the intercept only model!


Now let's do the same for `model2`:

```{r}
# specify the model
model2.BF <- lmBF(finalex ~ entrex + age + project, data = as.data.frame(ExamData) )

# show the model
model2.BF
```

**Explanation:** The Bayes Factor is equal to 2427.68. Again, this indicates that the model with `entrex` and `age` is much more likely than a model with only the intercept in (this is not surprising given the result for `model1.BF`). 

But, what we want to know is whether `model2` (with `entrex` and `age`) is **more** likely than `model1` (with just `entrex`). We can determine this by comparing the Bayes Factors for each model as follows:

```{r}

model2.BF / model1.BF

```

**Explanation:** The Bayes Factor for this comparison is 0.29. This means that `model2` is less than a third as likely than `model1`. So, `model2` is _less_ likely than `model1` (i.e., not good news for `model2`!).


:::{.tip}

**Interpreting the Bayes Factor**

- A Bayes Factor **equal to 1** tells us that probability of each model is the same.

- A Bayes Factor **greater than 1** means that `model2` is more likely than the `model1`.

- A Bayes Factor **less than 1** means that `model1` is more likely than `model2`. 

Thus, our Bayes Factor of 0.51 indicates that `model1` is more likely than `model2`.

:::


:::{.tip}

**Reporting Bayes Factors**:

We usually write the Bayes Factor in reports as $BF_{10}$ where: 

- the subscript **1** in $BF_(10}$ denotes the less-constrained model (i.e., the alternative hypothesis). This is the model with **more predictors** (or `model2`, in our case).

- the subscript **0** in $BF_(10}$ denotes the more constrained or simpler model (i.e., the null hypothesis). This is the model with **fewer predictors** (or `model1`, in our case).


\

**The Size of the Bayes Factor**:

If the Bayes Factor is **less than 0.33** (i.e., $BF_{10}$ < 0.33), we usually say that there is **substantial evidence for `model1`** (the more constrained model).

If the Bayes Factor is **greater than 3** (i.e., $BF_{10}$ > 3), we say that there is **substantial evidence for `model2`** (the less constrained model).

We say that intermediate values for the Bayes Factor (i.e., $BF_{10}$ between 0.33 and 3) don't offer strong evidence either way.

:::

Thus, because our Bayes Factor of 0.29 is less than 1, this indicates greater evidence for `model1` than `model2`. Also, because the Bayes Factor is less than 0.33, we have substantial evidence for `model1` over `model2`.

It's becoming increasingly common to report the Bayes Factor alongside the results of a classical analysis. Thus, we could report our results as follows: "There was insufficient evidence that the addition of `age` and `project` to the model containing `entrex` resulted in an increase in $R^2$, _F_(2, 30) = 39.02, _p_ < .001; $BF_{10}$ = 0.29."


:::{.exercise}

Now you have a go.

To supplement the comparison of `model3` and `model1` that you did with `anova`, now compute the Bayes Factor for `model3` vs. `model1`.

You'll need to take the following steps:

1. Obtain the Bayes Factor for a model with `extrex` as a sole predictor of `finalex` (we did this already above; it's stored in `model1.BF`) 

2. Obtain the Bayes Factor for a model where `finalex` is predicted by `extrex` and `attendance` and store this in `model3.BF`. 

3. Compute the Bayes Factor comparing `model3.BF` and `model1.BF`.

`r hide("Try yourself first, then click here for the code")`

```{r}
# 1. show the BF for model1 vs. intercept only
model1.BF  

# 2. Obtain the BF for model3 vs. intercept only, then show it
model3.BF <- lmBF(finalex ~ entrex + attendance, data = as.data.frame(ExamData) )
model3.BF

# 3. Compare the BFs for model3 vs model1
model3.BF / model1.BF
```

`r unhide()`

Answer the following questions from the output:

How much more likely is a model with`entrex` than an intercept only model? 

- `r fitb(answer=8310.85, num=TRUE)` times more likely.

How much more likely is a model with `entrex` and `attendance` than an intercept only model? 

- `r fitb(answer=2351114, num=TRUE)` times more likely.

How much more likely is a model with `entrex` and `attendance` as predictors than a model with `entrex` alone? 

- `r fitb(answer=282.90, num=TRUE)` times more likely.

There is `r mcq(c("insufficient", answer = "strong"))` evidence that `model3` should be preferred over `model1`, given the data.

:::


## Exercise

Now you will practise using ANOVA and Bayes Factors to compare models with a new dataset.

**Scenario:** A researcher would like to construct a model to predict scores in a memory task from several different variables. The data from 234 individuals are stored in the `memory_data` dataset, which are located at https://bit.ly/37pOTrC. 

:::{.exercise}

Load in the data at the link above to the variable `memory_data` and preview it with `head()`.


`r hide("Try this yourself first. Click to show code")`

```{r}
memory_data <- read_csv('https://bit.ly/37pOTrC')
memory_data %>% head()
```

`r unhide()`




:::{.tip}

**About the data:**

  **attention**: sustained attention score (higher = better attention)

  **sex**: 0 = female, 1 = male

  **blueberries**: average number of blueberries consumed per year

  **iq**: the individual's IQ

  **age**: age of person in years

  **sleep**: average hours of sleep per night

  **memory_score**: memory test score

:::

The researcher wants to test whether `attention` and `sleep` predict `memory_score`. She is a aware of a well-established finding in the literature that `iq` and `age` predict `memory_score`. She therefore wants to use a hierarchical regression approach to determine whether `attention` and `sleep` explain additional variance in `memory_score` _over and above_ `iq` and `age`.


First, fit a linear model to determine the extent to which `memory_score` is predicted by `iq` and `age`. Store the results in `memory1`.


`r hide("Try first, then click to see the code")`
```{r}
memory1 <- lm(memory_score ~ iq + age, data = memory_data)
summary(memory1)
```
`r unhide()`

\
Next, add `attention` and `sleep` to the model, storing your results in `memory2`.


`r hide("Try first, then click to see the code")`
```{r}
memory2 <- lm(memory_score ~ iq + age + attention + sleep, data = memory_data)
summary(memory2)
```
`r unhide()`

\
Now, compare the `memory1` and `memory2` models using `anova()`

`r hide("Try first, then click to see the code")`
```{r}
anova(memory1, memory2)
```
`r unhide()`



\
Answer the following questions:

A model with `iq` and `age` as predictors explains `r fitb(answer="13.03",  num=TRUE)` % of the variance in `memory_scores`

A model with `iq`, `age`, `attention` and `sleep` as predictors explains `r fitb(answer="48.39",  num=TRUE)` % of the variance in `memory_scores`

Calculate the additional variance explained by the second model: Change in $R^2$ = `r fitb(answer="35.36",  num=TRUE)` %

Is there a statistically significant improvement in the prediction of `memory_scores` as a result of adding `attention` and `sleep` to the model? `r mcq(c("no", answer="yes"))`

\

Determine how much more likely the `memory2` model is than the `memory1` model by computing the Bayes Factor.


`r hide("Click here if you need a reminder of the steps to take")`

- Determine the Bayes Factor for `memory1`

- Determine the Bayes Factor for `memory2`

- Compare the Bayes Factors for `memory2` and `memory1`

`r unhide()`


`r hide("Click here to see the code for this")`

```{r}
# Store the Bayes Factor for the first model in memory1.BF
memory1.BF <- lmBF(memory_score ~ iq + age, data = as.data.frame(memory_data) )

# Store the Bayes Factor for the second model in memory2.BF
memory2.BF <- lmBF(memory_score ~ iq + age + attention + sleep, data = as.data.frame(memory_data) )

# Compute the Bayes Factors for memory2.BF vs memory1.BF
memory2.BF / memory1.BF
```
`r unhide()`

The Bayes Factor to (2 decimal places) is `r fitb(answer="4.16",  num=TRUE)` e+ `r fitb(answer="23",  num=TRUE)`.

Does the Bayes Factor support the conclusions from the ANOVA? `r mcq(c("no", answer="yes"))`

`r hide("click for answer")`
Yes! The Bayes Factor is equal to $4.16 \times 10^{23}$, and therefore strongly supports the inclusion of `attention` and `sleep` in the model already containing `iq` and `age`.
`r unhide()`


### Extra exercises

1.

The researcher wishes to predict the `memory_score` for an individual with `iq` = 105, `age` = 27, `attention` = 90, `sleep` = 8. Determine the prediction. Hint: in a previous session, you have previously used the `predict()` function to do this.

The predicted `score`memory_score` is `r fitb(answer="121",  num=TRUE)`

`r hide("click to show how this was determined")`
```{r}
new_data <- tibble(iq = 205, age = 27, attention = 90, sleep = 8)
predict(memory2, new_data)
```
`r unhide()`


2.

The researcher is interested to know whether annual consumption of blueberries has any bearing on `memory_scores`, and so wants to add the `blueberries` variable to the model.

Determine the Bayes Factor comparing `memory2` with a model that additionally contains `blueberries`. 

- The Bayes Factor for the model comparison is `r fitb(answer="0.17",  num=TRUE)` (to 2 decimal places)

- Should the researcher add `blueberries` to the model? `r mcq(c(answer="no", "yes"))`

`r hide("Try yourself first, then click for the code")`
```{r}
# add blueberries to memory2
memory3.BF <- lmBF(memory_score ~ iq + age + attention + sleep + blueberries, data = as.data.frame(memory_data) )

# calculate the BF for memory3 vs memory2
memory3.BF / memory2.BF
```
`r unhide()`

:::


## Summary of key points

- We can compare two models using `anova(model1, model2)`.

- We compare models using Bayes Factors with `lmBF` in the `BayesFactor` package.

- A Bayes Factor is probability of one model relative to another, given the data.

- To compare Bayes Factors of models:

  - First obtain the Bayes Factors for `model1` and `model2`. 
  
  - Then obtain the probability of `model2` over `model1` using `model2 / model1`
  
  - Bayes Factors less than 1 indicate evidence for `model1`
    
  - Bayes Factors greater than 1 indicate evidence for `model2`
    
  - We can report Bayes Factors as $BF_{10}$ = 2.23 (or BF10 = 2.23)
