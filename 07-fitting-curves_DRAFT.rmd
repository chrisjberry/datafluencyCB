# Fitting curves

### In brief

> So far all our regression models have assumed that our variables have
> **_linear relationships_**. That isn't always the case, and sometimes we need
> to fit curved lines to describe the relationship of predictors and outcomes.
> As we saw before, fitting curved lines has costs as well as benefits: A curved
> line is more likely to **overfit** the data, and may be less good at
> predicting new data. But for some models curved lines are essential to
> describe the world as it really is.


```{r, echo=F, include=F}
knitr::opts_chunk$set(echo = TRUE, collapse=TRUE, cache=TRUE, comment=">", message=FALSE)
library(tidyverse)
library(webex)
library(pander)
theme_set(theme_minimal())
```

\

## Using polynomials to fit curves

-   [**Slides from the session**](slides/regression3-chris.ppt) (available 02-02-2020)

\

### Overview



In this session we will:

- See how we can add *polynomial terms* such as $x^2$, $x^3$ to a regression model to capture non-linear relationships.

- Use ANOVA and Bayes Factors to determine whether these terms improve the model.


You should be comfortable with what we did in the previous Building Models 1 and Building Models 2 sessions before attempting this one.

\

## Polynomials

\

The regression models we have been fitting assume a _linear_ (i.e., straight line) relationship between variables. However, variables may not always be related in a linear fashion.

\

Suppose variables x and y showed the following trend:

```{r fig1, fig.height = 3.5, fig.width = 3.5, fig.align = "center",echo=F}
set.seed(1)
d <- tibble(x=seq(-6,6,0.2),
            y=3.5 + 5*x - 8*x^2)

e <- tibble(
  x=seq(-6,6,0.2),
  y=3.5+5*x-8*x^2 + rnorm(length(x),0,50),
  y2=3.5+5*x-8*x^2 + rnorm(length(x),0,50),
  y3=3.5+5*x-8*x^2 + rnorm(length(x),0,50)
)


d %>% ggplot(aes(x,y)) +
      #geom_line(aes(y=y),size=1) + 
      theme_bw() +
      geom_point(aes(y=e$y)) +   
      theme(axis.text.x=element_blank(),
            axis.ticks.x=element_blank(),
            axis.text.y=element_blank(),
            axis.ticks.y=element_blank()
            )
  
```

It is clear that this relationship would not be explained well by a straight line. We'd lose important information about the relationship if we only fit a straight line. A curve would be better.

\

We can fit a curve to the data by adding **polynomial** terms to the regression equation. 

\

**Polynomial** means that a variable is raised to a particular power. For example: 

- $x^2$ means x-squared, which is x-multiplied-by-x, or "x to the power of two"

- $x^3$ means x-cubed, which is x-multiplied-by-x-multiplied-by-x, or "x to the power of three"


If a model has a **quadratic component** it means it has an $x^2$ term in the equation. 

If a model has a **cubic component** it means it has an $x^3$ term in the equation.


### Constant

To see why this approach works, recall that lines can be represented by equations.


The equation **$y = 1 + 0.5(x)$** would be represented as follows:


```{r,fig4, fig.height = 3.5, fig.width = 3.5, fig.align = "center",echo=F}
d<-tibble(x=seq(-10,10,0.1),y=1+0.5*x,a=rep(1,length(x)),b=0.5*x)
d %>% ggplot(aes(x=x)) + 
  geom_line(aes(y=y),size=2,colour='cornflower blue') +
  geom_line(aes(y=0)) +
  geom_line(aes(y=x,x=0)) +
  geom_line(aes(y=a),linetype=2,size=1,colour='cornflower blue') +
  geom_line(aes(y=b),linetype=2,size=1,colour='cornflower blue') +
  theme_bw()
```

We can think of this line as being made up of the constant and a linear component.

- The constant in this equation is indicated by the dashed horizontal line.
- The linear component to this equation 0.5(x) is indicated by the dashed slope line.
- The solid blue line is a combination of these two components.

It has no bends.
\

### Quadratic

The equation **$y = x^2$** would be represented as follows:

```{r fig5, fig.height = 3.5, fig.width = 3.5, fig.align = "center",echo=F}
d<-tibble(x=seq(-10,10,0.1),y=x^2)
d %>% ggplot(aes(x=x)) + 
  geom_line(aes(y=y),size=2,colour='cornflower blue') +
  geom_line(aes(y=0)) +
  geom_line(aes(y=x*10,x=0)) +
  theme_bw()
```

To get each value of y, we square the value of x. 

So, when x = -5, y is 25. And if x = -4, y = 16, and so on...

\

**$y = -x^2$**,  would look as follows:

```{r fig6, fig.height = 3.5, fig.width = 3.5, fig.align = "center",echo=F}
d<-tibble(x=seq(-10,10,0.1),y=-(x^2))
d %>% ggplot(aes(x=x)) + 
  geom_line(aes(y=y),size=2,colour='cornflower blue') +
  geom_line(aes(y=0)) +
  geom_line(aes(y=x*10,x=0)) +
  theme_bw()
```

Curves with quadratic components have one bend.

\

### Cubic

The equation **$y = x^3$** would be represented as follows:

```{r fig7, fig.height = 3.5, fig.width = 3.5, fig.align = "center",echo=F}
d<-tibble(x=seq(-10,10,0.1),y=x^3)
d %>% ggplot(aes(x=x)) + 
  geom_line(aes(y=y),size=2,colour='cornflower blue') +
  geom_line(aes(y=0)) +
  geom_line(aes(y=x*10,x=0)) +
  theme_bw()
```

To get each value of y, we cube the value of x. 

So, when x = -5, y is -125. And if x = -4, y = -64, and so on...


**$y = -x^3$**,  would look as follows:

```{r fig8, fig.height = 3.5, fig.width = 3.5, fig.align = "center",echo=F}
d<-tibble(x=seq(-10,10,0.1),y=-x^3)
d %>% ggplot(aes(x=x)) + 
  geom_line(aes(y=y),size=2,colour='cornflower blue') +
  geom_line(aes(y=0)) +
  geom_line(aes(y=x*10,x=0)) +
  theme_bw()
```

Curves with cubic components have three bends.


\

### Linear plus quadratic components

The equation **$y = 50 + 5(x) - x^2$** has 

- a constant equal to 50
- a linear component 5(x)
- a quadratic component $-x^2$:

```{r fig9, fig.height = 3.5, fig.width = 3.5, fig.align = "center",echo=F}
d<-tibble(x=seq(-10,10,0.1),y=50+5*x-x^2,a=rep(50,length(x)),b1=5*x,b2=-(x^2))
d %>% ggplot(aes(x=x)) + 
  geom_line(aes(y=y),size=2,colour='cornflower blue') +
  geom_line(aes(y=0)) +
  geom_line(aes(y=x*10,x=0)) +
  geom_line(aes(y=a),linetype=2,size=1,colour='grey') +
  geom_line(aes(y=b1),linetype=2,size=1,colour='grey') +
  geom_line(aes(y=b2),linetype=2,size=1,colour='cornflower blue') +
  theme_bw()
```

The dashed lines on the plot indicate the intercept, linear component, and quadratic components of the equation. The solid line represents the equation.

\

### Linear plus quadratic and cubic components

The equation **$y = 5000 + 750x - 50x^2 - 20x^3$** has 

- a constant equal to 50
- a linear component 750x
- a negative quadratic component $-50x^2$
- a negative cubic component $-20x^3$

```{r fig10, fig.height = 3.5, fig.width = 3.5, fig.align = "center", echo=FALSE}
d<-tibble(x=seq(-10,10,0.1),y=5000+750*x-50*x^2-20*x^3,a=rep(5000,length(x)),b1=750*x,b2=-(50*x^2),b3=-20*x^3)

d %>% ggplot(aes(x=x)) + 
  geom_line(aes(y=y),size=2,colour='cornflower blue') +
  geom_line(aes(y=0)) +
  geom_line(aes(y=x*10,x=0)) +
  geom_line(aes(y=a),linetype=2,size=1,colour='grey') +
  geom_line(aes(y=b1),linetype=2,size=1,colour='grey') +
  geom_line(aes(y=b2),linetype=2,size=1,colour='cornflower blue') +
  geom_line(aes(y=b3),linetype=2,size=1,colour='brown') +
  theme_bw()
```


When we see any curve, it is possible to think of it as being composed of components like this.



## Identifying polynomial components


To determine whether a model should have quadratic, cubic, or higher order components, we can use the sequential regression approach covered in the previous session. We take the following steps, and look at the change in $R^2$ assocaited with each step.

- First fit the linear model

- then test for the addition of the quadratic $x^2$ component

- then test for the addition of the cubic $x^3$ component


Let's see this in action!


:::{.tip}

**Learning tip**

Try storing all your code in an R Markdown file today if you are not doing so already!

:::

\
We'll use a dataset inspired by a 2016 survey of the National Office for Statistics investigated happiness across the life span. Approximately 300,000 individuals of all ages answered questions related to well-being. A BBC news report of the study can be found [here](https://www.bbc.co.uk/news/uk-35471624).


Each participant answered the following question regarding happiness:

:::{.tip}
"Overall, how happy did you feel yesterday?  Where 0 is 'not at all happy' and 10 is 'completely happy'."
:::

This `happy` dataset are located at "https://bit.ly/2uIxM5K".



Load the data into R, and preview the data using `glimpse`: 


```{r}
# load the data
SurveyData <- read_csv("https://bit.ly/2uIxM5K")

# preview it
SurveyData %>% glimpse()
```



Plot the relationship between age and happiness:

```{r fig11, fig.height = 3.5, fig.width = 3.5, fig.align = "center",echo=F}
SurveyData %>%
  ggplot(aes(x=age, y=happiness)) +
  geom_point() 
```


:::{.exercise}
If you had to guess at this stage, which components seem to be present in the relationship between `happiness` and `age`?

Linear: `r mcq(c("maybe no", answer="maybe yes"))`

Quadratic: `r mcq(c("maybe no", answer="maybe yes"))`

Cubic: `r mcq(c(answer= "maybe no", "maybe yes"))`

\

Try to describe the relationship in psychological terms.

`r hide("Try first, click here for a description")`
Happiness of individuals declines from 30 years to the late forties. Happiness increases beyond the late forties, reaching its peak at 70 years, at which age people reported the highest levels of happiness. 
`r unhide()`
:::

### Linear regression

To determine whether there is a linear component, run a simple regression to determine whether `happiness` can be predicted from `age`:

```{r}
pmod1 <- lm(happiness ~ age, data = SurveyData)
summary(pmod1)
```

**Explanation:** I stored the model in `pmod1`, as shorthand for "polynomial model 1".


:::{.exercise}

What percentage of the variance in `happiness` scores is explained by `age`? `r mcq(c("0.44",answer="43.39","29.12"))` %

Is `age` a statistically significant predictor of `happiness` `r mcq(c("no", answer="yes"))`

:::

\

To see how well the happiness values predicted by the regression equation match the trends in our data.  Plot the regression line from `model1` on the same plot as the data:


```{r fig12, fig.height = 3.5, fig.width = 3.5, fig.align = "center",echo=F}
SurveyData %>% 
  mutate(fit1 = fitted(pmod1)) %>% 
  ggplot(aes(x=age, y=happiness)) + 
  geom_point() +
  geom_line(aes(y=fit1))
```

**Explanation of the code:** The above code uses the `mutate` function from the `dplyr` package to add an additional column named `fit1` to the SurveyData. `fit1` contains the values of happiness that are predicted by the model, given each individual's age.



:::{.exercise}
By visual inspection, does the model capture the trend in the data well? `r mcq(c("yes!", answer="not really"))`
:::

According to the model, happiness increases with age in a _linear_ fashion. However, it is clear from the plot that this straight line is **not** capturing the non-linear trend that is evident in our data. 


### Adding a quadratic component

We can add a quadratic component to the regression model using the `poly()` function. 

```{r}

pmod2 <- lm(happiness ~ poly(age,2), data = SurveyData)
summary(pmod2)

```

**Explanation:** Using `poly()` saves us lots of typing. The '2' in the `poly()` function tells R that we want to fit a model with a quadratic component. R will then fit a model of the following form:


$predicted happiness = a + b_1(age) + b_2(age^2)$


where $a$ is the intercept, and $b_1$ and $b_2$ are the coefficients for the linear and quadratic components, respectively. 


R will fit this model, and provide estimates of $a$, $b_1$ and $b_2$.


To visually inspect the fit of the second model use `mutate` and `fitted` to derive the predicted values, as before. The regression line should now appear curved:


```{r fig13, fig.height = 3.5, fig.width = 3.5, fig.align = "center",echo=F}
SurveyData %>% 
  mutate(fit2 = fitted(pmod2)) %>% 
  ggplot(aes(x = age, y = happiness)) + 
  geom_point() +
  geom_line(aes(y = fit2))
```



:::{.exercise}
What percentage of the variance in `happiness` does a model with a quadratic component of `age` explain? `r fitb(answer=68.53,  num=TRUE)` %


Compare the value of $R^2$ in `pmod1` and `pmod2`. 

Does the addition of a quadratic component result in an increase in R-Squared in model 2?  `r mcq(c(answer = "yes", "no"))`

What is the change in $R^2$? `r fitb(answer=25.14,  num=TRUE)` % (to 2 decimal places)


`r hide("Click to see how the answer is calculated")`

_R_-squared change from pmod1 to pmod2 = 68.53 - 43.39 = 25.14%

Therefore, the model with the quadratic component accounts for 25.14% more variance in `happiness`.

`r unhide()`
:::

We can test whether the _increase_ in _R_-squared in `model2` represents a statistically significant increase or not by comparing the models using an ANOVA:


```{r}

anova(pmod1, pmod2)

```

`anova` computes a new _F_-statistic, testing whether the _increase_ in variance explained by `model2` is statistically significant or not. 


:::{.exercise}
By examining the _F_-statistic, is there sufficient evidence for an improvement in fit in the model as a result of adding in the quadratic component? `r mcq(c(answer = "yes", "no"))`


`r hide("Answer")`
We can report the improvement in fit as follows: 

A model with a quadratic component of age accounted for a statistically significantly greater proportion of variance in happiness than a model with only a linear component, _F_(1,148) = 117.00, _p_ < .001.
`r unhide()`

:::

### Adding a cubic component


Now run a model with a cubic component:


```{r}
pmod3 <- lm(happiness ~ poly(age,3), data = SurveyData)
summary(pmod3)
```

The '3' in `poly(age,3)` tells R that we want to specify a model with a cubic component, of the form: 


$happiness = a + b_1(age) + b_2(age^2) + b_3(age^3)$


:::{.exercise}
What percentage of the variance in `happiness` does a model with a cubic component of `age` explain? `r fitb(answer=68.80,  num=TRUE)` %

Compare the value of $R^2$ in `pmod3` and `pmod2`. 

Does the addition of a quadratic component result in an increase in $R^2$ in `pmod3`?  `r mcq(c(answer = "yes", "no"))`

What is the increase in _R_-squared as a result of adding in the cubic component? (Compare $R^2$ between `pmod3` and `pmod2`).

The increase in R-squared is `r fitb(answer=0.27, num=TRUE)` %
:::

To determine if this change in _R_-squared is statistically significant or not, we can again use `anova()`:


```{r}

anova(pmod2, pmod3)

```

:::{.exercise}
Is the increase in $R^2$ a statistically significant increase? `r mcq(c(answer="no", "yes"))`

`r hide('Description of the answer')`
The _F_-statistic comparing `pmod3` and `pmod2` is not statistically significant, _F_(1, 146) = 1.27, _p_ = .26, indicating that the addition of the cubic component of age into the regression model does not result in a statistically significant increase in the variability explained by the model.
`r unhide()`

\
On the basis of the model comparison with `anova`, which model should be preferred:

`r mcq(c("linear component of age only", answer="linear and quadratic components of age", "linear, quadratic, cubic components of age"))`

:::


In conclusion, our analyses suggest that a model with a quadratic component of `age` (`pmod2`) is sufficient to explain the data, and that would be the model that we would choose to report.


## Bayesian approach

As we did in the previous session, we can use Bayes Factors to compare the models with different higher-order polynomial components.

Unfortunately, `poly()` does not work seamlessly with `lmBF`, as it did with `lm`. We need to create separate variables in `SurveyData` for the quadratic and cubic components before working out the Bayes Factors with `lmBF`.

To add the quadratic component to `SurveyData`:

```{r}
SurveyData <- 
  SurveyData %>% mutate( age2 = poly(age,2)[,"2"] )
```

**Explanation of the code:** The code takes `SurveyData`, then uses `mutate` to add a new variable `age2` to the dataset. `age2` contains the quadratic component of `age`, specified using `poly(age,2)[,"2"]`. 

We can see the new variable `age2` when we look at `SurveyData` again:

```{r}
SurveyData
```

Now create the variable for the cubic component:

```{r}
SurveyData <- 
  SurveyData %>% mutate( age3 = poly(age,3)[,"3"] )
```

**Explanation of the code:** The code takes `SurveyData`, then uses `mutate` to add a new variable `age3` to the dataset. `age3` contains the quadratic component of `age`, specified using `poly(age,3)[,"3"]`. 

We can see the new variable `age3` when we look at `SurveyData` again:

```{r}
SurveyData
```


\
To derive the Bayes Factor for `pmod1` using `lmBF()`:
```{r,warning=FALSE}
# first make sure the BayesFactor package is loaded so we can use lmBF
library(BayesFactor)

# store the Bayes Factor for pmod1
pmod1BF <- lmBF(happiness ~ age, data = as.data.frame(SurveyData) )
```

\


To derive the Bayes Factor for `pmod2` using `lmBF()`:
```{r}
# store the Bayes Factor for pmod2
pmod2BF <- lmBF(happiness ~ age + age2, data = as.data.frame(SurveyData) )
```


The Bayes Factor comparing `pmod2BF` and `pmod1BF` is then:
```{r}
pmod2BF / pmod1BF
```

:::{.exercise}
How many more times likely is a model with a quadratic component of age than only a linear component? 
`r mcq(c("2.62e-17", answer = "2.62e+17"))` 

Does this constitute strong evidence for `pmod2` over `pmod1`? 
`r mcq(c(answer="yes","no"))`

`r hide('Explain why')`
The Bayes Factor tells us that a model with a quadratic component of `age` is 2.62e+17 or $2.62 \times 10^{17}$ times more likely than one that simply contains a linear component. This is overwhelming evidence for the inclusion of a quadratic component of `age` in the model.
`r unhide()`

:::


Next, determine the Bayes Factor for `pmod3`:

```{r}

pmod3BF <- BayesFactor::lmBF(happiness ~ age + age2 + age3, data = as.data.frame(SurveyData) )

```

Compare `pmod3BF` and `pmod2BF`:

```{r}
pmod3BF / pmod2BF
```

:::{.exercise}
How many more times likely is a model with a cubic component than one with only linear and quadratic components? 
`r fitb(answer="0.16")`

Does this constitute strong evidence for the inclusion of a cubic component in the model? 
`r mcq(c("yes",answer="no"))`

`r hide('Explain why')`
The Bayes Factor tells us that a model with a cubic component of `age` is only 0.16 times more likely than one that contains both linear and quadratic components. Because the Bayes Factor is less than 0.33, this constitutes strong evidence for the simpler model with a quadratic component.
`r unhide()`

\
On the basis of the model comparison with Bayes Factors, which model should be preferred?

`r mcq(c("linear component of age only",answer="linear and quadratic components of age","linear, quadratic, cubic components of age"))`

:::


A comparison of Bayes Factors agrees with the results using `anova`: the Bayes Factors indicate that there is more evidence for `pmod2` than `pmod1`, and that `pmod2` should be preferred over `pmod2`.




## Exercise


:::{.exercise}

Now you try incorporating polynomials to a regression, and do so by investigating the relationshipp between `age` and `anxiety` in `SurveyData`.


**Does `age` predict `anxiety`, and are there ?**


The column `anxiety` in `SurveyData` contains responses to the question:


**"Overall, how anxious did you feel yesterday?  Where 0 is 'not at all anxious' and 10 is 'completely anxious'."**


- Create a scatterplot of `age` vs. `anxiety`. Does there appear to be a linear or non-linear relationship?

`r hide("Try to create the plot yourself first. Click to show the code")`
```{r fig14, fig.height = 3.5, fig.width = 3.5, fig.align = "center",echo=F}
SurveyData %>%
  ggplot(aes(x=age, y=anxiety)) +
  geom_point()
```
`r unhide()`

`r hide("Try to discern the trend in the data yourself first, then click to see answer")`
A slight bow is evident is evident in the plot such that `age` and `anxiety` seem to follow an inverted U-shaped relationship. 
\
Reported `anxiety` levels increase from 30 years to middle age (approx. 50 years) and then declines from 50 to 70 years. This mirrors the relationship with `age` and `happiness`: `anxiety` is greatest when `happiness` seems lowest. 
`r unhide()`

\
**Answer the following questions:**

- What percentage of the variance in `anxiety` is explained by a model with `age` as predictor?
`r fitb(answer="1.19")` %

\

- What percentage of the variance in `anxiety` is explained by a model containing both linear and quadratic components of `age` as predictors? 
`r fitb(answer="5.63")` %

- What is the _increase_ in $R^2$ if a quadratic component of `age` is added to the model?
`r fitb(answer="4.44")` %

- Does this increase represent a statistically significant increase? 
`r mcq(c(answer="yes","no"))`

- What is the _F_-statistic for the test of the model with a linear component vs. linear + quadratic components?
_F_(1, 147) = `r fitb(answer="6.92")`, _p_ = .009

\

- What percentage of the variance in `anxiety` is explained by a model containing both linear, quadratic and cubic components of `age` as predictors? 
`r fitb(answer="7.02")` %

- What is the _increase_ in $R^2$ if a cubic component of `age` is added to the model?
`r fitb(answer="2.58")` %

- Does this increase represent a statistically significant increase? `r mcq(c("yes", answer="no"))`

- What is the _F_-statistic for the test of the model with a linear component vs. linear + quadratic components?
_F_(1, 146) = `r fitb("1.22")`, _p_ = `r fitb("0.14")`

\

- On the basis of the model comparison with ANOVA, which model should be preferred?
`r mcq(c("linear component of age only",answer="linear and quadratic components of age","linear, quadratic, cubic components of age"))`


`r hide("Show me the code to do this")`

```{r,eval=FALSE}
# fit a linear model, show results
anx1 <- lm(anxiety ~ age, data = SurveyData)
summary(anx1)

# fit a quadratic component, show results
anx2 <- lm(anxiety ~ poly(age,2), data=SurveyData)
summary(anx2)

# compare linear and linear+quadratic models
anova(anx1, anx2)

# fit a cubic component
anx3 <- lm(anxiety ~ poly(age,3), data=SurveyData)
summary(anx3)

# compare (linear + quadratic) and (linear + quadratic + cubic) models
anova(anx2, anx3)
```
`r unhide()`


**Now compare models with Bayes Factors:**

(Note: you do not need to re-add the quadratic and cubic components of age to `SurveyData` - these should still be in `SurveyData` if you followed the steps before.)

 - The Bayes Factor comparing a model with linear and quadratic components vs. linear component only is `r fitb(answer="5.48")`
 
 - This indicates that there is more evidence for which model? `r mcq(c("linear component only",answer="linear plus quadratic components"))`
 
 \
- The Bayes Factor comparing a model with linear, quadratic and cubic components vs. linear and quadratic components only is `r fitb("0.76")`
  
- This indicates that there is more evidence for which model? `r mcq(c(answer="linear plus quadratic component","linear, quadratic, and cubic components"))`
 
 \
 
Do the comparisons of models with Bayes Factors support the conclusions made with anova? `r mcq(c("no", answer="yes"))`


`r hide("Show me the code to determine the Bayes Factors for each model")`

```{r}
library(BayesFactor)

# BF for model anx1
anx1BF <- lmBF(anxiety ~ age, data = as.data.frame(SurveyData) )

# BF for model anx2
anx2BF <- lmBF(anxiety ~ age + age2, data = as.data.frame(SurveyData) )

# compare BFs for anx2 and anx1
anx2BF / anx1BF

# BF for model anx3
anx3BF <- lmBF(anxiety~ age + age2 + age3, data = as.data.frame(SurveyData) )

# compare BFs for anx3 and anx2
anx3BF / anx2BF

```
`r unhide()`




:::


## Summary of key points

- Polynomial terms can be added to regression models to fit curves in our data. 

\

- `poly(predictor name, X)` can be used with `lm` to specify models with polynomial terms of the Xth order.

  - The improvement in fit ($R^2$) as a result of adding in a polynomial term can be tested using `anova(model1, model2)`.

\

- Bayes Factors can also be used to compare models with polynomial terms using `lmBF`.

  - You must store the polynomial components in the dataset first before using `lmBF`. Use `poly(predictor name, X)[,"X"]`.

\

- **A note of caution**: Although curves of any complexity can be fit, it may not always be meaningful or parsimonious to do so. Complex models may _overfit_ the data and may not necessarily generalise to new datasets well. It is also important not to extrapolate beyond the range of data used to generate the model when making predictions from the model, as the same relationship may not be present.

