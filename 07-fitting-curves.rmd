# Fitting curves

### In brief

> So far all our regression models have assumed that our variables have
> **_linear relationships_**. That isn't always the case, and sometimes we need
> to fit curved lines to describe the relationship of predictors and outcomes.
> As we saw before, fitting curved lines has costs as well as benefits: A curved
> line is more likely to **overfit** the data, and may be less good at
> predicting new data. But for some models curved lines are essential to
> describe the world as it really is.


<!--

```{r, echo=F, include=F}
knitr::opts_chunk$set(echo = TRUE, collapse=TRUE, cache=TRUE, comment=">", message=FALSE)
library(tidyverse)
library(webex)
library(pander)
theme_set(theme_minimal())
```

## Session 1

We will cover use of polynomials and other techniques to fit curved lines with multiple regression.

## Learning outcomes

- Understand that polynomial terms can be added to a regression model to fit curves to the data (rather than just a straight line).

- Understand how to evaluate the fit of a model using R-squared.

- Understand how to select between different models using ANOVA.


## Structure

- In the first part of the workshop, we will play around with plotting various curves using `ggplot`.

- In the second part, we will see how we can fit a curve to some experimental data using polynomials. 

- In the exercise at the end, you will try to figure out the curve that best fits a set of data. Accordingly, it is recommended to store your commands in a .r file as you go along, to make it easy to modify later.



# Polynomials

The regression models we have been fitting assume a linear (i.e., straight line) relationship between variables.

However, variables may not always be related in a linear fashion. When we plot our data, the line of best fit may appear curved indicating that a _non-linear_ relationship may be present.

Suppose variables x and y showed the following trend:

```{r,echo=F}
set.seed(1)
d <- tibble(x=seq(0,6,0.2),
            y=3.5 + 5*x - 8*x^2 + 3*x^3 -.3*x^4)

d %>% ggplot(aes(x,y)) +
        geom_line(size=1) + 
        theme_bw()
```

It is clear that this relationship would not be explained well by fitting a standard regression model, which assumes a linear relationship between x and y. We'd lose important information about the relationship if we only fit a straight line. 

To improve our model, we can fit a curve by using **polynomial** terms in our regression. 

**Polynomial** means that a variable is raised to a particular power. For example: 

- $x^2$ means x-squared, which is x-multiplied-by-x.

- $x^3$ means x-cubed, which is x-multiplied-by-x-multiplied-by-x

The power that we raise x to could be any number.

To understand how polynomials allow us to do this, let's review how lines can be represented as equations. It may also be worth revisiting the section on [Linear Equations in the Regression lecture]( https://plymouthpsychology.github.io/psystats/regression.html#linear-equations).


# Constant

The equation **$y = 1$** would be represented like this:

```{r,echo=F}
d<-tibble(x=seq(-10,10,0.1),y=rep(1,length(x)))
d %>% ggplot(aes(x=x)) + 
  geom_line(aes(y=y),size=2,colour='orange') +
  geom_line(aes(y=0)) +
  geom_line(aes(y=x,x=0)) +
  theme_bw()
```

Because y is the same value (i.e., 1) at each value of x, we say that y is equal to a **"constant"**. The value of y determines the **height** of the line.


# Linear

The equation **$y = 0.5(x)$** would be represented as follows:


```{r,echo=F}
d<-tibble(x=seq(-10,10,0.1),y=0.5*x)
d %>% ggplot(aes(x=x)) + 
  geom_line(aes(y=y),size=2,colour='orange') +
  geom_line(aes(y=0)) +
  geom_line(aes(y=x,x=0)) +
  theme_bw()
```

Each value of y on the line is half that of x.

- When x is 2, y is 1
- When x is 5, y is 2.5

We say that the relationship between y and x is **linear**, because it is a described by a straight line, and y is simply equal to a value of x multiplied by a constant (0.5).

Multiplying x by larger numbers would make this line steeper; smaller numbers make this line less
steep. If the number were negative, the line would slope from the upper left of the plot to the lower right.


# Constant + linear component

The equation **$y = 1 + 0.5(x)$** would be represented as follows:


```{r,echo=F}
d<-tibble(x=seq(-10,10,0.1),y=1+0.5*x,a=rep(1,length(x)),b=0.5*x)
d %>% ggplot(aes(x=x)) + 
  geom_line(aes(y=y),size=2,colour='orange') +
  geom_line(aes(y=0)) +
  geom_line(aes(y=x,x=0)) +
  geom_line(aes(y=a),linetype=2,size=1,colour='orange') +
  geom_line(aes(y=b),linetype=2,size=1,colour='orange') +
  theme_bw()
```

We can think of this line as being made up of a constant and a linear component.

- The constant in this equation is indicated by the dashed horizontal line.
- The linear component to this equation 0.5(x) is indicated by the dashed slope line.
- The solid orange line is a combination of these two components.


# Quadratic

The equation **$y = x^2$** would be represented as follows:

```{r,echo=F}
d<-tibble(x=seq(-10,10,0.1),y=x^2)
d %>% ggplot(aes(x=x)) + 
  geom_line(aes(y=y),size=2,colour='orange') +
  geom_line(aes(y=0)) +
  geom_line(aes(y=x*10,x=0)) +
  theme_bw()
```

To get each value of y, we square the value of x. 

So, when x = -5, y is 25. And if x = -4, y = 16, and so on...


**$y = -x^2$**,  would look as follows:

```{r,echo=F}
d<-tibble(x=seq(-10,10,0.1),y=-(x^2))
d %>% ggplot(aes(x=x)) + 
  geom_line(aes(y=y),size=2,colour='orange') +
  geom_line(aes(y=0)) +
  geom_line(aes(y=x*10,x=0)) +
  theme_bw()
```


# Linear plus quadratic components

The equation **$y = 50 + 5(x) - x^2$** has 

- a constant 50
- a linear component 5(x)
- a quadratic component $-x^2$:

```{r,echo=F}
d<-tibble(x=seq(-10,10,0.1),y=50+5*x-x^2,a=rep(50,length(x)),b1=5*x,b2=-(x^2))
d %>% ggplot(aes(x=x)) + 
  geom_line(aes(y=y),size=2,colour='orange') +
  geom_line(aes(y=0)) +
  geom_line(aes(y=x*10,x=0)) +
  geom_line(aes(y=a),linetype=2,size=1,colour='orange') +
  geom_line(aes(y=b1),linetype=2,size=1,colour='orange') +
  geom_line(aes(y=b2),linetype=2,size=1,colour='orange') +
  theme_bw()
```

The dashed lines on the plot indicate the intercept, linear component, and quadratic components of the equation. The solid line represents the equation.

When we see any curve, it is possible to think of it as being composed of components like this.


<div class='exercise'>

Let's plot some curves. First, load the `tidyverse` package:

```{r}
library(tidyverse)
```

We can use `ggplot()` to plot curves:

```{r}
tibble(x=1:10) %>% 
  ggplot(aes(x)) + 
  stat_function(fun=function(x) 2*x + x^2)
```

`stat_function(fun=function(x) ....` allows us to superimpose any function we specify on a plot, in this case $2x + x^2$.

'x^2' means 'x-to-the-power-of-2', or $x^2$.

**Modify the equation in `stat_function(fun=function(x) EQUATION)` to plot the following:**

- $y = 3$
- $y = x$
- $y = 3 + x$
- $y = x^2$

Hint: the first one would be `stat_function(fun=function(x) 3)`

**Try to reproduce the following plot: **

(Hint: what do you have to do to the values of x to get the values of y?)

```{r,echo=F}
tibble(x=1:10) %>% 
  ggplot(aes(x)) + 
  stat_function(fun=function(x) - x^2)
```

`r hide("Answer")`

```{r,visible=F}
tibble(x=1:10) %>% 
  ggplot(aes(x)) + 
  stat_function(fun=function(x) - x^2)
```

`r unhide()`


</div>


# Identifying components

It is worth noting some general characteristics of polynomial equations to give you a better idea of what these various components in the equation look like.

## Linear (first-order) components

In a linear equation of the form **$y = b_0 + b_1(x)$**, the coefficient $b_0$ represents the **intercept**. $b_1$ is the coefficient for x.

The line for a linear equation keeps on increasing and decreasing for all values of x, with no bends:

```{r,echo=F}
d<-tibble(x=seq(-10,10,0.1),y=x)
d %>% ggplot(aes(x=x)) + 
  geom_line(aes(y=y),size=2,colour='orange') +
  geom_line(aes(y=0)) +
  geom_line(aes(y=x,x=0)) +
  theme_minimal()
```


## Quadratic (second-order) components

Equations with a quadratic component have form **$y = b_0 + b_1(x) + b_2(x^2)$**. Again, $b_0$ represents the intercept. $b_1$ and $b_2$ are the coefficients for $x$ and $x^2$, respectively.

The **maximum/minimum** of a curve is the **point of inflection** at which the curve switches from decreasing to increasing (or increasing to decreasing). Equations with a quadratic component have one minimum (if the $x^2$ component is positive) or one maximum (if the $x^2$ component is negative).

For curves with positive $x^2$ components:

```{r,echo=F}
d<-tibble(x=seq(-10,10,0.1),y=50+5*x+x^2)
d %>% ggplot(aes(x=x)) + 
  geom_line(aes(y=y),size=2,colour='orange') +
  geom_line(aes(y=0)) +
  geom_line(aes(y=x*20,x=0)) +
  geom_point(y=min(d$y),x=-2.5,size=4) + 
  theme_bw()
```

The dot indicates the minimum. Either side of this point, the values of y increase.



For curves with $-x^2$ components:

```{r,echo=F}
d<-tibble(x=seq(-10,10,0.1),y=50+5*x-x^2)
d %>% ggplot(aes(x=x)) + 
  geom_line(aes(y=y),size=2,colour='orange') +
  geom_line(aes(y=0)) +
  geom_line(aes(y=x*10,x=0)) +
  geom_point(y=max(d$y),x=2.5,size=4) +
  theme_bw()
```

The dot indicates the maximum. Either side of this point, the values of y decrease.

<div class='exercise'>

Use `ggplot()` and `stat_function()` to plot a 2nd degree (quadratic) polynomial with the following characteristics:

- a negative intercept
- a positive linear component
- a negative quadratic component


`r hide("Show me an example")`
```{r}
tibble(x=1:10) %>% 
   ggplot(aes(x)) + 
   stat_function(fun=function(x) -5 + 0.5*x - 6.0*x^2) +
   xlim(-5,5) + 
   ylim(-100,100)
```

`xlim()` is used to specify the range of values on the x-axis (-5 to 5).

`ylim()` is used to specify the range of values on the y-axis (-100 to 100).

The curve specified in the line `stat_function(fun=function(x) -5 + 0.5*x - 6.0*x^2)` is 

$y = -5 + 0.5x - 6.0x^2$

</div>

## Cubic (third) order components

Curves with a cubic component have _two minima/maxima_ (i.e., two points of inflection), and are of the form **$y = b_0 + b_1(x) + b_2(x^2) + b_3(x^3)$**, where $b_0$ is the intercept, and $b_1$, $b_2$, and $b_3$ are the coefficients for the $x$, $x^2$, and $x^3$ components. 

```{r,echo=F}
d<-tibble(x=seq(-10,10,0.1),y=0 + 0.2*x + 0.2*x^2 + 0.2*x^3)
d %>% ggplot(aes(x=x)) + 
  geom_line(aes(y=y),size=2,colour='orange') +
  geom_line(aes(y=0)) +
  geom_line(aes(y=x*30,x=0)) +
  theme_bw()
```

<div class='exercise'>

Use `ggplot()` and `stat_function()` to plot a 3rd degree (cubic) polynomial with the following characteristics:

- a positive intercept
- a negative linear component
- a positive quadratic component
- a negative cubic component


`r hide("Show me an example")`
```{r}
tibble(x=1:10) %>% 
   ggplot(aes(x)) + 
   stat_function(fun=function(x) 5 - 0.5*x + 6.0*x^2 - 0.5*x^3) +
   xlim(-10,10) + 
   ylim(0,1000)
```

The curve specified in the line `stat_function(fun=function(x) 5 - 0.5*x + 6.0*x^2 - 0.5*x^3)` is

$y = 5 - 0.5x + 6.0x^2 - 0.5x^3$

</div>

## Quartic (fourth) order components

A curve with a quartic component would have _three maxima/minima_ (i.e., three points of inflection). The equation would be of the form **$y = b_0 + b_1(x) + b_2(x^2) + b_3(x^3) + b_4(x^4)$**.

```{r,echo=F,warning=FALSE}
d<-tibble(x=seq(-10,10,0.1),y=0+0.5*x-6.0*x^2+1.0*x^3+0.5*x^4)
d %>% ggplot(aes(x=x)) + 
  geom_line(aes(y=y),size=2,colour='orange') +
  geom_line(aes(y=0)) +
  geom_line(aes(y=x*30,x=0)) +
  ylim(-100,100) +
  theme_bw()
```
<div class='exercise'>

Use `ggplot` and `stat_function` to plot a 4th degree polynomial.

Note: The points of inflection may not always be visible if the scale on either the y- or x-axis is very large. You can use `ylim()` and `xlim()` to adjust the scale of the axes, e.g., for the quartic function above:

`r hide("Show me")`
```{r}
tibble(x=1:10) %>% 
   ggplot(aes(x)) + 
   stat_function(fun=function(x) 0 + 0.5*x - 6.0*x^2 + 1.0*x^3 + 0.5*x^4) +
   xlim(-5,5) + 
   ylim(-100,100)
```
The code specified in `stat_function(fun=function(x) 0 + 0.5*x - 6.0*x^2 + 1.0*x^3 + 0.5*x^4)` is

$y = 0 + 0.5x - 6.0x^2 + 1.0x^3 + 0.5x^4)$

`r unhide()`


</div>





# Fitting curves to data rather than straight lines


When fitting a regression model to the data, how can we identify whether our regression model should have polynomial components?


A 2016 survey of the National Office for Statistics investigated happiness across the life span (https://bit.ly/2jaZKBx). Approximately 300,000 individuals of all ages answered questions related to well-being. A BBC news report of the study can be found [here](https://www.bbc.co.uk/news/uk-35471624).


Each participant answered the following question regarding happiness:

<div class='exercise'>
"Overall, how happy did you feel yesterday?  Where 0 is 'not at all happy' and 10 is 'completely happy'."
</div>

A fictitious sample of this data is in the file located at ("https://bit.ly/2QlLFAZ"). (Although I made the data up for teaching purposes, the relationships present are the same.)



Load the data into R: 


```{r}
SurveyData <- read_csv("https://bit.ly/30HuHPE")

```

Preview it:

```{r}
SurveyData %>% glimpse()
```


Obtain some descriptive statistics:

```{r}
SurveyData %>% summary()
```

<div class = "exercise">
What was the age range of the sample?
</div>

Use a scatterplot to look at the relationship between age and happiness:

```{r}

SurveyData %>%
  ggplot(aes(x=age, y=happiness)) +
  geom_point()

```


<div class = "exercise">
Does `age` appear to be related to `happiness`? How would you describe the nature of the relationship?

Does there appear to be a linear, quadratic, or cubic component?

How would you describe this relationship in psychological terms?
</div>

## Linear regression

Run a linear regression to determine whether `happiness` can be predicted from `age`:

```{r}
model1 <- lm(happiness ~ age, data = SurveyData)
summary(model1)
```

<div class='exercise'>
Write the regression equation for predicting `happiness` from `age`, using the values from the `Estimate` column.


`r hide("Show me")`

The regression equation is:

Predicted happiness = 6.48 + 0.02*age

`r unhide()`
</div>



The following parts of the output tell us how well the model explains the data:

- **Multiple _R_-squared** ($R^2$) is the proportion of variance in the outcome variable (happiness) that can be accounted for by the regression model. It gives us some idea of how well our model explains the data. If we multiply the value by 100, we get a percentage. Therefore, in this model, `age` explains **43.4%** of the variance in `happiness` scores.

- The **_F_-statistic** gives us an idea of how much variance in happiness the model explains, relative to how much variance in happiness it does not explain (i.e., the error in prediction or residuals). There are two separate degrees of freedom associated with the _F_-statistic (1, 148). 

- A _p_-value associated with _F_ is also given. This is the probability of obtaining an _F_-statistic as extreme as this, given that the null hypothesis is true. Given that our _p_-value is less than .05, we can declare age to be a statistically significant predictor of happiness, and could report _F_ as follows:

<div class='exercise'>

- Age is a statistically significant predictor of happiness, _F_(1,148) = 113.00, _p_ < .001.

</div>

Now let's see how well the happiness values predicted by the regression equation match the trends in our data.  Plot the regression line from `model1` on the same plot as the data:


```{r}
SurveyData %>% mutate(fit1 = fitted(model1)) %>% 
  ggplot(aes(x=age, y=happiness)) + 
  geom_point() +
  geom_line(aes(y=fit1))
```

The above code uses the `mutate` function from the `dplyr` package to add an additional column named `fit1` to the SurveyData. `fit1` contains the values of happiness that are predicted by the model, given each individual's age.

<div class='tip'>
An alternative way to plot this graph, consistent with the session on regression, would be to use the `augment()` function in the `broom` package.

```{r}
broom::augment(model1) %>%  ggplot(aes(x=age, y=happiness)) + 
   geom_point() +
   geom_line(aes(y=.fitted))
```

`.fitted` are the predicted values computed by `augment()`

</div>

<div class = "exercise">
By visual inspection, does the model capture the trend in the data well? 
</div>

According to the model, happiness increases with age in a linear fashion. However, it is clear from the plot that this straight line is _not_ capturing the non-linear trend that is evident in our data. 


## Adding a quadratic component

We can add a quadratic component to the regression model using the `poly()` function. 


```{r}

model2 <- lm(happiness~poly(age,2), data=SurveyData)
summary(model2)

```

The '2' in the `poly()` function tells R that we want to fit a model with a quadratic component. R will then fit a model of the following form:


$happiness = a + b_1(age) + b_2(age^2)$


where $a$ is the intercept, and $b_1$ and $b_2$ are the coefficients for the linear and quadratic components, respectively. 


R will fit this model, and provide estimates of $a$, $b_1$ and $b_2$.


$b_1$ and $b_2$ are the **coefficients** in the regression equation. 


Visually inspect the fit of the second model:


Use `mutate` and `fitted` to derive the predicted values, as before. The regression line should now appear curved:


```{r}
SurveyData %>% mutate(fit2=fitted(model2)) %>% 
  ggplot(aes(x=age,y=happiness)) + 
  geom_point() +
  geom_line(aes(y=fit2))
```



<div class = 'exercise'>
Compare the value of Multiple R-Squared in `model1` and `model2`. 

Does the addition of a quadratic component result in an increase in R-Squared in model 2? By how much?


`r hide("Answer")`

_R_-squared change from model1 to model2 = 0.685 - 0.434 = 0.25.

Therefore, the model with the quadratic component accounts for 25% more variance in `happiness`.

`r unhide()`
</div>

We can test whether the _increase_ in _R_-squared in `model2` represents a statistically significant increase or not by comparing the models using an ANOVA:


```{r}

anova(model1, model2)

```

`anova` computes a new _F_-statistic, testing whether the _increase_ in variance explained by `model2` is statistically significant or not. 

<div class="tip">
In the previous lecture you were recommended against using `anova()` to **specify** models. The use here is different: we are comparing two existing models, rather than specifying and running a model in order to run an ANOVA in the way you did in the previous lecture. You are still advised _not_ to use `anova()` to specify models as this is cumbersome.
</div>

<div class="exercise">
By examining the F-statistic, is there sufficient evidence for an improvement in fit in the model as a result of adding in the quadratic component?


`r hide("Answer")`
We can report the improvement in fit as follows: 

A model with a quadratic component accounted for a statistically significantly greater proportion of variance in happiness than a model with only a linear component, _F_(1,148) = 117.00, _p_ < .001.
`r unhide()`

</div>

## Adding a cubic component


Now run a model with a cubic component:


```{r}

model3 <- lm(happiness~poly(age,3),data=SurveyData)
summary(model3)

```

The '3' in `poly(age,3)` tells R that we want to specify a model with a cubic component, of the form: 


$happiness = a + b_1(age) + b_2(age^2) + b_3(age^3)$


As before, plot the predicted values from this model in a scatterplot:


```{r}

SurveyData %>% mutate(fit3= fitted(model3)) %>% 
  ggplot(aes(x=age,y=happiness)) + 
  geom_point() +
  geom_line(aes(y=fit3))

```

<div class="exercise">
Does the visual fit of the model with a cubic component seem better than the fit of previous models?
</div>

<div class ="exercise">
What is the increase in _R_-squared as a result of adding in the cubic component? (Compare R-squared between `model3` and `model2`).

The increase in R-squared is `r fitb(answer=0.003, num=TRUE,tol=.001)` 
</div>

To determine if this change in _R_-squared is statistically significant or not, we can again use `anova()`:


```{r}

anova(model2,model3)

```

The _F_-statistic comparing `model3` and `model2` is not statistically significant, _F_(1, 146) = 1.27, _p_ = .26, indicating that the addition of the cubic component into the regression model does not result in a statistically significant increase in the variability explained by the model.


This suggests that a model with a quadratic component (`model2`) is sufficient to explain the data, and that would be the model that we would choose to report. There was insufficient evidence for a model with a cubic trend.


In conclusion, over the range of `age` considered, there is evidence for a quadratic relationship between `age` and `happiness`. There was insufficient evidence for a model with a cubic trend.


# Bayesian approach

Load the `BayesFactor` package:

```{r,include=FALSE}
library(BayesFactor)
```

If we wanted to compare model1, model2, and model3 using a Bayesian approach, then the process is a bit more fiddly.


First, because `poly()` does not work seamlessly with `BayesFactor::lmBF()`, we need to compute $age^2$ and $age^3$ ourselves:


```{r}
SurveyData <- SurveyData %>% mutate(age2= age^2, age3 = age^3)
``` 


This creates a new column in SurveyData with $age^2$ calculated for each individual. Likewise for $age^3$.


Now derive model1 and model2, but using `BayesFactor::lmBF()`:


```{r,warning=FALSE}
model1BF <- BayesFactor::lmBF(happiness~age, data=SurveyData)
model2BF <- BayesFactor::lmBF(happiness~age+age2, data=SurveyData)
```

R produces a warning here (don't worry about this).

Note that when adding polynomial terms to the regression equation in `lmBF()`, we use '+', rather than '*'. 

Compute the Bayes Factor for the comparison of `model2BF` and `model1BF` as follows:


```{r}
model2BF / model1BF
```

<div class="exercise">
How many more times likely is `model2BF` than `model1BF`? Does this constitute strong or anecdotal evidence for `model2BF`? Refer to the guidelines on [interpretation of Bayes Factors in the previous lecture](https://plymouthpsychology.github.io/psystats/anova.html#interpreting-bayes-factors)


`r hide("Answer")`

`model2BF` is 2.62 times more likely than `model1BF`.
This constitutes only weak or anecdotal evidence for `model2BF`.

What would the dartboard representation for this Bayes Factor look like?

`r unhide()`
</div>


To compare `model3BF` and `model2BF` using a Bayesian approach, we can take similar steps.


Determine the Bayes Factor for model3:

```{r}

model3BF <- BayesFactor::lmBF(happiness~age+age2+age3, data=SurveyData)

```

Compare `model2BF` and `model3BF`:

```{r}
model3BF / model2BF
```

Should `model3BF` be preferred over `model2BF`?

`r hide("Answer")`

No. There is scarcely any evidence for model3 according to the Bayes Factor.

The model without the cubic component (`model2BF`) should be preferred.

`r unhide()`

The results of this analysis largely agree with the traditional (NHST) analyses: the Bayes Factors indicate that there is more evidence for model2 than model1 (although the evidence is weak), but there is scarcely any evidence for model3 over model2.



<div class="tip">
Tip: An advanced way to add polynomial terms to `SurveyData` would be:
```{r}
SurveyData <- bind_cols(SurveyData,
                        poly(SurveyData$age,3) %>% 
                        as.tibble() %>% 
                        setNames(paste0("age",1:3)))
```
This can be useful if testing a large number of polynomials.

The code uses `bind_cols()` to add new columns to `SurveyData`:

- `poly(SurveyData$age,3)` creates three new columns containing the $x$, $x^2$, and $x^3$ components.
- `as.tibble()` converts these columns to tibble format
- `setNames(paste0("age"),1:3)` sets the columns names for the three new columns as `age1`, `age2`, and `age3`.

</div>
  


# Exercise

## Does `age` predict `anxiety`?

<div class='exercise'>

The column `anxiety` in `SurveyData` contains responses to the question:


"Overall, how anxious did you feel yesterday?  Where 0 is 'not at all anxious' and 10 is 'completely anxious'."


- Create a scatterplot of `age` vs. `anxiety`. Does there appear to be a non-linear relationship?

`r hide("Show me")`
```{r,eval=FALSE}
SurveyData %>%
  ggplot(aes(x=age, y=anxiety)) +
  geom_point()
```
A slight bow is evident is evident in the plot such that `age` and `anxiety` seem to follow an inverted U-shaped relationship. 
`r unhide()`

- By testing for polynomial components, derive the regression model for predicting `anxiety` from `age`. To do this, use the same sequence of steps that we used above, but replace `happiness` with `anxiety`:

Answer the following questions:

- Is the relationship between age and anxiety linear or non-linear?

`r hide("Show me")`
There is sufficient evidence to suggest that the relationship between age and anxiety is non-linear
`r unhide()`

- If the relationship is non-linear, then what is the form of the relationship (i.e., quadratic, cubic, quartic etc.)? 

`r hide("Show me")`

A model with a quadratic component seems to provide a best account of the data:

```{r,eval=FALSE}
# fit a linear model
model1 <- lm(anxiety ~ age, data = SurveyData)
summary(model1)

# fit a quadratic component
model2 <- lm(anxiety~ poly(age,2), data=SurveyData)
summary(model2)

# determine if the model with a quadratic component results in an improvement in fit vs. linear
anova(model1, model2)

# check for a cubic component
model3 <- lm(anxiety ~ poly(age,3), data=SurveyData)
summary(model3)

# determine if the model with a cubic component results in an improvement in fit vs. quadratic
anova(model2, model3)
```
`r unhide()`


- Do the results of a Bayesian analysis agree?

`r hide("Show me")`
The results of a Bayesian analysis agreed:
```{r,eval=FALSE}
library(BayesFactor)

# create polynomials by hand
SurveyData <- SurveyData %>% mutate(age2= age^2, age3 = age^3)

# BF for model 1
model1BF <- BayesFactor::lmBF(anxiety~ age, data=SurveyData)

# BF for model 2
model2BF <- BayesFactor::lmBF(anxiety~ age + age2, data=SurveyData)

# compare BFs for models 1 and 2
model2BF / model1BF

# BF for model 3
model3BF <- BayesFactor::lmBF(anxiety~ age + age2 + age3, data=SurveyData)

# compare BFs for models 2 and 3
model3BF / model2BF

```
`r unhide()`

- Once you have decided on the **final model**:

    - What proportion of variance in anxiety is explained by the model?
    
    - Write the regression equation.
    
    - Produce a scatterplot showing the data with the final model superimposed on it.
    
    - Describe the relationship between `age` and `anxiety` in psychological terms. Does this align with the relationship between `age` and `happiness`?
    
    
`r hide("Show me")`
```{r,eval=FALSE}
summary(model2)

SurveyData %>% mutate(fit2=fitted(model2)) %>% 
  ggplot(aes(x=age,y=anxiety)) + 
  geom_point() +
  geom_line(aes(y=fit2))
```

- The proportion of variance explained by the model R-squared is **5.63%**

- The regression equation is: 

Predicted Anxiety = 2.94 - 1.02Age - 1.98Age^2

- Reported anxiety levels increase from 30 years to middle age (approx. 50 years) and then declines from 50 to 70 years. This mirrors the relationship with age and happiness: anxiety is greatest when happiness seems lowest. 


`r unhide()`


</div>


# Summary

- Polynomial terms can be added to regression models to fit curves in our data. 

- `poly(predictor,X)` can be used when conducting the regression to test for the significance of polynomial terms of the Xth order.

- The improvement in fit (R-squared) as a result of adding in a polynomial term can be tested using `anova(model1,model2)`.

- A note: Although curves of any complexity can be fit, it may not always be meaningful or parsimonious to do so. Complex models may 'overfit' the data and may not necessarily generalise to new datasets well. It is also important not to extrapolate beyond the range of data used to generate the model when making predictions from the model, as the same relationship may not be present.

-->
